{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About \u00b6 CS-INDEX \u00b6","title":"About"},{"location":"#about","text":"","title":"About"},{"location":"#cs-index","text":"","title":"CS-INDEX"},{"location":"knowledge-base/books/","text":"list of recommended books \u00b6 git pro","title":"list of recommended books"},{"location":"knowledge-base/books/#list-of-recommended-books","text":"git pro","title":"list of recommended books"},{"location":"knowledge-base/cs2203-database1/","text":"Databases 1 CS-2203 \u00b6 Unit1: Databases and Relational Data Model Unit 2: Conceptual Data Model","title":"Databases 1 CS-2203"},{"location":"knowledge-base/cs2203-database1/#databases-1-cs-2203","text":"Unit1: Databases and Relational Data Model Unit 2: Conceptual Data Model","title":"Databases 1 CS-2203"},{"location":"knowledge-base/cs2203-database1/unit1/","text":"Unit1: Databases and Relational Data Model \u00b6 Unit1: Databases and Relational Data Model Reading Types of information models Career paths with databases Chapter 2: Relational data model Basic concepts 1.attributes 2. domain 3. Tuples 4. relations 5. Schemas 6. keys 6-1 Candidate keys 6-2 primary keys 6-3 Foreign keys Relational Data Model Constrains Entity integrity constrains Referential integrity constrains In Case of deleting / Updating a tuple that has a FK, there are 3 possibilities: Semantic integrity constrains domain constrains NULL constrains UNIQUE constrains check constrains Relational Algebra Reading \u00b6 Database Fundamentals, IBM. chapter 1 & 2 Database design, chapters 1 to 7. Types of information models \u00b6 Network Model (CODASYL). Hierarchial Model (IMS). Relational. Entity-Relationship (ER). Extended Relational. Semantic. Object-oriented. Object-relational. semi-structured (XML). Career paths with databases \u00b6 data architect. database architect. database administrator (DBA). applications developer. Chapter 2: Relational data model \u00b6 Basic concepts \u00b6 1.attributes \u00b6 represent column in a table,field in the object. data characteristics of the object. all attributes hve values. 2. domain \u00b6 set of all possible values for an attribute. these values MUST be atomic : 1. non-decomposable. 2. smallest possible unit of dat that can not be divided. eg: domain of Boolean attribute is [true, false]. the comparison between the values of 2 attributes is only possible if the 2 attributes have the same domain, otherwise, the comparison does not make sense. domains a re not explicitly stored in the DB, but they MUST be part of the DB definition. each attribute definition MUST include a reference to its domain definition in the DB definition part. 3. Tuples \u00b6 ordered set of values that represent data in the relation. represents: row in a table, record in a file. 4. relations \u00b6 relations represent tables. each relation has a header and body. relation header: fixed set of attributes, represent the head of a table, and each of theses attributes corresponds to a domain. relation body: set of tuples represent the table rows, each tuple has value corresponds ot the attribute in the header. relation Degree : number of attribute in the header, eg. unary, binary, nary... relation cardinality : number of tuples in the relation = rows count, changes overtime (with adding/removing records). relation instance : the state of a relation at a specific moment of time. relation properties : there are no duplicate tuples. tuples are unordered. attributes are unordered. attribute values are atomic. 5. Schemas \u00b6 formal description for all DB relations and relationships between them. 6. keys \u00b6 keys are identifiers for the tuple. used for: enforce rules and constrains on DB. constrains are important for maintaining consistency and correctness of DB. DBMS are responsible for maintaining keys. keys types : candidate keys. primary keys (PK). foreign keys (FK). 6-1 Candidate keys \u00b6 unique identifier for the tuples of a relation, that consists of one or more attributes. if no candidate keys have been specified, the set of k = (all tuple values), might work as a candidate key if and only if: K is unique overtime (no other tuple with the same values). minimality: none of the tuple values can be discarded without destroying the uniqueness property. using Data Definition Language (DDL) you can specify candidate keys by adding UNIQUE keyword to the definition of the attribute. a relation can have multiple candidate keys: one key -> primary key. other keys -> alternate keys. 6-2 primary keys \u00b6 candidate key that has been chosen to represent the relationship. must be: 1. unique. 2. not null. primary key is an attribute that has no meaning in the real life, but it always exist and unique, thus it can be named: surrogate key. artificial key. 6-3 Foreign keys \u00b6 an attribute that references a primary key from another relation. FK and its correspondents PK (from the other relation) MUST have the same domain. Relational Data Model Constrains \u00b6 entity integrity constrains. referential integrity constrains. semantic integrity constrains. Entity integrity constrains \u00b6 no attribute that participates in PK can have NULL value. in real life: an entity with NULL PK means that this entity does not exist. NULL value means on of the following: absence of value. undefined value. value that does not belong to the attribute domain. Referential integrity constrains \u00b6 for every value of FK in a relation R1, there MUST be a tuple from the other relation R2 so that, PK (R2) = FK (R1). FK with NULL value, means that the relationship between the this tuple and the other relation does not exist. In Case of deleting / Updating a tuple that has a FK, there are 3 possibilities: \u00b6 CASCADE : the operation cascades to the record of the second relations and affects the tuple that has been referenced with FK . RESTRICTS : prevent the operation from happening on all tuples with FK that is not NULL, so the operation is rejected. NULLIFIES : update the record from the second relation that has been referenced by the FK, set the value that points to the affected tuple to be NULL . Semantic integrity constrains \u00b6 includes: domain constrains. null constrains. unique constrains. check constrains. domain constrains \u00b6 all values of an attribute MUST belong to its domain. domain constrains include: Format constrains : all values must match a specific pattern, eg. regex, 6 digits... Range Constrains : all values must be in a specific range, eg. number of employees in one department can not exceed the number of employees in the whole company. NULL constrains \u00b6 value can not be null. DEFAULT keyword can be used to give the a attribute a default value in the case of NULL. UNIQUE constrains \u00b6 no 2 tuples can have the same value for this attribute. NULL is a valid unique value. check constrains \u00b6 a condition in a relation data that always checked when the data is manipulated. when defining the check constrains you can add instructions that can be executed when the check fails, and if these instructions are not provided, the operation will be rejected. Relational Algebra \u00b6 set of operators to manipulate relations. each operator takes one or more relations as INPUT and returns a new relation as OUTPUT . operators are divided into: traditional -------------- special 1. union 1. select 2. intersection 2. project 3. difference 3. join 4. cartesian product 4. divide operator definition of results characteristics UNION - R1 UNION R2 = set of all tuples that belongs to R1, or R2, or both 1. associative. 2. commutative 3. R1, R2 must be union-compatible - INTERSECTION - R1 INTERSECT R2 = set of tuples that belong to Both relations. same as UNION - DIFFERENCE - R1 DIFF R2 = all tuples that belongs to R1, AND NOT belong to R2. 1. R1, R2 must be union-compatible - CARTESIAN PRODUCT - R1 times R2 = set of all tuples where each tuple of the output results 1. R1, R2 must be union-compatible from a concatenation operation between one tuple from R1, and correspondents tuple from R2. - if R1 si of degree n, R2 of degree m => result is of degree n+m - SELECTION - takes one relation AND one condition as input 1. result degree is same as input relation - selects all tuples from the relation that satisfies the input condition. 2. result cardinality is less or equal to the input. - PROJECTION - takes one relation AND list of attributes as input. - returns subset of tuples of a relation with duplicate tuples are eliminated. - JOIN - concatenates 2 relations based on a joining condition or predicate. - theta-join : join result MUST must includes 2 identical attributes (one from each relation), when one attribute is eliminated, it is natural join . when some tuples of 2 relations don't have matching tuple => outer join left outer join : result includes all tuples of R but not all tuples of L right outer join : result includes all tuples of L but not all tuples of R full outer join : result includes all tuples of R AND ALL tuples of L - DIVISION - divides a relation R1 (degree n+m), and relation R2 degree(m) result of degree n - the attribute we divide on should be on the same domain. - we divide on the attribute (n+i) from R1, and i from R2. - results contains the tuples of R2 that belongs to R1. - result cardinality equals cardinality of R2.","title":"Unit1: Databases and Relational Data Model"},{"location":"knowledge-base/cs2203-database1/unit1/#unit1-databases-and-relational-data-model","text":"Unit1: Databases and Relational Data Model Reading Types of information models Career paths with databases Chapter 2: Relational data model Basic concepts 1.attributes 2. domain 3. Tuples 4. relations 5. Schemas 6. keys 6-1 Candidate keys 6-2 primary keys 6-3 Foreign keys Relational Data Model Constrains Entity integrity constrains Referential integrity constrains In Case of deleting / Updating a tuple that has a FK, there are 3 possibilities: Semantic integrity constrains domain constrains NULL constrains UNIQUE constrains check constrains Relational Algebra","title":"Unit1: Databases and Relational Data Model"},{"location":"knowledge-base/cs2203-database1/unit1/#reading","text":"Database Fundamentals, IBM. chapter 1 & 2 Database design, chapters 1 to 7.","title":"Reading"},{"location":"knowledge-base/cs2203-database1/unit1/#types-of-information-models","text":"Network Model (CODASYL). Hierarchial Model (IMS). Relational. Entity-Relationship (ER). Extended Relational. Semantic. Object-oriented. Object-relational. semi-structured (XML).","title":"Types of information models"},{"location":"knowledge-base/cs2203-database1/unit1/#career-paths-with-databases","text":"data architect. database architect. database administrator (DBA). applications developer.","title":"Career paths with databases"},{"location":"knowledge-base/cs2203-database1/unit1/#chapter-2-relational-data-model","text":"","title":"Chapter 2: Relational data model"},{"location":"knowledge-base/cs2203-database1/unit1/#basic-concepts","text":"","title":"Basic concepts"},{"location":"knowledge-base/cs2203-database1/unit1/#1attributes","text":"represent column in a table,field in the object. data characteristics of the object. all attributes hve values.","title":"1.attributes"},{"location":"knowledge-base/cs2203-database1/unit1/#2-domain","text":"set of all possible values for an attribute. these values MUST be atomic : 1. non-decomposable. 2. smallest possible unit of dat that can not be divided. eg: domain of Boolean attribute is [true, false]. the comparison between the values of 2 attributes is only possible if the 2 attributes have the same domain, otherwise, the comparison does not make sense. domains a re not explicitly stored in the DB, but they MUST be part of the DB definition. each attribute definition MUST include a reference to its domain definition in the DB definition part.","title":"2. domain"},{"location":"knowledge-base/cs2203-database1/unit1/#3-tuples","text":"ordered set of values that represent data in the relation. represents: row in a table, record in a file.","title":"3. Tuples"},{"location":"knowledge-base/cs2203-database1/unit1/#4-relations","text":"relations represent tables. each relation has a header and body. relation header: fixed set of attributes, represent the head of a table, and each of theses attributes corresponds to a domain. relation body: set of tuples represent the table rows, each tuple has value corresponds ot the attribute in the header. relation Degree : number of attribute in the header, eg. unary, binary, nary... relation cardinality : number of tuples in the relation = rows count, changes overtime (with adding/removing records). relation instance : the state of a relation at a specific moment of time. relation properties : there are no duplicate tuples. tuples are unordered. attributes are unordered. attribute values are atomic.","title":"4. relations"},{"location":"knowledge-base/cs2203-database1/unit1/#5-schemas","text":"formal description for all DB relations and relationships between them.","title":"5. Schemas"},{"location":"knowledge-base/cs2203-database1/unit1/#6-keys","text":"keys are identifiers for the tuple. used for: enforce rules and constrains on DB. constrains are important for maintaining consistency and correctness of DB. DBMS are responsible for maintaining keys. keys types : candidate keys. primary keys (PK). foreign keys (FK).","title":"6. keys"},{"location":"knowledge-base/cs2203-database1/unit1/#6-1-candidate-keys","text":"unique identifier for the tuples of a relation, that consists of one or more attributes. if no candidate keys have been specified, the set of k = (all tuple values), might work as a candidate key if and only if: K is unique overtime (no other tuple with the same values). minimality: none of the tuple values can be discarded without destroying the uniqueness property. using Data Definition Language (DDL) you can specify candidate keys by adding UNIQUE keyword to the definition of the attribute. a relation can have multiple candidate keys: one key -> primary key. other keys -> alternate keys.","title":"6-1 Candidate keys"},{"location":"knowledge-base/cs2203-database1/unit1/#6-2-primary-keys","text":"candidate key that has been chosen to represent the relationship. must be: 1. unique. 2. not null. primary key is an attribute that has no meaning in the real life, but it always exist and unique, thus it can be named: surrogate key. artificial key.","title":"6-2 primary keys"},{"location":"knowledge-base/cs2203-database1/unit1/#6-3-foreign-keys","text":"an attribute that references a primary key from another relation. FK and its correspondents PK (from the other relation) MUST have the same domain.","title":"6-3 Foreign keys"},{"location":"knowledge-base/cs2203-database1/unit1/#relational-data-model-constrains","text":"entity integrity constrains. referential integrity constrains. semantic integrity constrains.","title":"Relational Data Model Constrains"},{"location":"knowledge-base/cs2203-database1/unit1/#entity-integrity-constrains","text":"no attribute that participates in PK can have NULL value. in real life: an entity with NULL PK means that this entity does not exist. NULL value means on of the following: absence of value. undefined value. value that does not belong to the attribute domain.","title":"Entity integrity constrains"},{"location":"knowledge-base/cs2203-database1/unit1/#referential-integrity-constrains","text":"for every value of FK in a relation R1, there MUST be a tuple from the other relation R2 so that, PK (R2) = FK (R1). FK with NULL value, means that the relationship between the this tuple and the other relation does not exist.","title":"Referential integrity constrains"},{"location":"knowledge-base/cs2203-database1/unit1/#in-case-of-deleting-updating-a-tuple-that-has-a-fk-there-are-3-possibilities","text":"CASCADE : the operation cascades to the record of the second relations and affects the tuple that has been referenced with FK . RESTRICTS : prevent the operation from happening on all tuples with FK that is not NULL, so the operation is rejected. NULLIFIES : update the record from the second relation that has been referenced by the FK, set the value that points to the affected tuple to be NULL .","title":"In Case of deleting / Updating a tuple that has a FK, there are 3 possibilities:"},{"location":"knowledge-base/cs2203-database1/unit1/#semantic-integrity-constrains","text":"includes: domain constrains. null constrains. unique constrains. check constrains.","title":"Semantic integrity constrains"},{"location":"knowledge-base/cs2203-database1/unit1/#domain-constrains","text":"all values of an attribute MUST belong to its domain. domain constrains include: Format constrains : all values must match a specific pattern, eg. regex, 6 digits... Range Constrains : all values must be in a specific range, eg. number of employees in one department can not exceed the number of employees in the whole company.","title":"domain constrains"},{"location":"knowledge-base/cs2203-database1/unit1/#null-constrains","text":"value can not be null. DEFAULT keyword can be used to give the a attribute a default value in the case of NULL.","title":"NULL constrains"},{"location":"knowledge-base/cs2203-database1/unit1/#unique-constrains","text":"no 2 tuples can have the same value for this attribute. NULL is a valid unique value.","title":"UNIQUE constrains"},{"location":"knowledge-base/cs2203-database1/unit1/#check-constrains","text":"a condition in a relation data that always checked when the data is manipulated. when defining the check constrains you can add instructions that can be executed when the check fails, and if these instructions are not provided, the operation will be rejected.","title":"check constrains"},{"location":"knowledge-base/cs2203-database1/unit1/#relational-algebra","text":"set of operators to manipulate relations. each operator takes one or more relations as INPUT and returns a new relation as OUTPUT . operators are divided into: traditional -------------- special 1. union 1. select 2. intersection 2. project 3. difference 3. join 4. cartesian product 4. divide operator definition of results characteristics UNION - R1 UNION R2 = set of all tuples that belongs to R1, or R2, or both 1. associative. 2. commutative 3. R1, R2 must be union-compatible - INTERSECTION - R1 INTERSECT R2 = set of tuples that belong to Both relations. same as UNION - DIFFERENCE - R1 DIFF R2 = all tuples that belongs to R1, AND NOT belong to R2. 1. R1, R2 must be union-compatible - CARTESIAN PRODUCT - R1 times R2 = set of all tuples where each tuple of the output results 1. R1, R2 must be union-compatible from a concatenation operation between one tuple from R1, and correspondents tuple from R2. - if R1 si of degree n, R2 of degree m => result is of degree n+m - SELECTION - takes one relation AND one condition as input 1. result degree is same as input relation - selects all tuples from the relation that satisfies the input condition. 2. result cardinality is less or equal to the input. - PROJECTION - takes one relation AND list of attributes as input. - returns subset of tuples of a relation with duplicate tuples are eliminated. - JOIN - concatenates 2 relations based on a joining condition or predicate. - theta-join : join result MUST must includes 2 identical attributes (one from each relation), when one attribute is eliminated, it is natural join . when some tuples of 2 relations don't have matching tuple => outer join left outer join : result includes all tuples of R but not all tuples of L right outer join : result includes all tuples of L but not all tuples of R full outer join : result includes all tuples of R AND ALL tuples of L - DIVISION - divides a relation R1 (degree n+m), and relation R2 degree(m) result of degree n - the attribute we divide on should be on the same domain. - we divide on the attribute (n+i) from R1, and i from R2. - results contains the tuples of R2 that belongs to R1. - result cardinality equals cardinality of R2.","title":"Relational Algebra"},{"location":"knowledge-base/cs2203-database1/unit2/","text":"Unit 2: Conceptual Data Model \u00b6 Table of contents \u00b6 Unit 2: Conceptual Data Model Table of contents terminology database model types of database model concepts of database model conceptional data model concepts Entity-Relationship model Entities and entities setts attribute relationship sets constraints Extension Intension Notes Entity-Relationship Modeling Principles relationships one-to-one relationships one-to-many relationships many-to-many relationships Involuted (recursive) relationships arc mistakes in data modeling database normalization References when working with DBs, there are three modeling terms [1] : conceptional modeling: cares about information seen by the business world. logical modeling: based on a mathematical model, presents information in a fully normalized matter where there is no duplication of data. physical modeling: implements a given logical model specifically to a particular DB product or version. terminology \u00b6 model : an abstraction or representation of a real world object/problem that reveals all details of interest to the user. ERD : Entity Relationship Diagram database model \u00b6 integrated collection of concepts for a data description, data relationships, data semantics and constraints. used to represent metadata about the DB and to describe its schema [1] . types of database model \u00b6 External data model : used for viewing representation of every user. also called Universe of Discourse . represents Record-based Logical Model . Conceptional Data Model : used for a general view of the data and it is independent of the DBMS . represents Object-based Logical Model . Internal Data Model : used for a translation of the conceptional model to a specific DBMS. represents Physical Data Model . Relational Database Model : the most used model today. it is simple and theoretically sound. used for building business rules system . represents Entity-Relationship Model based on the conceptional model . concepts of database model \u00b6 structural component : rules for the structure of database itself. manipulation component : defines the operations that can be applied on this database.. data integrity component : rules that guarantee the correctness of data. conceptional data model concepts \u00b6 first step of data modeling, represents a mental image of real-life object/problem. it is not specific to a database, describe things that organization wants to collect and the relationships between them. steps of conceptional data model: draw Entity-Relationship diagram. define integrity constraints. review the final model: remove M:N relationships. remove recursive relationships. remove relationships with attributes. remove 1:1 relationships which are normally not necessary. Entity-Relationship model \u00b6 concepts of Entity-Relationship model: Entity set attribute relationship set constraints attribute domain extension intension Entities and entities setts \u00b6 entity set : is a set of entities of thew same type that share the same properties. entity : is an instance of entity set, it is a self-determining and distinguishable item that can be: concrete. insubstantial. an occurrence. Note: In the Logical Model, entities are called tuples, and entity sets are called relations. . Depending on the context, a given noun like TEACHER could be used as an entity, or an entity set. For example, if you need different types of people, such as TEACHER or STUDENT, you will create an entity set called PERSON with the entities TEACHER and STUDENT [1] . attribute \u00b6 item that describes a property of an entity set, each attribute can have one value for each instance of the entity set. in physical model an attribute is a named column with a domain. Types of attributes: Simple (atomic) : has single component. eg. Boolean has true/false. Composite : consists of multiple components. eg. Name has 2 components, lastname + firstname. Single-Valued : an attribute that has one value for one entity, eg. Title. Multi-Valued : an attribute that has multiple values for one entity, eg. phoneNumber, a person might have multiple phone numbers. since an attribute can have only one value for each instance of the entity set , when encountering multi-valued attribute we need to transfer it to another entity set . Derived : derived attribute has its value derived or computed form another attribute. eg. NumberOfPersons. the derived attribute is not part of a table , but it is included fro clarity or design purposes. Unstable : have values that always change, eg. NumberOfFollowers for a user. Stable : values that rarely change. eg. Name. stable attributes are favourited over unstable ones. Mandatory : must have a value. Optional : might be null. Unique Identifier : ID. key within Logical Model . there are several types of keys: candidate keys . primary keys : they should be: stable. primary key value should never change. minimal. primary key should be composed of the minimal number of fields. alternate keys : all candidate keys that don't participate in the primary key. surrogate key : a primary key that does not exist on real-world attribute, this kind of keys should be avoided since it: increases the data size. does not hold data is important to the entity. simple keys : have a single attribute. Composite keys : keys that has multiple attributes. Foreign keys . relationship sets \u00b6 Relationship set : set of relationships between set of entities. usually a verb . relationship : is an instance of relationship set and establishes an association between entities that are related. constraints \u00b6 types of constraints: cardinalities : based on the number of possible relationship sets for every entity set. can be: one-to-one (1:1) . one-to-many(1:M) . many-to-many(M:M) . these relationships are not supported by the relational model and must be resolved by splitting into two 1:M relationships . participation cardinalities (optionality) : specifies wether the existence of an entity depends on the being related to another entity set via the relationship set. can be: Total or Mandatory : each entity set must participate in a relationship and can not exist without that participation. Partial or Optional : each entity set might participate in a relationship or not. subsets and supersets :When a group of instances has special properties such as attributes or relationship sets that exist only for that group, it makes sense to subdivide an entity set into subsets. The entity set is a superset called a parent. Each group is a subset called a child. eg. An entity set PERSON can be divided in subsets STUDENT and TEACHER (photo below). Hierarchy : represents an ordered set of items. Unary Relationship sets : the same entity set participates multiple times in the same relationship set, also known as recursive relationship set . History : the constraint can specify the end date to always be later than the start date for a given attribute. In the physical model you can use a CHECK constraint for this purpose. Extension \u00b6 Data in a database at a particular point in time is called an extension of the database. Extension refers to the current set of tuples in a relation, it is an instance of the record sets and the relationship sets between them. Intension \u00b6 Intension or schema is the logical model of a database and is represented by entity sets which holds: structure and constraints : An instance describes and constrains the structure of tuples it is allowed to contain. An instance is represented by entity sets, and is the model of a database. manipulation : Data manipulation operations on tuples are allowed only if they observe the expressed intensions of the affected relations. Notes \u00b6 You need also to specify if the relationship set is strong (identifying) or weak (non-identifying) . Weak relationships are connections between a strong entity set and weak entity set. Strong relationships are connections between two strong entities. identifying and non-identifying respectively. An identifying relationship set is selected to specify that the relationship set is one in which one of the child entities is also a dependent entity. Non-Identifying relationship set is selected to specify that the relationship set is one in which both entities are independent. Entity-Relationship Modeling Principles \u00b6 terminology [2] : # ATTRIBUTE_NAME : attribute is part of the primary key. * ATTRIBUTE_NAME : attribute is required. o ATTRIBUTE_NAME : attribute is optional. ___ : has. MAY have zero or more of the other relation. : valid for. MUST belong to one valid tuple of the other relation. relationships \u00b6 one-to-one relationships \u00b6 mandatory-mandatory mandatory-mandatory. the diagram tells that a given PRODUCT MUST come form one and only one SUPPLIER PRODUCT. a given SUPPLIER PRODUCT MUST be the origin for one and only one PRODUCT. this is a deadlock, since we will not able to create a PRODUCT until we have a a PRODUCT SUPPLIER. AND we can not create a PRODUCT SUPPLIER until we have a PRODUCT. mandatory-optional. PRODUCT MAY come form one and only one PRODUCT SUPPLIER, however, a given PRODUCT SUPPLIER MUST be the origin of one and only one PRODUCT. optional-optional. a PRODUCT MAY come one and only one PRODUCT SUPPLIER, a PRODUCT SUPPLIER MAY be the origin of one and only one PRODUCT. one-to-one relationships are a signal that you have different entities that are probably the same entity. one-to-many relationships \u00b6 mandatory-mandatory ORDER MUST belong to one and only one CUSTOMER, CUSTOMER MUST place one or more orders. this also a deadlock, since you can't create CUSTOMER without ORDER or vise versa. mandatory-optional CUSTOMER MUST place one or more ORDERs, while an ORDER MAY belong to a CUSTOMER. this is a hard and rare business rule, and should be avoided. optional-optional this a weak relationship. CUSTOMER MAY place one or more ORDERs, while an ORDER MAY belong to one and only one CUSTOMER. this is called indecisive mode . optional-mandatory most useful relationship. CUSTOMER MAY have zero or more ORDERs, while an ORDER MUST belong to one and only one CUSTOMER. many-to-many relationships \u00b6 mandatory-mandatory this relationship is impossible . or called catch-22 . mandatory-optional needs to be resolved; analysis is unclear. optional-optional this is very useful. it is called intersection entity . NOTE: to resolve the optional-optional many-to-many relationship between CUSTOMER and ORDER, we need to represent a third table that joins the 2 relation. an example of such a relationship between BUSINESS CONTACT and CONTACT TYPE listed below. NOTE : means that the primary keys of both relations contributes to the primary key. so it is a composite primary key . Involuted (recursive) relationships \u00b6 one-to-one, optional-optional = allows for holding historical data. one-to-many, optional-optional = classical hierarchy. many-to-many, optional-optional = network structure, needs a join table . arc \u00b6 arc is a constraint that crosses to one or more relationships going into an entity, and indicates that the relationships in the arc are mutually exclusive. in the photo below, for any TRANSACTION, it MUST be valid for SHAREHOLDER OR a BUSINESS CONTACT, but not both at the same time . try to avoid arcs. mistakes in data modeling \u00b6 modeling with incompleteness. modeling with incomplete understanding of the business. database normalization \u00b6 Database normalization is the process of structuring a database, usually a relational database, in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by Edgar F. Codd as part of his relational model. FN1 (First Normal Form) : do not repeat attributes/groups of attribute, instead take them out to their own entity. final result of the case study: more info here: https://en.wikipedia.org/wiki/Database_normalization cardinality symbols \u00b6 more indo: https://www.smartdraw.com/entity-relationship-diagram References \u00b6 [1] Sharma, N., Perniu, L., Chong, R. F., Iyer, A., Nandan, C., Mitea, A. C., Nonvinkere, M., & Danubianu, M. (2010). Databases fundamentals. Chapter 3 . [2] Pedersen, A. A. (n.d.). Entity relationship modeling - Principles.","title":"Unit 2: Conceptual Data Model"},{"location":"knowledge-base/cs2203-database1/unit2/#unit-2-conceptual-data-model","text":"","title":"Unit 2: Conceptual Data Model"},{"location":"knowledge-base/cs2203-database1/unit2/#table-of-contents","text":"Unit 2: Conceptual Data Model Table of contents terminology database model types of database model concepts of database model conceptional data model concepts Entity-Relationship model Entities and entities setts attribute relationship sets constraints Extension Intension Notes Entity-Relationship Modeling Principles relationships one-to-one relationships one-to-many relationships many-to-many relationships Involuted (recursive) relationships arc mistakes in data modeling database normalization References when working with DBs, there are three modeling terms [1] : conceptional modeling: cares about information seen by the business world. logical modeling: based on a mathematical model, presents information in a fully normalized matter where there is no duplication of data. physical modeling: implements a given logical model specifically to a particular DB product or version.","title":"Table of contents"},{"location":"knowledge-base/cs2203-database1/unit2/#terminology","text":"model : an abstraction or representation of a real world object/problem that reveals all details of interest to the user. ERD : Entity Relationship Diagram","title":"terminology"},{"location":"knowledge-base/cs2203-database1/unit2/#database-model","text":"integrated collection of concepts for a data description, data relationships, data semantics and constraints. used to represent metadata about the DB and to describe its schema [1] .","title":"database model"},{"location":"knowledge-base/cs2203-database1/unit2/#types-of-database-model","text":"External data model : used for viewing representation of every user. also called Universe of Discourse . represents Record-based Logical Model . Conceptional Data Model : used for a general view of the data and it is independent of the DBMS . represents Object-based Logical Model . Internal Data Model : used for a translation of the conceptional model to a specific DBMS. represents Physical Data Model . Relational Database Model : the most used model today. it is simple and theoretically sound. used for building business rules system . represents Entity-Relationship Model based on the conceptional model .","title":"types of database model"},{"location":"knowledge-base/cs2203-database1/unit2/#concepts-of-database-model","text":"structural component : rules for the structure of database itself. manipulation component : defines the operations that can be applied on this database.. data integrity component : rules that guarantee the correctness of data.","title":"concepts of database model"},{"location":"knowledge-base/cs2203-database1/unit2/#conceptional-data-model-concepts","text":"first step of data modeling, represents a mental image of real-life object/problem. it is not specific to a database, describe things that organization wants to collect and the relationships between them. steps of conceptional data model: draw Entity-Relationship diagram. define integrity constraints. review the final model: remove M:N relationships. remove recursive relationships. remove relationships with attributes. remove 1:1 relationships which are normally not necessary.","title":"conceptional data model concepts"},{"location":"knowledge-base/cs2203-database1/unit2/#entity-relationship-model","text":"concepts of Entity-Relationship model: Entity set attribute relationship set constraints attribute domain extension intension","title":"Entity-Relationship model"},{"location":"knowledge-base/cs2203-database1/unit2/#entities-and-entities-setts","text":"entity set : is a set of entities of thew same type that share the same properties. entity : is an instance of entity set, it is a self-determining and distinguishable item that can be: concrete. insubstantial. an occurrence. Note: In the Logical Model, entities are called tuples, and entity sets are called relations. . Depending on the context, a given noun like TEACHER could be used as an entity, or an entity set. For example, if you need different types of people, such as TEACHER or STUDENT, you will create an entity set called PERSON with the entities TEACHER and STUDENT [1] .","title":"Entities and entities setts"},{"location":"knowledge-base/cs2203-database1/unit2/#attribute","text":"item that describes a property of an entity set, each attribute can have one value for each instance of the entity set. in physical model an attribute is a named column with a domain. Types of attributes: Simple (atomic) : has single component. eg. Boolean has true/false. Composite : consists of multiple components. eg. Name has 2 components, lastname + firstname. Single-Valued : an attribute that has one value for one entity, eg. Title. Multi-Valued : an attribute that has multiple values for one entity, eg. phoneNumber, a person might have multiple phone numbers. since an attribute can have only one value for each instance of the entity set , when encountering multi-valued attribute we need to transfer it to another entity set . Derived : derived attribute has its value derived or computed form another attribute. eg. NumberOfPersons. the derived attribute is not part of a table , but it is included fro clarity or design purposes. Unstable : have values that always change, eg. NumberOfFollowers for a user. Stable : values that rarely change. eg. Name. stable attributes are favourited over unstable ones. Mandatory : must have a value. Optional : might be null. Unique Identifier : ID. key within Logical Model . there are several types of keys: candidate keys . primary keys : they should be: stable. primary key value should never change. minimal. primary key should be composed of the minimal number of fields. alternate keys : all candidate keys that don't participate in the primary key. surrogate key : a primary key that does not exist on real-world attribute, this kind of keys should be avoided since it: increases the data size. does not hold data is important to the entity. simple keys : have a single attribute. Composite keys : keys that has multiple attributes. Foreign keys .","title":"attribute"},{"location":"knowledge-base/cs2203-database1/unit2/#relationship-sets","text":"Relationship set : set of relationships between set of entities. usually a verb . relationship : is an instance of relationship set and establishes an association between entities that are related.","title":"relationship sets"},{"location":"knowledge-base/cs2203-database1/unit2/#constraints","text":"types of constraints: cardinalities : based on the number of possible relationship sets for every entity set. can be: one-to-one (1:1) . one-to-many(1:M) . many-to-many(M:M) . these relationships are not supported by the relational model and must be resolved by splitting into two 1:M relationships . participation cardinalities (optionality) : specifies wether the existence of an entity depends on the being related to another entity set via the relationship set. can be: Total or Mandatory : each entity set must participate in a relationship and can not exist without that participation. Partial or Optional : each entity set might participate in a relationship or not. subsets and supersets :When a group of instances has special properties such as attributes or relationship sets that exist only for that group, it makes sense to subdivide an entity set into subsets. The entity set is a superset called a parent. Each group is a subset called a child. eg. An entity set PERSON can be divided in subsets STUDENT and TEACHER (photo below). Hierarchy : represents an ordered set of items. Unary Relationship sets : the same entity set participates multiple times in the same relationship set, also known as recursive relationship set . History : the constraint can specify the end date to always be later than the start date for a given attribute. In the physical model you can use a CHECK constraint for this purpose.","title":"constraints"},{"location":"knowledge-base/cs2203-database1/unit2/#extension","text":"Data in a database at a particular point in time is called an extension of the database. Extension refers to the current set of tuples in a relation, it is an instance of the record sets and the relationship sets between them.","title":"Extension"},{"location":"knowledge-base/cs2203-database1/unit2/#intension","text":"Intension or schema is the logical model of a database and is represented by entity sets which holds: structure and constraints : An instance describes and constrains the structure of tuples it is allowed to contain. An instance is represented by entity sets, and is the model of a database. manipulation : Data manipulation operations on tuples are allowed only if they observe the expressed intensions of the affected relations.","title":"Intension"},{"location":"knowledge-base/cs2203-database1/unit2/#notes","text":"You need also to specify if the relationship set is strong (identifying) or weak (non-identifying) . Weak relationships are connections between a strong entity set and weak entity set. Strong relationships are connections between two strong entities. identifying and non-identifying respectively. An identifying relationship set is selected to specify that the relationship set is one in which one of the child entities is also a dependent entity. Non-Identifying relationship set is selected to specify that the relationship set is one in which both entities are independent.","title":"Notes"},{"location":"knowledge-base/cs2203-database1/unit2/#entity-relationship-modeling-principles","text":"terminology [2] : # ATTRIBUTE_NAME : attribute is part of the primary key. * ATTRIBUTE_NAME : attribute is required. o ATTRIBUTE_NAME : attribute is optional. ___ : has. MAY have zero or more of the other relation. : valid for. MUST belong to one valid tuple of the other relation.","title":"Entity-Relationship Modeling Principles"},{"location":"knowledge-base/cs2203-database1/unit2/#relationships","text":"","title":"relationships"},{"location":"knowledge-base/cs2203-database1/unit2/#one-to-one-relationships","text":"mandatory-mandatory mandatory-mandatory. the diagram tells that a given PRODUCT MUST come form one and only one SUPPLIER PRODUCT. a given SUPPLIER PRODUCT MUST be the origin for one and only one PRODUCT. this is a deadlock, since we will not able to create a PRODUCT until we have a a PRODUCT SUPPLIER. AND we can not create a PRODUCT SUPPLIER until we have a PRODUCT. mandatory-optional. PRODUCT MAY come form one and only one PRODUCT SUPPLIER, however, a given PRODUCT SUPPLIER MUST be the origin of one and only one PRODUCT. optional-optional. a PRODUCT MAY come one and only one PRODUCT SUPPLIER, a PRODUCT SUPPLIER MAY be the origin of one and only one PRODUCT. one-to-one relationships are a signal that you have different entities that are probably the same entity.","title":"one-to-one relationships"},{"location":"knowledge-base/cs2203-database1/unit2/#one-to-many-relationships","text":"mandatory-mandatory ORDER MUST belong to one and only one CUSTOMER, CUSTOMER MUST place one or more orders. this also a deadlock, since you can't create CUSTOMER without ORDER or vise versa. mandatory-optional CUSTOMER MUST place one or more ORDERs, while an ORDER MAY belong to a CUSTOMER. this is a hard and rare business rule, and should be avoided. optional-optional this a weak relationship. CUSTOMER MAY place one or more ORDERs, while an ORDER MAY belong to one and only one CUSTOMER. this is called indecisive mode . optional-mandatory most useful relationship. CUSTOMER MAY have zero or more ORDERs, while an ORDER MUST belong to one and only one CUSTOMER.","title":"one-to-many relationships"},{"location":"knowledge-base/cs2203-database1/unit2/#many-to-many-relationships","text":"mandatory-mandatory this relationship is impossible . or called catch-22 . mandatory-optional needs to be resolved; analysis is unclear. optional-optional this is very useful. it is called intersection entity . NOTE: to resolve the optional-optional many-to-many relationship between CUSTOMER and ORDER, we need to represent a third table that joins the 2 relation. an example of such a relationship between BUSINESS CONTACT and CONTACT TYPE listed below. NOTE : means that the primary keys of both relations contributes to the primary key. so it is a composite primary key .","title":"many-to-many relationships"},{"location":"knowledge-base/cs2203-database1/unit2/#involuted-recursive-relationships","text":"one-to-one, optional-optional = allows for holding historical data. one-to-many, optional-optional = classical hierarchy. many-to-many, optional-optional = network structure, needs a join table .","title":"Involuted (recursive) relationships"},{"location":"knowledge-base/cs2203-database1/unit2/#arc","text":"arc is a constraint that crosses to one or more relationships going into an entity, and indicates that the relationships in the arc are mutually exclusive. in the photo below, for any TRANSACTION, it MUST be valid for SHAREHOLDER OR a BUSINESS CONTACT, but not both at the same time . try to avoid arcs.","title":"arc"},{"location":"knowledge-base/cs2203-database1/unit2/#mistakes-in-data-modeling","text":"modeling with incompleteness. modeling with incomplete understanding of the business.","title":"mistakes in data modeling"},{"location":"knowledge-base/cs2203-database1/unit2/#database-normalization","text":"Database normalization is the process of structuring a database, usually a relational database, in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by Edgar F. Codd as part of his relational model. FN1 (First Normal Form) : do not repeat attributes/groups of attribute, instead take them out to their own entity. final result of the case study: more info here: https://en.wikipedia.org/wiki/Database_normalization","title":"database normalization"},{"location":"knowledge-base/cs2203-database1/unit2/#cardinality-symbols","text":"more indo: https://www.smartdraw.com/entity-relationship-diagram","title":"cardinality symbols"},{"location":"knowledge-base/cs2203-database1/unit2/#references","text":"[1] Sharma, N., Perniu, L., Chong, R. F., Iyer, A., Nandan, C., Mitea, A. C., Nonvinkere, M., & Danubianu, M. (2010). Databases fundamentals. Chapter 3 . [2] Pedersen, A. A. (n.d.). Entity relationship modeling - Principles.","title":"References"},{"location":"knowledge-base/cs2203-database1/unit3/","text":"Unit 3: Relational Database Design \u00b6 data redundancy \u00b6 redundancy means the existence of same data in various places. redundancy in a relational schema is not optimal because it interrupts and insertion, deletion, update ops, and causes: Insertion Anomalies : insertion of a data record is not possible unless we get some unrelated data from another record. eg. inserting a student record requires data to be retrieved from his college record. Deletion Anomalies : deletion of a data record results in losing some unrelated information that was stored as part of the record that was deleted from a table. Update Anomalies : updating data for an entity in one place may lead to inconsistency, with the existing redundant data in another place in the table. decompositions \u00b6 decompositions in relational database design is breaking down a relational schema into smaller and simpler relations that avoid redundancy. we will be able to query the smaller relations for any information that we were previously able to retrieve from the original relational schema. functional dependencies \u00b6 Functional Dependency ( FD ) is a type of integrity constraint that extends the idea of a super key . It defines a dependency between subsets of attributes of a given relation. Functional Dependency can be understood as \u201cA determines B\u201d, \u201cB is dependent on A\u201d or \u201cA implies B\u201d and denoted as \u201cA \u2192 B\u201d, where A and B are 2 different subsets of attributes in a relation. Trivial Functional Dependencies : A functional dependency that holds true for all values of a given attribute. Closure Set of Functional Dependencies S + \u00b6 all functional dependencies that are implied from a given set of functional dependency S. rules to compute closure set of dependencies: Armstrong's Axiom : also known as \"Inference Rules\" that help infer all the implied functional dependencies from a given set of functional dependencies. it includes 3 rules: Reflexivity : If B is a subset of attributes in set A, then A \u2192 B. (by trivial FD) Augmentation : If A \u2192 B and C is another attribute, then AC \u2192 BC Applying reflexivity to this rule, we can also say that, AC \u2192 B. Transitivity : If A \u2192 B and B \u2192 C, then A \u2192 C. and also 2 additional rules that can be extracted from the first 3: Union : If A \u2192 B and A \u2192 C, then A \u2192 BC. Decomposition : If A \u2192 BC, then A \u2192 B and A \u2192 C. Computing the closure set of attributes : Closure set of attributes of a given attribute, A, is set of all attributes in the relation, R that can be uniquely determined by A, based on the given FDs. Given a relation, R with a set of attributes, we calculate closure set of attributes for A, closure (A) as follows: Initially set closure (A) = A For each given FD, if A \u2192 B, then add B to closure (A), that is, closure (A) U B For any subset of A, (let C be a subset of A), A\u2192 C (by trivial FD) and if C \u2192 D such that D is not a subset of A, then add D to the closure (A) Repeat step 3 until there are no more attribute sets to be added to closure (A) Entailment : Functional Dependencies (FDs) guide us on how to best decompose relations so that the dependent values may be stored in a single table. Normal Forms \u00b6 Normalization is a procedure in relational database design that aims at converting relational schemas into a more desirable form. The goal is to remove redundancy in relations and the problems that follow from it, namely insertion, deletion and update anomalies The Normal forms progress towards obtaining an optimal design. Normalization is a step-wise process, where each step transforms the relational schemas into a higher normal form. Each Normal form contains all the previous normal forms and some additional optimization over them. First Normal Form (1NF) \u00b6 all attribute domains are atomic. the idea is no repeating groups . a relation is considered to be in 1NF if satisfies these 5 conditions: no top-to-bottom ordering of rows. no left-to-right ordering of columns. no duplicate rows every row-and-column intersection contains exactly one value from the applicable domain. all columns are regular, [i.e. rows have no hidden components such as row IDs, object IDs, or hidden timestamps]. Second Normal Form (2NF) \u00b6 1NF AND no non-key attribute that depends on part of the candidate key, but on the entire candidate key. relation has single attribute as its candidate key. Third Normal Form (3NF) \u00b6 1NF AND 2NF AND no non-key attribute that depends transitively on the candidate key. every attribute depends directly on the primary key and not through a transitive relation, where an attribute Z may depend on a non-key attribute Y and Y in turn depends on the primary key X.Transitivity means that when X\u2192Y and Y\u2192 Z, then X\u2192Z. non-key attributes are mutually independent Boyce-Codd Normal Form (BCNF) \u00b6 applies to relations where there may be overlapping candidate keys. A relation is said to be in Boyce-Codd normal form if it is in 3NF and every non-trivial FD given for this relation has a candidate key as its determinant. That is, for every X \u2192 Y, X is a candidate key. Properties of Decompositions \u00b6 properties in summary: decompositions should be lossless. Lossless and Lossy Decompositions \u00b6 Decomposition of a relation R into relations X and Y is lossless if no information from the original relation is lost after the decomposition. In other words, the original relation can be constructed back from the decomposed relations and no spurious rows of information are added as data to the resulting rows. lossy decompositions are bad. if the common attributes in the decomposed (new) relations form a super key for either of the new relations, then the decomposition is lossless. Dependency-Preserving Decompositions \u00b6 we can check all the constraints against the decomposed table only, we need not check it against the original table . Dependency-preserving decomposition does not imply a lossless join decomposition and vice versa. While lossless join is a must during decomposition, dependency-preservation may not be achieved every time. Minimal Cover \u00b6 Minimal cover, Fc is the smallest set of functional dependencies such that the closure set of function dependencies for both minimal cover and given set of FDs F for a relation R are equivalent. That is, F+ = Fc+ Minimal Cover for a given set of FDs is not unique. Synthesis of 3NF schemas \u00b6 Synthesis of 3NF schemas is a bottom-up approach to build lossless join and dependency preserving decompositions of a relation into 3NF. 3NF decomposition \u00b6 A 3NF decomposition can be achieved using a top-down method by the process of normalization where we decompose a relation as per the definition of 3NF and then ensure that it is lossless and dependency-preserving by performing certain checks on the relations thus created. The Fourth Normal Form (4NF) \u00b6 The fourth normal form can be understood in terms of multi-valued dependencies. The Fourth Normal Form (4NF) for a relational schema is said to exist when the non-related multi-valued dependencies that exist are not more than one in the given relation. Multi-valued Dependency (MVD) is denoted as, A \u2192\u2192 B. This means that A multi-determines B, B is multi-dependent on A or A double arrow B. Other normal forms \u00b6 fifth-normal form. domain key normal form DKNF. sixth-normal form","title":"Unit 3: Relational Database Design"},{"location":"knowledge-base/cs2203-database1/unit3/#unit-3-relational-database-design","text":"","title":"Unit 3: Relational Database Design"},{"location":"knowledge-base/cs2203-database1/unit3/#data-redundancy","text":"redundancy means the existence of same data in various places. redundancy in a relational schema is not optimal because it interrupts and insertion, deletion, update ops, and causes: Insertion Anomalies : insertion of a data record is not possible unless we get some unrelated data from another record. eg. inserting a student record requires data to be retrieved from his college record. Deletion Anomalies : deletion of a data record results in losing some unrelated information that was stored as part of the record that was deleted from a table. Update Anomalies : updating data for an entity in one place may lead to inconsistency, with the existing redundant data in another place in the table.","title":"data redundancy"},{"location":"knowledge-base/cs2203-database1/unit3/#decompositions","text":"decompositions in relational database design is breaking down a relational schema into smaller and simpler relations that avoid redundancy. we will be able to query the smaller relations for any information that we were previously able to retrieve from the original relational schema.","title":"decompositions"},{"location":"knowledge-base/cs2203-database1/unit3/#functional-dependencies","text":"Functional Dependency ( FD ) is a type of integrity constraint that extends the idea of a super key . It defines a dependency between subsets of attributes of a given relation. Functional Dependency can be understood as \u201cA determines B\u201d, \u201cB is dependent on A\u201d or \u201cA implies B\u201d and denoted as \u201cA \u2192 B\u201d, where A and B are 2 different subsets of attributes in a relation. Trivial Functional Dependencies : A functional dependency that holds true for all values of a given attribute.","title":"functional dependencies"},{"location":"knowledge-base/cs2203-database1/unit3/#closure-set-of-functional-dependencies-s","text":"all functional dependencies that are implied from a given set of functional dependency S. rules to compute closure set of dependencies: Armstrong's Axiom : also known as \"Inference Rules\" that help infer all the implied functional dependencies from a given set of functional dependencies. it includes 3 rules: Reflexivity : If B is a subset of attributes in set A, then A \u2192 B. (by trivial FD) Augmentation : If A \u2192 B and C is another attribute, then AC \u2192 BC Applying reflexivity to this rule, we can also say that, AC \u2192 B. Transitivity : If A \u2192 B and B \u2192 C, then A \u2192 C. and also 2 additional rules that can be extracted from the first 3: Union : If A \u2192 B and A \u2192 C, then A \u2192 BC. Decomposition : If A \u2192 BC, then A \u2192 B and A \u2192 C. Computing the closure set of attributes : Closure set of attributes of a given attribute, A, is set of all attributes in the relation, R that can be uniquely determined by A, based on the given FDs. Given a relation, R with a set of attributes, we calculate closure set of attributes for A, closure (A) as follows: Initially set closure (A) = A For each given FD, if A \u2192 B, then add B to closure (A), that is, closure (A) U B For any subset of A, (let C be a subset of A), A\u2192 C (by trivial FD) and if C \u2192 D such that D is not a subset of A, then add D to the closure (A) Repeat step 3 until there are no more attribute sets to be added to closure (A) Entailment : Functional Dependencies (FDs) guide us on how to best decompose relations so that the dependent values may be stored in a single table.","title":"Closure Set of Functional Dependencies S+"},{"location":"knowledge-base/cs2203-database1/unit3/#normal-forms","text":"Normalization is a procedure in relational database design that aims at converting relational schemas into a more desirable form. The goal is to remove redundancy in relations and the problems that follow from it, namely insertion, deletion and update anomalies The Normal forms progress towards obtaining an optimal design. Normalization is a step-wise process, where each step transforms the relational schemas into a higher normal form. Each Normal form contains all the previous normal forms and some additional optimization over them.","title":"Normal Forms"},{"location":"knowledge-base/cs2203-database1/unit3/#first-normal-form-1nf","text":"all attribute domains are atomic. the idea is no repeating groups . a relation is considered to be in 1NF if satisfies these 5 conditions: no top-to-bottom ordering of rows. no left-to-right ordering of columns. no duplicate rows every row-and-column intersection contains exactly one value from the applicable domain. all columns are regular, [i.e. rows have no hidden components such as row IDs, object IDs, or hidden timestamps].","title":"First Normal Form (1NF)"},{"location":"knowledge-base/cs2203-database1/unit3/#second-normal-form-2nf","text":"1NF AND no non-key attribute that depends on part of the candidate key, but on the entire candidate key. relation has single attribute as its candidate key.","title":"Second Normal Form (2NF)"},{"location":"knowledge-base/cs2203-database1/unit3/#third-normal-form-3nf","text":"1NF AND 2NF AND no non-key attribute that depends transitively on the candidate key. every attribute depends directly on the primary key and not through a transitive relation, where an attribute Z may depend on a non-key attribute Y and Y in turn depends on the primary key X.Transitivity means that when X\u2192Y and Y\u2192 Z, then X\u2192Z. non-key attributes are mutually independent","title":"Third Normal Form (3NF)"},{"location":"knowledge-base/cs2203-database1/unit3/#boyce-codd-normal-form-bcnf","text":"applies to relations where there may be overlapping candidate keys. A relation is said to be in Boyce-Codd normal form if it is in 3NF and every non-trivial FD given for this relation has a candidate key as its determinant. That is, for every X \u2192 Y, X is a candidate key.","title":"Boyce-Codd Normal Form (BCNF)"},{"location":"knowledge-base/cs2203-database1/unit3/#properties-of-decompositions","text":"properties in summary: decompositions should be lossless.","title":"Properties of Decompositions"},{"location":"knowledge-base/cs2203-database1/unit3/#lossless-and-lossy-decompositions","text":"Decomposition of a relation R into relations X and Y is lossless if no information from the original relation is lost after the decomposition. In other words, the original relation can be constructed back from the decomposed relations and no spurious rows of information are added as data to the resulting rows. lossy decompositions are bad. if the common attributes in the decomposed (new) relations form a super key for either of the new relations, then the decomposition is lossless.","title":"Lossless and Lossy Decompositions"},{"location":"knowledge-base/cs2203-database1/unit3/#dependency-preserving-decompositions","text":"we can check all the constraints against the decomposed table only, we need not check it against the original table . Dependency-preserving decomposition does not imply a lossless join decomposition and vice versa. While lossless join is a must during decomposition, dependency-preservation may not be achieved every time.","title":"Dependency-Preserving Decompositions"},{"location":"knowledge-base/cs2203-database1/unit3/#minimal-cover","text":"Minimal cover, Fc is the smallest set of functional dependencies such that the closure set of function dependencies for both minimal cover and given set of FDs F for a relation R are equivalent. That is, F+ = Fc+ Minimal Cover for a given set of FDs is not unique.","title":"Minimal Cover"},{"location":"knowledge-base/cs2203-database1/unit3/#synthesis-of-3nf-schemas","text":"Synthesis of 3NF schemas is a bottom-up approach to build lossless join and dependency preserving decompositions of a relation into 3NF.","title":"Synthesis of 3NF schemas"},{"location":"knowledge-base/cs2203-database1/unit3/#3nf-decomposition","text":"A 3NF decomposition can be achieved using a top-down method by the process of normalization where we decompose a relation as per the definition of 3NF and then ensure that it is lossless and dependency-preserving by performing certain checks on the relations thus created.","title":"3NF decomposition"},{"location":"knowledge-base/cs2203-database1/unit3/#the-fourth-normal-form-4nf","text":"The fourth normal form can be understood in terms of multi-valued dependencies. The Fourth Normal Form (4NF) for a relational schema is said to exist when the non-related multi-valued dependencies that exist are not more than one in the given relation. Multi-valued Dependency (MVD) is denoted as, A \u2192\u2192 B. This means that A multi-determines B, B is multi-dependent on A or A double arrow B.","title":"The Fourth Normal Form (4NF)"},{"location":"knowledge-base/cs2203-database1/unit3/#other-normal-forms","text":"fifth-normal form. domain key normal form DKNF. sixth-normal form","title":"Other normal forms"},{"location":"knowledge-base/cs2203-database1/unit4/","text":"Unit 4: Introduction to SQL \u00b6 table of contents \u00b6 Unit 4: Introduction to SQL table of contents Introduction [[1]](#references) useful definitions History of SQL Defining a relational database schema in SQL Data manipulation with SQL SQL joins Union, intersection, and difference operations Relational operators grouping operators Aggregation operators HAVING Clause Sub-queries Sub-queries returning a scalar value Sub-queries returning vector values Correlated sub-query Sub-query in FROM Clauses map OOP concepts to relational database concepts A rough guide to SQL [[2]](#references) SQL Standards Support [[3]](#references) Short Guide to Data Types References Introduction [1] \u00b6 there 3 categories of statements in the SQL language: Data Definition Language (DDL) : used to create,alter, manipulate the objects within the DB. Data Manipulation Language (DML) : used to read and manipulate data within the DB. most used in applications. Data Control Language (DCL) : used to control access to the data within the DB. concerns about the security of the database. useful definitions \u00b6 Authentication : a user is who they say they are because they have the appropriate password. Authorization : restricts what a valid user (authenticated user) has access to and what kind of access they have. History of SQL \u00b6 created by Don Chamberlin and Ray Boyce from IBM in the 1970's as part of the System R project which meant to provide a practical implementation to Codd's relational model. SQL was adopted as a standard language in 1986 by the American National Standards Institute (ANSI) and by the International Standards Organization (ISO) in 1987. Defining a relational database schema in SQL \u00b6 SQL represents the physical data model of the schema (conceptual data model) A table whose column values depend on the values of other tables is called dependant , or child table and a table that is being referenced is called the base or parent table Referential integrity can be defined during table definition or after the table has been created define foregin key syntax Syntax 1 : CREATE TABLE DEPENDANT_TABLE ( ID INTEGER REFERENCES BASE_TABLE ( UNIQUE_OR_PRIMARY_KEY ), NAME VARCHAR ( 9 )); Syntax 2 : CREATE TABLE DEPENDANT_TABLE ( ID INTEGER , NAME VARCHAR ( 9 ), CONSTRAINT constraint_name FOREIGN KEY ( ID ) REFERENCES BASE_TABLE ( UNIQUE_OR_PRIMARY_KEY ) ); Syntax 3 : CREATE TABLE DEPENDANT_TABLE ( ID INTEGER , NAME VARCHAR ( 9 ) ); ALTER TABLE DEPENDANT_TABLE ADD CONSTRAINT constraint_name FOREIGN KEY ( ID ) REFERENCES BASE_TABLE ( UNIQUE_OR_PRIMARY_KEY ) ON DELETE < delete_action_type > ON UPDATE < update_action_type > ; A delete action type can be a CASCADE , SET NULL , NO ACTION , or RESTRICT . An update action type can be a NO ACTION , or RESTRICT create a table with schema create schema mySchema create table mySchema . myTable ( col1 integer ) view : is a virtual table derived from one or more tables or other views. It is virtual because it does not contain any data, but a definition of a table based on the result of a SELECT statement. Views allow you to hide data or limit access to a select number of columns; therefore, they can also be used for security purposes. CREATE VIEW MYVIEW AS SELECT LASTNAME , HIREDATE FROM EMPLOYEE SELECT * FROM MYVIEW other database objects: 1. indexes 2. functions 3. procedures 4. triggers 5. ..etc rename DB object: RENAME < object type > < object name > to < new name > ALTER TABLE < table name > RENAME COLUMN < column name > TO < new name > Data manipulation with SQL \u00b6 A SELECT statement returns its result set in no particular order executing the same SELECT statement multiple times will generate same rows but in different order, adding ORDER BY clause will return the same order every times SELECT col1 FROM myTable ORDER BY col1 DESC cursor : is a result set holding the result of a SELECT statement. The syntax to declare, open, fetch, and close DECLARE < cursor name > CURSOR [ WITH RETURN < return target > ] < SELECT statement > ; OPEN < cursor name > ; FETCH < cursor name > INTO < variables > ; CLOSE < cursor name > ; Rather than returning all the rows of an SQL statement to an application at once, a cursor allows the application to process rows one at a time. Using FETCH statements within a loop in the application, developers can navigate through each row pointed by the cursor and apply some logic to the row or based on the row contents. For example, the following code snippet sums all the salaries of employees using a cursor DECLARE p_sum INTEGER ; DECLARE p_sal INTEGER ; DECLARE c CURSOR FOR SELECT SALARY FROM EMPLOYEE ; DECLARE SQLSTATE CHAR ( 5 ) DEFAULT '00000' ; SET p_sum = 0 ; OPEN c ; FETCH FROM c INTO p_sal ; WHILE ( SQLSTATE = '00000' ) DO SET p_sum = p_sum + p_sal ; FETCH FROM c INTO p_sal ; END WHILE ; CLOSE c ; copy the results of a table into another table, you need to be careful for the attributes order though: insert into myTable ( select * from myTable2 ) delete statement: DELETE FROM myTable WHERE col1 > 1000 update statement: UPDATE myTable SET col1 = - 1 , col2 = \u2018 a \u2019 , col3 = \u2018 2010 - 01 - 01 \u2019 WHERE col4 = \u2018 0 \u2019 ; SQL joins \u00b6 inner joins: Equi-join, Natural join, Cross join outer joins: Left outer join, Right outer join, Full outer join type definitions Equal join (equi) two tables are joined based on the equality of specified columns natural join improved version of an equi-join where the joining column does not require specification. The system automatically selects the column with same name in the tables and applies the equality operation on it and remove all duplicate attributes. ambiguous, not liked by most DBs. cross join Cartesian product of the tables to be joined left outer join the result set is a union of the results of an equi-join, including any non-matching rows from the LEFT table right outer join the union of results of an equi-join, including any non-matching rows from the RIGHT table. full outer join the result set is the union of results of an equi- join, including any non-matching rows of the LEFT and the RIGHT table. examples: -- Example 1 (equi join): SELECT * FROM student , enrollment WHERE student . enrollment_no = enrollment . enrollment_no -- Example 2 (equi join): SELECT * FROM student INNER JOIN enrollment ON student . enrollment_no = enrollment . enrollment_no -- example 3 (natural join): SELECT * FROM STUDENT NATURAL JOIN ENROLLMENT -- Example 4 (cross join): SELECT * FROM STUDENT , ENROLLMENT -- Example 5 (left outer join): SELECT * FROM STUDENT LEFT OUTER JOIN ENROLLMENT ON STUDENT . ENROLLMENT_NO = ENROLLMENT_NO -- Example 6 (right outer join): SELECT * FROM STUDENT RIGHT OUTER JOIN ENROLLMENT ON STUDENT . ENROLLMENT_NO = ENROLLMENT_NO -- Example 7 (full outer join ): SELECT * FROM STUDENT FULL OUTER JOIN ENROLLMENT ON STUDENT . ENROLLMENT_NO = ENROLLMENT_NO Union, intersection, and difference operations \u00b6 operation definition union join two data sets having the same column definitions and in the same order. removes any duplicate rows. intersection returns a result set common to both data sets difference (EXCEPT) returns the result set that exists only in the RIGHT or First table as A EXCEPT B = A MINUS [A INTERSECT B] examples -- Example 1 (union): SELECT * FROM student_table_a UNION SELECT * from student_table_b -- Example 2 (union with duplicates): SELECT * FROM student_table_a UNION ALL -- duplicate rows will stay in the result SELECT * from student_table_b -- Example 3 (intersection): select * from student_table_a INTERSECT select * from student_table_b -- Example 4 (intersection with duplicates): select * from student_table_a INTERSECT ALL -- duplicate rows will stay in the result select * from student_table_b Relational operators \u00b6 Basic mathematical operations like \u2018+\u2019, \u2018-\u2018, \u2018*\u2019 and \u2018/\u2019 Logical operators like \u2018AND\u2019, \u2018OR\u2019 and \u2018NOT\u2019 String manipulation operators like \u2018CONCATENATE\u2019, \u2018LENGTH\u2019, \u2018SUBSTRING\u2019 Comparative operators like \u2018=\u2019, \u2018<\u2019, \u2018>\u2019, \u2018>=\u2019, \u2018<=\u2019 and \u2018!=\u2019 Grouping and aggregate operators Other miscellaneous operations like DISTINCT grouping operators \u00b6 perform operations on two or more rows of data, and provide a summarized output result set select course_enrolled , count ( * ) from students_enrollment group by course_enrolled and the result from the above statement: COURSE_ENROLLED STUDENT_COUNT English 10 Maths 20 Physics 40 Aggregation operators \u00b6 Operators, which perform on two or more tuples or rows, and return a scalar result set, are called aggregate operators. Examples include: COUNT, SUM, AVERAGE, MINIMUM, MAXIMUM , and so on. HAVING Clause \u00b6 can be used only with a GROUP BY clause to filter the desired rows in grouped data. basically, it is a WHERE clause but for Grouped data , since the WHERE clause can not work on grouped sets. Sub-queries \u00b6 When a query is applied within a query, the outer query is referred to as the main query or parent query and the internal query is referred as the sub-query or inner query sub query may return a scalar value, single or multiple tuples, or a NULL data set Sub-queries are executed first, and then the parent query is executed utilizing data returned by the sub-queries. Sub-queries returning a scalar value \u00b6 Scalar values represent a single value of any attribute or entity, for example Name, Age. The example below returns a list of students who are the youngest among all students. The sub-query \u201cSELECT min(age) FROM students\u201d returns a scalar value that indicates the minimum age among all students. The parent query returns a list of all students whose age is equal to the value returned by the sub-query. SELECT name FROM students_enrollment WHERE age = ( SELECT min ( age ) FROM students ); Sub-queries returning vector values \u00b6 When a sub-query returns a data set that represents multiple values for a column (like a list of names) or array of values for multiple columns (like Name, age and date of birth for all students), then the sub-query is said to be returning vector values. the example will get a list of students who are enrolled in courses offered by the computer science department: SELECT name FROM students WHERE course_enrolled IN ( SELECT distinct course_name FROM courses WHERE department_name = \u2018 Computer Science \u2019 ) sub-query returns a list of all courses that are offered in the \u201cComputer Science\u201d department and the outer query lists all students enrolled in the courses of the sub-query result set. Correlated sub-query \u00b6 When a sub-query is executed for each row of the parent table, instead of once then the sub-query is referred to as a correlated sub-query. the example below searches for a list of students with who have been awarded maximum marks in each department. For each row on the LEFT table, the sub-query finds max(marks) for the department of the current row and if the values of marks in the current row is equal to the sub-query result set, then it is added to the outer query result set. SELECT dept , name , marks FROM final_result a WHERE marks = ( SELECT max ( marks ) FROM final_result WHERE dept = a . dept ) Sub-query in FROM Clauses \u00b6 A sub-query can be used in a FROM clause as well. SELECT dept , max_marks , min_marks , avg_marks FROM ( SELECT dept , max ( marks ) as max_marks , min ( marks ) as min_marks , avg ( marks ) as avg_marks FROM final_result GROUP BY dept ) WHERE ( max_marks \u2013 min_marks ) > 50 and avg_marks < 50 The above query uses a sub-query in the FROM clause. The sub-query returns maximum, minimum and average marks for each department. The outer query uses this data and filters the data further by adding filter conditions in the WHERE clause of the outer query map OOP concepts to relational database concepts \u00b6 Object-relational mapping (ORM) libraries such as Hibernate , pureQuery are popular to provide a framework for this mapping between the object-oriented world and the relational world. The table below shows the correspondence between conceptual, logical and physical model concepts A rough guide to SQL [2] \u00b6 data stored in the DB in a way that can not be read by humans that's why we need DBMS. in early days of DBs the network and hierarchy data models were the most popular. SQL is a relational database language, but it is not a DBMS , instead it is a way to communicate with the DBMS. SQL is non procedural database language, which means that when executing a a SQL command you don't need to run,compile a program for each query or define the location of data, DBMS will take care of that. interactive SQL can be run in the command line and output results to the console. programmatic SQL where SQL commands are embedded in a host language (COBOL, C.SQL). SQL needs a host language cause it is not a complete programming language , it has no branching or looping functionalities, so it relays on the host language to allow this. VIEWS are virtual tables, creating a view will create a new table in the database based on select statement provided when creating the view. the view then gets updated when any updates happen on the original table(s). when a new DB is created it is owned by the user who created it and have all privileges over it. SQL Standards Support [3] \u00b6 SQL SECTION COMMANDS Data Definition Language (DDL) CREATE, ALTER, DROP, GRANT, REVOKE, COMMENT ON, EXPLAIN REFERENCES, DECLARE Data Manipulation Language (DML) INSERT, UPDATE, DELETE, TRUNCATE, MERGE Data Query Language (DQL) SELECT, VALUES, WITH, EXPLAIN PLAN General operations on DB BACKUP, PERFORM, SCRIPS, CHECKPOINT, SHUTDOWN transaction statements START TRANSACTION, SET TRANSACTION, COMMIT, ROLLBACK, SAVEPOINT, RELEASE SAVEPOINT, LOCK, CONNECT, DISCONNECT Short Guide to Data Types \u00b6 Numeric types TINYINT, SMALLINT, INTEGER and BIGINT with fixed binary precision, NUMERIC and DECIMAL are types with user-defined decimal precision. DOUBLE type is a 64-bit. BOOLEAN type is for logical values and can hold TRUE, FALSE or UNKNOWN . Character string types are CHAR(L), VARCHAR(L) and CLOB , type comments TINYINT, SMALLINT, INTEGER and BIGINT fixed binary precision NUMERIC and DECIMAL user-defined decimal precision DOUBLE 64-bit CHAR(L) fixed length strings, if you provide shorter strings, spaces will be added. if no L provided its a single char VARCHAR(L) general strings, performance issues appear for long strings with this type, avoid in large strings (> 10* KB). CLOB for large strings, avoid for short strings LONGCHAR a synonym for a long VARCHAR and can be used without specifying the size BINARY(L) fixed length strings such as UUID, e pads short binary strings with zero bytes, if no L it is a single byte VARBINARY(L) general binary strings BLOB(L) for large binary objects LONGVARBINARY used as VARBINARY without specifying the size, can be mapped to BLOB instead. BIT(L) and BITVARYING(L) avoid use without specifying L, use BOOLEAN for single bit. UUID stored as BINARY. UUID and BINARY strings, can be used to insert or to compare. DATE, TIME, and TIMESTAMP, TIME ZONE INTERVAL used with datetime, not widely supported OTHER storage of json objects, if object is large, serialize and save as BLOB. ARRAY support all types except OTHER, BLOB References \u00b6 [1] Sharma, N., Perniu, L., Chong, R. F., Iyer, A., Nandan, C., Mitea, A. C., Nonvinkere, M., & Danubianu, M. (2010). Databases fundamentals. chapter 5. [2] Din, A. I. (2006). Structured Query Language (SQL): A practical introduction. NCC Blackwell. chapters 1 & 2. Retrieved from http://www.managedtime.com/freesqlbook.php . [3] Simpson, B., Toussi, F, & The HSQL Development Group. (2019, June 2). HyperSQL user guide. Chapter 2. Retrieved from http://hsqldb.org/doc/2.0/guide/sqlgeneral-chapt.html","title":"Unit 4: Introduction to SQL"},{"location":"knowledge-base/cs2203-database1/unit4/#unit-4-introduction-to-sql","text":"","title":"Unit 4: Introduction to SQL"},{"location":"knowledge-base/cs2203-database1/unit4/#table-of-contents","text":"Unit 4: Introduction to SQL table of contents Introduction [[1]](#references) useful definitions History of SQL Defining a relational database schema in SQL Data manipulation with SQL SQL joins Union, intersection, and difference operations Relational operators grouping operators Aggregation operators HAVING Clause Sub-queries Sub-queries returning a scalar value Sub-queries returning vector values Correlated sub-query Sub-query in FROM Clauses map OOP concepts to relational database concepts A rough guide to SQL [[2]](#references) SQL Standards Support [[3]](#references) Short Guide to Data Types References","title":"table of contents"},{"location":"knowledge-base/cs2203-database1/unit4/#introduction-1","text":"there 3 categories of statements in the SQL language: Data Definition Language (DDL) : used to create,alter, manipulate the objects within the DB. Data Manipulation Language (DML) : used to read and manipulate data within the DB. most used in applications. Data Control Language (DCL) : used to control access to the data within the DB. concerns about the security of the database.","title":"Introduction [1]"},{"location":"knowledge-base/cs2203-database1/unit4/#useful-definitions","text":"Authentication : a user is who they say they are because they have the appropriate password. Authorization : restricts what a valid user (authenticated user) has access to and what kind of access they have.","title":"useful definitions"},{"location":"knowledge-base/cs2203-database1/unit4/#history-of-sql","text":"created by Don Chamberlin and Ray Boyce from IBM in the 1970's as part of the System R project which meant to provide a practical implementation to Codd's relational model. SQL was adopted as a standard language in 1986 by the American National Standards Institute (ANSI) and by the International Standards Organization (ISO) in 1987.","title":"History of SQL"},{"location":"knowledge-base/cs2203-database1/unit4/#defining-a-relational-database-schema-in-sql","text":"SQL represents the physical data model of the schema (conceptual data model) A table whose column values depend on the values of other tables is called dependant , or child table and a table that is being referenced is called the base or parent table Referential integrity can be defined during table definition or after the table has been created define foregin key syntax Syntax 1 : CREATE TABLE DEPENDANT_TABLE ( ID INTEGER REFERENCES BASE_TABLE ( UNIQUE_OR_PRIMARY_KEY ), NAME VARCHAR ( 9 )); Syntax 2 : CREATE TABLE DEPENDANT_TABLE ( ID INTEGER , NAME VARCHAR ( 9 ), CONSTRAINT constraint_name FOREIGN KEY ( ID ) REFERENCES BASE_TABLE ( UNIQUE_OR_PRIMARY_KEY ) ); Syntax 3 : CREATE TABLE DEPENDANT_TABLE ( ID INTEGER , NAME VARCHAR ( 9 ) ); ALTER TABLE DEPENDANT_TABLE ADD CONSTRAINT constraint_name FOREIGN KEY ( ID ) REFERENCES BASE_TABLE ( UNIQUE_OR_PRIMARY_KEY ) ON DELETE < delete_action_type > ON UPDATE < update_action_type > ; A delete action type can be a CASCADE , SET NULL , NO ACTION , or RESTRICT . An update action type can be a NO ACTION , or RESTRICT create a table with schema create schema mySchema create table mySchema . myTable ( col1 integer ) view : is a virtual table derived from one or more tables or other views. It is virtual because it does not contain any data, but a definition of a table based on the result of a SELECT statement. Views allow you to hide data or limit access to a select number of columns; therefore, they can also be used for security purposes. CREATE VIEW MYVIEW AS SELECT LASTNAME , HIREDATE FROM EMPLOYEE SELECT * FROM MYVIEW other database objects: 1. indexes 2. functions 3. procedures 4. triggers 5. ..etc rename DB object: RENAME < object type > < object name > to < new name > ALTER TABLE < table name > RENAME COLUMN < column name > TO < new name >","title":"Defining a relational database schema in SQL"},{"location":"knowledge-base/cs2203-database1/unit4/#data-manipulation-with-sql","text":"A SELECT statement returns its result set in no particular order executing the same SELECT statement multiple times will generate same rows but in different order, adding ORDER BY clause will return the same order every times SELECT col1 FROM myTable ORDER BY col1 DESC cursor : is a result set holding the result of a SELECT statement. The syntax to declare, open, fetch, and close DECLARE < cursor name > CURSOR [ WITH RETURN < return target > ] < SELECT statement > ; OPEN < cursor name > ; FETCH < cursor name > INTO < variables > ; CLOSE < cursor name > ; Rather than returning all the rows of an SQL statement to an application at once, a cursor allows the application to process rows one at a time. Using FETCH statements within a loop in the application, developers can navigate through each row pointed by the cursor and apply some logic to the row or based on the row contents. For example, the following code snippet sums all the salaries of employees using a cursor DECLARE p_sum INTEGER ; DECLARE p_sal INTEGER ; DECLARE c CURSOR FOR SELECT SALARY FROM EMPLOYEE ; DECLARE SQLSTATE CHAR ( 5 ) DEFAULT '00000' ; SET p_sum = 0 ; OPEN c ; FETCH FROM c INTO p_sal ; WHILE ( SQLSTATE = '00000' ) DO SET p_sum = p_sum + p_sal ; FETCH FROM c INTO p_sal ; END WHILE ; CLOSE c ; copy the results of a table into another table, you need to be careful for the attributes order though: insert into myTable ( select * from myTable2 ) delete statement: DELETE FROM myTable WHERE col1 > 1000 update statement: UPDATE myTable SET col1 = - 1 , col2 = \u2018 a \u2019 , col3 = \u2018 2010 - 01 - 01 \u2019 WHERE col4 = \u2018 0 \u2019 ;","title":"Data manipulation with SQL"},{"location":"knowledge-base/cs2203-database1/unit4/#sql-joins","text":"inner joins: Equi-join, Natural join, Cross join outer joins: Left outer join, Right outer join, Full outer join type definitions Equal join (equi) two tables are joined based on the equality of specified columns natural join improved version of an equi-join where the joining column does not require specification. The system automatically selects the column with same name in the tables and applies the equality operation on it and remove all duplicate attributes. ambiguous, not liked by most DBs. cross join Cartesian product of the tables to be joined left outer join the result set is a union of the results of an equi-join, including any non-matching rows from the LEFT table right outer join the union of results of an equi-join, including any non-matching rows from the RIGHT table. full outer join the result set is the union of results of an equi- join, including any non-matching rows of the LEFT and the RIGHT table. examples: -- Example 1 (equi join): SELECT * FROM student , enrollment WHERE student . enrollment_no = enrollment . enrollment_no -- Example 2 (equi join): SELECT * FROM student INNER JOIN enrollment ON student . enrollment_no = enrollment . enrollment_no -- example 3 (natural join): SELECT * FROM STUDENT NATURAL JOIN ENROLLMENT -- Example 4 (cross join): SELECT * FROM STUDENT , ENROLLMENT -- Example 5 (left outer join): SELECT * FROM STUDENT LEFT OUTER JOIN ENROLLMENT ON STUDENT . ENROLLMENT_NO = ENROLLMENT_NO -- Example 6 (right outer join): SELECT * FROM STUDENT RIGHT OUTER JOIN ENROLLMENT ON STUDENT . ENROLLMENT_NO = ENROLLMENT_NO -- Example 7 (full outer join ): SELECT * FROM STUDENT FULL OUTER JOIN ENROLLMENT ON STUDENT . ENROLLMENT_NO = ENROLLMENT_NO","title":"SQL joins"},{"location":"knowledge-base/cs2203-database1/unit4/#union-intersection-and-difference-operations","text":"operation definition union join two data sets having the same column definitions and in the same order. removes any duplicate rows. intersection returns a result set common to both data sets difference (EXCEPT) returns the result set that exists only in the RIGHT or First table as A EXCEPT B = A MINUS [A INTERSECT B] examples -- Example 1 (union): SELECT * FROM student_table_a UNION SELECT * from student_table_b -- Example 2 (union with duplicates): SELECT * FROM student_table_a UNION ALL -- duplicate rows will stay in the result SELECT * from student_table_b -- Example 3 (intersection): select * from student_table_a INTERSECT select * from student_table_b -- Example 4 (intersection with duplicates): select * from student_table_a INTERSECT ALL -- duplicate rows will stay in the result select * from student_table_b","title":"Union, intersection, and difference operations"},{"location":"knowledge-base/cs2203-database1/unit4/#relational-operators","text":"Basic mathematical operations like \u2018+\u2019, \u2018-\u2018, \u2018*\u2019 and \u2018/\u2019 Logical operators like \u2018AND\u2019, \u2018OR\u2019 and \u2018NOT\u2019 String manipulation operators like \u2018CONCATENATE\u2019, \u2018LENGTH\u2019, \u2018SUBSTRING\u2019 Comparative operators like \u2018=\u2019, \u2018<\u2019, \u2018>\u2019, \u2018>=\u2019, \u2018<=\u2019 and \u2018!=\u2019 Grouping and aggregate operators Other miscellaneous operations like DISTINCT","title":"Relational operators"},{"location":"knowledge-base/cs2203-database1/unit4/#grouping-operators","text":"perform operations on two or more rows of data, and provide a summarized output result set select course_enrolled , count ( * ) from students_enrollment group by course_enrolled and the result from the above statement: COURSE_ENROLLED STUDENT_COUNT English 10 Maths 20 Physics 40","title":"grouping operators"},{"location":"knowledge-base/cs2203-database1/unit4/#aggregation-operators","text":"Operators, which perform on two or more tuples or rows, and return a scalar result set, are called aggregate operators. Examples include: COUNT, SUM, AVERAGE, MINIMUM, MAXIMUM , and so on.","title":"Aggregation operators"},{"location":"knowledge-base/cs2203-database1/unit4/#having-clause","text":"can be used only with a GROUP BY clause to filter the desired rows in grouped data. basically, it is a WHERE clause but for Grouped data , since the WHERE clause can not work on grouped sets.","title":"HAVING Clause"},{"location":"knowledge-base/cs2203-database1/unit4/#sub-queries","text":"When a query is applied within a query, the outer query is referred to as the main query or parent query and the internal query is referred as the sub-query or inner query sub query may return a scalar value, single or multiple tuples, or a NULL data set Sub-queries are executed first, and then the parent query is executed utilizing data returned by the sub-queries.","title":"Sub-queries"},{"location":"knowledge-base/cs2203-database1/unit4/#sub-queries-returning-a-scalar-value","text":"Scalar values represent a single value of any attribute or entity, for example Name, Age. The example below returns a list of students who are the youngest among all students. The sub-query \u201cSELECT min(age) FROM students\u201d returns a scalar value that indicates the minimum age among all students. The parent query returns a list of all students whose age is equal to the value returned by the sub-query. SELECT name FROM students_enrollment WHERE age = ( SELECT min ( age ) FROM students );","title":"Sub-queries returning a scalar value"},{"location":"knowledge-base/cs2203-database1/unit4/#sub-queries-returning-vector-values","text":"When a sub-query returns a data set that represents multiple values for a column (like a list of names) or array of values for multiple columns (like Name, age and date of birth for all students), then the sub-query is said to be returning vector values. the example will get a list of students who are enrolled in courses offered by the computer science department: SELECT name FROM students WHERE course_enrolled IN ( SELECT distinct course_name FROM courses WHERE department_name = \u2018 Computer Science \u2019 ) sub-query returns a list of all courses that are offered in the \u201cComputer Science\u201d department and the outer query lists all students enrolled in the courses of the sub-query result set.","title":"Sub-queries returning vector values"},{"location":"knowledge-base/cs2203-database1/unit4/#correlated-sub-query","text":"When a sub-query is executed for each row of the parent table, instead of once then the sub-query is referred to as a correlated sub-query. the example below searches for a list of students with who have been awarded maximum marks in each department. For each row on the LEFT table, the sub-query finds max(marks) for the department of the current row and if the values of marks in the current row is equal to the sub-query result set, then it is added to the outer query result set. SELECT dept , name , marks FROM final_result a WHERE marks = ( SELECT max ( marks ) FROM final_result WHERE dept = a . dept )","title":"Correlated sub-query"},{"location":"knowledge-base/cs2203-database1/unit4/#sub-query-in-from-clauses","text":"A sub-query can be used in a FROM clause as well. SELECT dept , max_marks , min_marks , avg_marks FROM ( SELECT dept , max ( marks ) as max_marks , min ( marks ) as min_marks , avg ( marks ) as avg_marks FROM final_result GROUP BY dept ) WHERE ( max_marks \u2013 min_marks ) > 50 and avg_marks < 50 The above query uses a sub-query in the FROM clause. The sub-query returns maximum, minimum and average marks for each department. The outer query uses this data and filters the data further by adding filter conditions in the WHERE clause of the outer query","title":"Sub-query in FROM Clauses"},{"location":"knowledge-base/cs2203-database1/unit4/#map-oop-concepts-to-relational-database-concepts","text":"Object-relational mapping (ORM) libraries such as Hibernate , pureQuery are popular to provide a framework for this mapping between the object-oriented world and the relational world. The table below shows the correspondence between conceptual, logical and physical model concepts","title":"map OOP concepts to relational database concepts"},{"location":"knowledge-base/cs2203-database1/unit4/#a-rough-guide-to-sql-2","text":"data stored in the DB in a way that can not be read by humans that's why we need DBMS. in early days of DBs the network and hierarchy data models were the most popular. SQL is a relational database language, but it is not a DBMS , instead it is a way to communicate with the DBMS. SQL is non procedural database language, which means that when executing a a SQL command you don't need to run,compile a program for each query or define the location of data, DBMS will take care of that. interactive SQL can be run in the command line and output results to the console. programmatic SQL where SQL commands are embedded in a host language (COBOL, C.SQL). SQL needs a host language cause it is not a complete programming language , it has no branching or looping functionalities, so it relays on the host language to allow this. VIEWS are virtual tables, creating a view will create a new table in the database based on select statement provided when creating the view. the view then gets updated when any updates happen on the original table(s). when a new DB is created it is owned by the user who created it and have all privileges over it.","title":"A rough guide to SQL [2]"},{"location":"knowledge-base/cs2203-database1/unit4/#sql-standards-support-3","text":"SQL SECTION COMMANDS Data Definition Language (DDL) CREATE, ALTER, DROP, GRANT, REVOKE, COMMENT ON, EXPLAIN REFERENCES, DECLARE Data Manipulation Language (DML) INSERT, UPDATE, DELETE, TRUNCATE, MERGE Data Query Language (DQL) SELECT, VALUES, WITH, EXPLAIN PLAN General operations on DB BACKUP, PERFORM, SCRIPS, CHECKPOINT, SHUTDOWN transaction statements START TRANSACTION, SET TRANSACTION, COMMIT, ROLLBACK, SAVEPOINT, RELEASE SAVEPOINT, LOCK, CONNECT, DISCONNECT","title":"SQL Standards Support [3]"},{"location":"knowledge-base/cs2203-database1/unit4/#short-guide-to-data-types","text":"Numeric types TINYINT, SMALLINT, INTEGER and BIGINT with fixed binary precision, NUMERIC and DECIMAL are types with user-defined decimal precision. DOUBLE type is a 64-bit. BOOLEAN type is for logical values and can hold TRUE, FALSE or UNKNOWN . Character string types are CHAR(L), VARCHAR(L) and CLOB , type comments TINYINT, SMALLINT, INTEGER and BIGINT fixed binary precision NUMERIC and DECIMAL user-defined decimal precision DOUBLE 64-bit CHAR(L) fixed length strings, if you provide shorter strings, spaces will be added. if no L provided its a single char VARCHAR(L) general strings, performance issues appear for long strings with this type, avoid in large strings (> 10* KB). CLOB for large strings, avoid for short strings LONGCHAR a synonym for a long VARCHAR and can be used without specifying the size BINARY(L) fixed length strings such as UUID, e pads short binary strings with zero bytes, if no L it is a single byte VARBINARY(L) general binary strings BLOB(L) for large binary objects LONGVARBINARY used as VARBINARY without specifying the size, can be mapped to BLOB instead. BIT(L) and BITVARYING(L) avoid use without specifying L, use BOOLEAN for single bit. UUID stored as BINARY. UUID and BINARY strings, can be used to insert or to compare. DATE, TIME, and TIMESTAMP, TIME ZONE INTERVAL used with datetime, not widely supported OTHER storage of json objects, if object is large, serialize and save as BLOB. ARRAY support all types except OTHER, BLOB","title":"Short Guide to Data Types"},{"location":"knowledge-base/cs2203-database1/unit4/#references","text":"[1] Sharma, N., Perniu, L., Chong, R. F., Iyer, A., Nandan, C., Mitea, A. C., Nonvinkere, M., & Danubianu, M. (2010). Databases fundamentals. chapter 5. [2] Din, A. I. (2006). Structured Query Language (SQL): A practical introduction. NCC Blackwell. chapters 1 & 2. Retrieved from http://www.managedtime.com/freesqlbook.php . [3] Simpson, B., Toussi, F, & The HSQL Development Group. (2019, June 2). HyperSQL user guide. Chapter 2. Retrieved from http://hsqldb.org/doc/2.0/guide/sqlgeneral-chapt.html","title":"References"},{"location":"knowledge-base/cs2203-database1/unit5/","text":"Unit 5: DDL and DML Commands \u00b6 Unit 5: DDL and DML Commands Tables in SQL Database security (DCL) ANSI/ISO standards for DDL, DML Structure of SQL database single database architecture Multi database architecture CREATE TABLE command Column and table modifiers NOT NULL modifier UNIQUE modifier INDEX modifier PRIMARY KEY modifier FOREIGN KEY modifier DEFAULT modifier CHECK modifier INDEXes ALTER TABLE command DROP TABLE command INSERT command UPDATE command DELETE command References Tables in SQL \u00b6 Base Tables : normal tables created by CREATE TABLE command, database object whose structure and data are both stored on the disk. Virtual Tables (Views) : tables whose contents are driven from other base tables , only views structure is stored on the disk. SQL DML can work on VIEWS as they do on BASE TABLES, but when modifying a VIEW, the data in its base tables still untouched. a VIEW can be thought as stored SELECT statement . Database security (DCL) \u00b6 VIEWS can be used to prevent access to sensitive information. GRANT, REVOKE statements can also be used. ANSI/ISO standards for DDL, DML \u00b6 The ANSI/ISO standards separates DDL from DML and assumes that RDBMS will not accept any DDL statements after the DB has been created. commercial RDBMS allow you to execute any DDL statements at anytime with no separation between DDL and DML. ANSI/ISO standards does not have ALTER TABLE or DROP TABLE commands. ANSI/ISO standards force organizations to perform strict analysis process before creating the system, then the changes to the DB after that is not tolerated. Structure of SQL database \u00b6 single database architecture \u00b6 ANSI/ISO standards specifies that the database schema consists of single large database with tables that owned by various users. the ownership of tables sub-classifies them into different virtual database groups . this called single database architecture and it is used by Oracle, IBM DB2 . disadvantages: over the time, tables will become very big and bulky. performing db adminstration tasks become very complex. eg: backups, performance analysis. Multi database architecture \u00b6 tables are organized into several distinct databases. used in Sybase, SQL server, Ingres . disadvantages: maintaining foreign key references to keys in another databases becomes more complicated with time. CREATE TABLE command \u00b6 create table command. modifiers in the select statement: UNIQUE, INDEX : both will create an index for this column. NOT NULL . PUBLIC : create a public table that can be accessed by any user of this DB. Column and table modifiers \u00b6 modifiers can be applied to a COLUMN, or to a group of columns at the TABLE level . NOT NULL modifier \u00b6 can be used on a single column . prevent inserting NULL (empty) values for this column. usually used for primary keys. UNIQUE modifier \u00b6 prevent inserting new rows if there is a single row that has the same value(s) for this column(s). makes sense only with NOT NULL columns. INDEX modifier \u00b6 INDEX modifier is not part of ANSI/ISO standards but it is common in commercial databases. creates an index based on the values stored in this column which speeds up the querying process. most SQL systems will create an index for columns that are specified as UNIQUE . index maintenance is taken care by the DBMS. PRIMARY KEY modifier \u00b6 relatively new feature in SQL. formally define a primary key. column must be NOT NULL before apply PRIMARY KEY modifier. FOREIGN KEY modifier \u00b6 define a reference for a PRIMARY KEY from another table. syntax as FOREIGN KEY (:column_name) REFERENCES (:other_table_name[.:primary_key_name]) DEFAULT modifier \u00b6 define a value to be given to the column in case of supplying NULL. CHECK modifier \u00b6 make custom validation on data before inserting it to the table. syntax: CREDITS NUMERIC(2) CHECK (CREDITS > 0 AND CREDITS <= 10) example ensures that all CREDITS inserted must be between 1 and 10. it can also be applied at the table level to check combination of a group of columns ,as in the example below CREATE TABLE Lecturers ( ID NOT NULL PRIMARY KEY , Pay DECIMAL ( 6 ), Grade CHAR , CHECK ( Pay < 100000 OR Grade <= 'B' ) -- ensures that pay > 100000 only allowed if Grade is A or B. ); INDEXes \u00b6 index is a database object created and maintained by the DBMS. speeds up querying process by keeping a stored list of values which the DBMS can search through. UNIQUE modifier will always create an index for this column. example of indexing table by city: CITY MEMORY LOCATION LDN 00124557887 LDN 00124478345 ... .... PARIS 001244533456 ... .... so all occurrences of LDN are sequentially listed (ordered by city name) in the index with their memory location on the DISK so the DBMS can quickly resolve the location of the row. index disadvantages use additional disk spae. the write operations on this table become slower wince the DBMS needs to populate these changes to all indexes. syntax: CREATE INDEX : index_name ON : table_name (: column_name ); DROP INDEX : index_name ; ALTER TABLE command \u00b6 change the structure of a table: ADD clause to add new columns. MODIFY clause to modify existing columns. DROP clause to drop existing columns. alter table command notes: MODIFY clause can only allows changing UNIQUE, NOT NULL status of a column, to make more changes you need to DROP the column and ADD a new one with the same name . ALTER table that is already populated with data is a risky process and can be a source of errors, for fundamental changes to a table with data, the best solution is to CREATE a new table with the new structure and POPULATE it with the data from the old table . ALTER table needs all privileges to work. DROP TABLE command \u00b6 before dropping a table, make sure that: no files in the system query this table. no references of this table in other tables as foreign keys. no other VIEWS that are querying this table. INSERT command \u00b6 you can insert single or multiple rows at once using the INSERT command. INSERT syntax INSERT INTO : table_name [(...: column_names )] VALUES (...: values ), [, (...: values ) * ]; you can also insert multiple rows selected using SELECT statement as the example below: INSERT INTO : table_name [(...: column_names )] { SELECT STATEMENT } ; INSERT INTO ELITE_EXAMS ( MARK , STUDENT ) SELECT MARK , STUDENT_NO FROM EXAMS WHERE MARK >= 80 ; -- USING SUB-QUERIES INSERT INTO LOW_BUDGET SELECT * FROM STUDENTS WHERE DEPT_NO IN ( SELECT DEPT_NO FROM DEPARTMENTS WHERE BUDGET < 1000 ); NOTES: when using sub-queries in the INSERT statement, the SUB-QUERY must NOT make any reference to the table that it is INSERTing in . UPDATE command \u00b6 update the values in a row. needs 3 things: 1.table name. 2. updated column names. 3. updated values. scalar expressions can be used in the SET clause. examples: UPDATE LECTURERS SET PAY = PAY * 1 . 1 ; -- increases the pay of all rows by 10%. -- USING SUB-QUERIES UPDATE EXAMS SET MARK = MARK + 4 WHERE SUB_NO = ANY ( SELECT SUB_NO FROM SUBJECTS WHERE DEPT_NO = 1 -- sub-query ) ; -- increase exam marks by 4 in all subjects that belongs to department 1. UPDATE EXAMS SET MARK = MARK + 4 WHERE SUB_NO = ANY ( SELECT SUB_NO FROM SUBJECTS WHERE DEPT_NO = ( -- sub-query SELECT DEPT_NO FROM DEPARTMENTS WHERE DEPT_NAME = 'ENGINEERING' -- second-sub-query ) ) ; -- increase exam marks by 4 in all subjects that belongs to the department of ENGINEERING. DELETE command \u00b6 remove several or single row. removes the entire row and NOT part of it . if used without predicate , it will delete all rows in the table. syntax: DELETE FROM STUDENTS WHERE SURNAME = 'ALI' AND YEAR = 3 ; -- BEST PRACTICE: always delete using primary key, check row first using SELECT statement and grab its ID. -- then delete using that ID. SELECT ID FROM STUDENTS WHERE SURNAME = 'ALI' AND YEAR = 3 ; // 16 DELETE FROM STUDENTS WHERE ID = 16 ; References \u00b6 [2] Din, A. I. (2006). Structured Query Language (SQL): A practical introduction. NCC Blackwell. chapters 3 & 5. Retrieved from http://www.managedtime.com/freesqlbook.php .","title":"Unit 5: DDL and DML Commands"},{"location":"knowledge-base/cs2203-database1/unit5/#unit-5-ddl-and-dml-commands","text":"Unit 5: DDL and DML Commands Tables in SQL Database security (DCL) ANSI/ISO standards for DDL, DML Structure of SQL database single database architecture Multi database architecture CREATE TABLE command Column and table modifiers NOT NULL modifier UNIQUE modifier INDEX modifier PRIMARY KEY modifier FOREIGN KEY modifier DEFAULT modifier CHECK modifier INDEXes ALTER TABLE command DROP TABLE command INSERT command UPDATE command DELETE command References","title":"Unit 5: DDL and DML Commands"},{"location":"knowledge-base/cs2203-database1/unit5/#tables-in-sql","text":"Base Tables : normal tables created by CREATE TABLE command, database object whose structure and data are both stored on the disk. Virtual Tables (Views) : tables whose contents are driven from other base tables , only views structure is stored on the disk. SQL DML can work on VIEWS as they do on BASE TABLES, but when modifying a VIEW, the data in its base tables still untouched. a VIEW can be thought as stored SELECT statement .","title":"Tables in SQL"},{"location":"knowledge-base/cs2203-database1/unit5/#database-security-dcl","text":"VIEWS can be used to prevent access to sensitive information. GRANT, REVOKE statements can also be used.","title":"Database security (DCL)"},{"location":"knowledge-base/cs2203-database1/unit5/#ansiiso-standards-for-ddl-dml","text":"The ANSI/ISO standards separates DDL from DML and assumes that RDBMS will not accept any DDL statements after the DB has been created. commercial RDBMS allow you to execute any DDL statements at anytime with no separation between DDL and DML. ANSI/ISO standards does not have ALTER TABLE or DROP TABLE commands. ANSI/ISO standards force organizations to perform strict analysis process before creating the system, then the changes to the DB after that is not tolerated.","title":"ANSI/ISO standards for DDL, DML"},{"location":"knowledge-base/cs2203-database1/unit5/#structure-of-sql-database","text":"","title":"Structure of SQL database"},{"location":"knowledge-base/cs2203-database1/unit5/#single-database-architecture","text":"ANSI/ISO standards specifies that the database schema consists of single large database with tables that owned by various users. the ownership of tables sub-classifies them into different virtual database groups . this called single database architecture and it is used by Oracle, IBM DB2 . disadvantages: over the time, tables will become very big and bulky. performing db adminstration tasks become very complex. eg: backups, performance analysis.","title":"single database architecture"},{"location":"knowledge-base/cs2203-database1/unit5/#multi-database-architecture","text":"tables are organized into several distinct databases. used in Sybase, SQL server, Ingres . disadvantages: maintaining foreign key references to keys in another databases becomes more complicated with time.","title":"Multi database architecture"},{"location":"knowledge-base/cs2203-database1/unit5/#create-table-command","text":"create table command. modifiers in the select statement: UNIQUE, INDEX : both will create an index for this column. NOT NULL . PUBLIC : create a public table that can be accessed by any user of this DB.","title":"CREATE TABLE command"},{"location":"knowledge-base/cs2203-database1/unit5/#column-and-table-modifiers","text":"modifiers can be applied to a COLUMN, or to a group of columns at the TABLE level .","title":"Column and table modifiers"},{"location":"knowledge-base/cs2203-database1/unit5/#not-null-modifier","text":"can be used on a single column . prevent inserting NULL (empty) values for this column. usually used for primary keys.","title":"NOT NULL modifier"},{"location":"knowledge-base/cs2203-database1/unit5/#unique-modifier","text":"prevent inserting new rows if there is a single row that has the same value(s) for this column(s). makes sense only with NOT NULL columns.","title":"UNIQUE modifier"},{"location":"knowledge-base/cs2203-database1/unit5/#index-modifier","text":"INDEX modifier is not part of ANSI/ISO standards but it is common in commercial databases. creates an index based on the values stored in this column which speeds up the querying process. most SQL systems will create an index for columns that are specified as UNIQUE . index maintenance is taken care by the DBMS.","title":"INDEX modifier"},{"location":"knowledge-base/cs2203-database1/unit5/#primary-key-modifier","text":"relatively new feature in SQL. formally define a primary key. column must be NOT NULL before apply PRIMARY KEY modifier.","title":"PRIMARY KEY modifier"},{"location":"knowledge-base/cs2203-database1/unit5/#foreign-key-modifier","text":"define a reference for a PRIMARY KEY from another table. syntax as FOREIGN KEY (:column_name) REFERENCES (:other_table_name[.:primary_key_name])","title":"FOREIGN KEY modifier"},{"location":"knowledge-base/cs2203-database1/unit5/#default-modifier","text":"define a value to be given to the column in case of supplying NULL.","title":"DEFAULT modifier"},{"location":"knowledge-base/cs2203-database1/unit5/#check-modifier","text":"make custom validation on data before inserting it to the table. syntax: CREDITS NUMERIC(2) CHECK (CREDITS > 0 AND CREDITS <= 10) example ensures that all CREDITS inserted must be between 1 and 10. it can also be applied at the table level to check combination of a group of columns ,as in the example below CREATE TABLE Lecturers ( ID NOT NULL PRIMARY KEY , Pay DECIMAL ( 6 ), Grade CHAR , CHECK ( Pay < 100000 OR Grade <= 'B' ) -- ensures that pay > 100000 only allowed if Grade is A or B. );","title":"CHECK modifier"},{"location":"knowledge-base/cs2203-database1/unit5/#indexes","text":"index is a database object created and maintained by the DBMS. speeds up querying process by keeping a stored list of values which the DBMS can search through. UNIQUE modifier will always create an index for this column. example of indexing table by city: CITY MEMORY LOCATION LDN 00124557887 LDN 00124478345 ... .... PARIS 001244533456 ... .... so all occurrences of LDN are sequentially listed (ordered by city name) in the index with their memory location on the DISK so the DBMS can quickly resolve the location of the row. index disadvantages use additional disk spae. the write operations on this table become slower wince the DBMS needs to populate these changes to all indexes. syntax: CREATE INDEX : index_name ON : table_name (: column_name ); DROP INDEX : index_name ;","title":"INDEXes"},{"location":"knowledge-base/cs2203-database1/unit5/#alter-table-command","text":"change the structure of a table: ADD clause to add new columns. MODIFY clause to modify existing columns. DROP clause to drop existing columns. alter table command notes: MODIFY clause can only allows changing UNIQUE, NOT NULL status of a column, to make more changes you need to DROP the column and ADD a new one with the same name . ALTER table that is already populated with data is a risky process and can be a source of errors, for fundamental changes to a table with data, the best solution is to CREATE a new table with the new structure and POPULATE it with the data from the old table . ALTER table needs all privileges to work.","title":"ALTER TABLE command"},{"location":"knowledge-base/cs2203-database1/unit5/#drop-table-command","text":"before dropping a table, make sure that: no files in the system query this table. no references of this table in other tables as foreign keys. no other VIEWS that are querying this table.","title":"DROP TABLE command"},{"location":"knowledge-base/cs2203-database1/unit5/#insert-command","text":"you can insert single or multiple rows at once using the INSERT command. INSERT syntax INSERT INTO : table_name [(...: column_names )] VALUES (...: values ), [, (...: values ) * ]; you can also insert multiple rows selected using SELECT statement as the example below: INSERT INTO : table_name [(...: column_names )] { SELECT STATEMENT } ; INSERT INTO ELITE_EXAMS ( MARK , STUDENT ) SELECT MARK , STUDENT_NO FROM EXAMS WHERE MARK >= 80 ; -- USING SUB-QUERIES INSERT INTO LOW_BUDGET SELECT * FROM STUDENTS WHERE DEPT_NO IN ( SELECT DEPT_NO FROM DEPARTMENTS WHERE BUDGET < 1000 ); NOTES: when using sub-queries in the INSERT statement, the SUB-QUERY must NOT make any reference to the table that it is INSERTing in .","title":"INSERT command"},{"location":"knowledge-base/cs2203-database1/unit5/#update-command","text":"update the values in a row. needs 3 things: 1.table name. 2. updated column names. 3. updated values. scalar expressions can be used in the SET clause. examples: UPDATE LECTURERS SET PAY = PAY * 1 . 1 ; -- increases the pay of all rows by 10%. -- USING SUB-QUERIES UPDATE EXAMS SET MARK = MARK + 4 WHERE SUB_NO = ANY ( SELECT SUB_NO FROM SUBJECTS WHERE DEPT_NO = 1 -- sub-query ) ; -- increase exam marks by 4 in all subjects that belongs to department 1. UPDATE EXAMS SET MARK = MARK + 4 WHERE SUB_NO = ANY ( SELECT SUB_NO FROM SUBJECTS WHERE DEPT_NO = ( -- sub-query SELECT DEPT_NO FROM DEPARTMENTS WHERE DEPT_NAME = 'ENGINEERING' -- second-sub-query ) ) ; -- increase exam marks by 4 in all subjects that belongs to the department of ENGINEERING.","title":"UPDATE command"},{"location":"knowledge-base/cs2203-database1/unit5/#delete-command","text":"remove several or single row. removes the entire row and NOT part of it . if used without predicate , it will delete all rows in the table. syntax: DELETE FROM STUDENTS WHERE SURNAME = 'ALI' AND YEAR = 3 ; -- BEST PRACTICE: always delete using primary key, check row first using SELECT statement and grab its ID. -- then delete using that ID. SELECT ID FROM STUDENTS WHERE SURNAME = 'ALI' AND YEAR = 3 ; // 16 DELETE FROM STUDENTS WHERE ID = 16 ;","title":"DELETE command"},{"location":"knowledge-base/cs2203-database1/unit5/#references","text":"[2] Din, A. I. (2006). Structured Query Language (SQL): A practical introduction. NCC Blackwell. chapters 3 & 5. Retrieved from http://www.managedtime.com/freesqlbook.php .","title":"References"},{"location":"knowledge-base/cs2203-database1/unit6/","text":"Unit 6: Querying the Database using SQL \u00b6 Unit 6: Querying the Database using SQL Useful Definitions SELECT statement Cartesian Product Example: Cartesian product cartesian product of 3 tables WHERE Clause BETWEEN Operator IN operator LIKE operator ESCAPE Clause NULL operator AND, OR, NOT Operators ORDER BY clause aggregate functions COUNT() aggregate function SUM() aggregate function AVG() aggregate function MIN() and MAX() aggregate functions GROUP BY clause HAVING clause SQL JOINS self join nested SELECT statements (sub-queries) linked SELECT statements (correlated sub-query) EXISTS operator ANY, ALL operators UNION clause References Useful Definitions \u00b6 projection in a SQL statement: selecting a subset of the whole attribute set of a relation. eg. selecting only ID of a full table (select subset of columns). scalar expressions : simple calculations can be performed on NUMERIC type column values. predicate : logical expression that can either be true or false. synonyms : permanent aliases for table names, different from the normal table name aliases which only lasts while the query is being executed. available in ORACLE . SELECT statement \u00b6 most complex statement of ANSI/ISO SQL. has 6 different clauses. 2 required information: COLUMN_NAME and TABLE_NAME. the six clauses are: DISTINCT : eliminate the duplicate rows form the query, useful to know values in the table regardless of how many times they have appeared, opposite is ALL . FROM WHERE ORDER BY GROUP BY HAVING: eliminate part of the results depending on a condition. field_expression (in the image above) can be: field name eg. ID. (field names (columns) in SQL are limited to 24 characters length). ANSI aggregate function as: SUM(), AVG(), MIN(), MAX(), COUNT(). when using * in SELECT statement, columns in the result appear in the order they defined in the table. when specifying the column to select, the column will appear in the result in the order that specified in the statement. select statement allows you to use scalar expressions and constant string to specify column names. scalar expressions allow for dynamic or calculated column names, but the column MUST have numeric type or it will throw an error. scalar expressions in select statements lateral strings will appear for each row, they can be enclosed in single or double quotes. constant (lateral) strings in the select statements Cartesian Product \u00b6 when you specify more than one table in the FROM clause, this will produce the cartesian product of the result. for every row from the left set, a new row matching every single row of the right set will be generated. if we have 2 rows in the left set, and 3 rows in the right set, the final result will contain 6 rows as in the example below. The cartesian product is not limited to 2 tables, but this can grow quickly with adding more . this is not very useful in real-life, but good for science or AI purposes. Example: Cartesian product \u00b6 we have 4 cars in the cars table and 2 rows in the sale table when we execute SELECT YEAR, Tax FROM cars, sale; we got a result of 8 rows, where every YEAR from cars appears in a row with every Tax from sale Notice that YEAR 1983 appears twice one with Tax 1.298 and one with Tax 1.656. and the same is true for EVERY YEAR . cartesian product of 3 tables \u00b6 Notice how the final results gone up to 48 row, although we have 4, 2, 4 rows in every table respectively. WHERE Clause \u00b6 lets you specify predicate that tells SQL which rows should appear on the results. when processing a query with a predicate, DBMS goes through all the rows and evaluate the condition to true or false, this process can be speed up using indexes . operators allowed in WHERE clause include: >, <, =,<=, >=, <> , <> means not equal . mathematical operators of WHERE clause can work on NUMERIC, CHAR types. those operators can be used in CHAR based column types, the operating system matters in this case where it uses ASCII or EBCDIC system. it is also case sensitive. in ASCII , uppercase chars known has lower values than lowercase chars. using comparison operators with CHAR based values BETWEEN Operator \u00b6 known as range test operator and allows to define a predicate in the form of a range. the BETWEEN range test determines the lower boundary of the range, and should be followed by AND to specify the upper boundary of the range. value of BETWEEN should be lower than the value of AND. upper and lower boundaries are inclusive . using between operator in NUMERIC and CHAR based columns. between does not add functionality to SQL, as it can be rephrased using WHERE, AND operators, aas shown in the photo below rephrase BETWEEN operator to use AND. IN operator \u00b6 know as set membership test operator . returns true when the value is in the provided set. set values must be comma separated in between parentheses. IN operator can be rephrased using OR operator. IN operator, and rephrase of IN using OR operator LIKE operator \u00b6 known as pattern matching test operator . uses characters like %, _ as wildcard characters. useful when you don't know the exact word that you are searching for. _ indicates one valid character , while % indicates multiple chars . _ and % can be combined. <img src=\"https://i.imgur.com/Ab5F2UC.png\" using % and _ as wild characters in LIKE operator ESCAPE Clause \u00b6 if % or _ are part of the string you need to use ESCAPE clause when using LIKE operator. you need to define a char (mostly $) before the char that you want to escape, and then add that char to ESCAPE clause. ESCAPE clause is not widely supported. using ESCAPE clause with LIKE operator so _ and % can be treated as normal letters NULL operator \u00b6 known as value test operator . checks if a value has been set or not, empty string and 0 are NOT NULL . checks for NULL in unknown column will return all rows in the table or throws an error(depending on SQL and DBMS versions). works as column = NULL or column IS NULL . the opposite statement would be column <> NULL or column IS NOT NULL . using NULL, IS NULL, NOT NULL, IS NOT NULL operators. Note that column <p> NULL and column != NULL did not work on this version of SQL. AND, OR, NOT Operators \u00b6 known as Logical operators logical operator links multiple predicates within a single WHERE clause. you can group expressions by using parentheses. ORDER BY clause \u00b6 by default SQL orders the results arbitrarily depending on the order in which the rows have been found which in turn depends on the location of these rows on the disk and the location of the objects representing these rows in server's memory . you can use ORDER BY clause with column name to order on. you can use more than one column name, then SQL will ORDER BY will use first column as primary ordering column, the second column as secondary and so on. ANSI/ISO standards requires the columns that you ORDER BY to appear in the results, so they have to be included in the SELECT statement. you can always use the column index (starts from 1) instead of the column name as: SELECT ID , NAME FROM T ORDER BY NAME DESC ; -- is equal to this statement SELECT ID , NAME FROM T ORDER BY 2 DESC ; -- 2 is the index of column NAME in the select statement, ID has index 1 aggregate functions \u00b6 5 functions that can be used to summarize data in tables as they operate on the table data and generate a single value as output. COUNT() : number of rows or columns the query selects, no rows are returned except the count. SUM() . AVG() . MIN() . MAX() . you can NOT nest the aggregate functions, or mix regular columns and aggregate functions in the same query. COUNT() aggregate function \u00b6 it has 2 types: counts and lists the number of all non-NULL values in a particular column, using DISTINCT keyword before the columns name . counts and displays the number of rows that would be retrieved by the query. SELECT COUNT ( DEPT_NO ) FROM STUDENTS ; -- 16 -- count the rows, second type. will return the number of all rows that has DEPT_NO in them SELECT COUNT ( DISTINCT DEPT_NO ) FROM STUDENTS ; -- 5 -- count the occurrence of each DEPT_NO, it happens that we have 5 different departments. -- null values for DEPT_NO are ignored. -- each DEPT_NO might be occurred more than once, but it only appears in the count for once. -- this is useful to count `how many different departments that we have that are assigned to students` SELECT COUNT ( * ) FROM STUDENTS ; -- count all rows, including null and duplicate rows. SUM() aggregate function \u00b6 calculates the total of the values in a NUMERIC type column. it take the name of the column, or the name of the column in a scalar expression as an argument. results of SUM() sometimes has greater precision than the column itself. SELECT SUM ( PRICE ) FROM ITEMS ; -- name of the column SELECT SUM ( PRICE + ( PRICE * 0 . 2 )) FROM ITEMS ; -- name of the column in a scalar expression AVG() aggregate function \u00b6 calculates the average or arithmetic mean of the values in a NUMERIC type column, adding all values then divide by the number of rows. SELECT AVG ( PRICE ) FROM ITEMS ; -- average of all rows = sum / count (all rows) SELECT AVG ( PRICE ) FROM ITEMS WHERE ITEM_ID = 5 ; -- average of selected set of rows = sum / count (selected rows) MIN() and MAX() aggregate functions \u00b6 MIN() returns the smallest value in a column. it can work on NUMERIC, STRING, and non-ANSI types (such date and time) column types. MAX() is the opposite, but works on the same column types. both allow the use of scalar expressions and column name as arguments. SELECT MIN ( JOIN_DATE ) FROM T ; -- 01-01-2020 SELECT MAX ( JOIN_DATE ) FROM T ; -- 01-01-2022 SELECT MIN ( MARK ) FROM EXAMS ; -- 18 SELECT MAX ( 100 - MARK ) FROM EXAMS ; -- 82, as 100-18 GROUP BY clause \u00b6 allows to split the values in a column into subsets , then apply the aggregate functions on those subsets might be very useful. we can group by single column or multiple columns . SELECT STUDENT_NO , AVG ( MARK ) FROM EXAMS GROUP BY STUDENT_NO ; -- results https://i.imgur.com/AmLRt9k.png -- this query splits up all exams into sets by STUDENT_NO, then calculates the AVG of the marks in each set. -- so it is calculating the average score for each student in all exams. -- we can get an equal query as: SELECT STUDENT_NO , AVG ( MARK ) FROM EXAMS WHERE STUDENT_NO = 1 ; -- then use a query for each student id. SELECT SUB_NAME , DEP_NO , CREDITS , MAX ( PASS ) FROM STUDENTS GROUP BY DEP_NO , CREDITS ; -- results https://i.imgur.com/RUxPVet.png -- we got split sets into combinations of all DEP_NO and CREDITS. HAVING clause \u00b6 you can NOT use aggregate functions in the WHERE clause , so you can NOT use WHERE clause to eliminate the results that don't interest you. HAVING clause allows for filtering the results that are produced by the aggregate functions and GROUP BY . it accepts only one single value SELECT STUDENT_NO , AVG ( MARK ) FROM EXAMS GROUP BY STUDENT_NO HAVING AVG ( MARK ) > 65 ; SQL JOINS \u00b6 JOIN is the ability to retrieve data from several different related tables, that's the power of rational databases. the tables to be joined should be named in the FROM clause as comma separated. the relationships between tables should be defined in the predicate or WHERE clause . if the columns are unique between all the tables int he join , we don't have to prefix the column name by the table name. but it is good practice to always use tableName.columnName notation in the join statement. we don't care about how SQL will actually do the join, DBMS will take care of that. the results are what important to us. when processing the JOIN, SQL will look up all possible combinations of the rows in the join tables , then evaluate them and decide to add each row to the results or not. and the steps are: construct a list of all possible row combinations from the tables in the JOIN. check if the predicate is true for each combination of rows . if the predicate is true, that row will be added to the results. once all possible rows have been checked, the results will be returned by the query. self join \u00b6 joining 2 copies of the same table. data retrieved by self-join can not be retrieved by any other query. we can use table name aliases to distinguish the 2 copies from each other. SELECT L . SURNAME , R . SURNAME , L . DEPT_NO FROM LECTURERS L , LECTURERS R -- USING table name aliases to reference tables WHERE L . DEPT_NO = R . DEPT_NO -- predicate AND L . SURNAME <> R . SURNAME ; -- <> means not-equals (!=), eliminate so duplicate records. nested SELECT statements (sub-queries) \u00b6 the results of a query becomes part of the predicate of another query (WHERE clause or HAVING clause) . DBMS will execute the sub-query first, then the results of the sub-query is being populated into the predicate. when using = in a predicate with sub-query: make sure that sub-query returns only single value like: selecting one column, a result of an aggregate function , and the type of the sub-query result should be the same of the predicate column. example: https://i.imgur.com/aG4Ran4.png we have to make sure that the sub-query also returns a value and not just empty. example https://i.imgur.com/OVkRjxd.png aggregate functions are allowed unless they are using GROUP BY, HAVING , even if the GROUP BY, or HAVING returned one value it will be rejected, because they are returning a set -even if it has one item- . SELECT * FROM EXAMS WHERE STUDENT_NO = ( SELECT STUDENT_NO FROM STUDENTS WHERE NAME = 'AHMAD' AND LASTNAME = 'ALI' ); -- SUB-QUERY, executed first SELECT SURNAME , PAY FROM LECTURERS WHERE PAY < ( SELECT AVG ( PAY ) FROM LECTURERS ); -- aggregate function that returns a single value. -- GET ALL lecturers that are getting paid less than average. the sub-query should always appear after the comparison operator on the predicate, adding the sub-query before the operator will throw an error. example https://i.imgur.com/79k1fOk.png if the sub-query returns a set, you can use the IN operator SELECT * FROM EXAMS WHERE STUDENT_NO IN ( SELECT STUDENT_NO FROM STUDENTS WHERE DEPT_NO = 1 ); -- sub-query returns a set, so we used `IN` operator ANSI/ISO standards don't have a limit on the level of query nesting, but practically it is very low. linked SELECT statements (correlated sub-query) \u00b6 another method of extracting data of multiple tables by linking them through a sub-query. correlated sub-query : is a sub-query that refers to columns of the main table in the main query. example here https://i.imgur.com/6h8mvh9.png SELECT * FROM EXAMS WHERE SUB_NO IN -- candidate row ( SELECT SUB_NO FROM SUBJECTS WHERE SUBJECTS . PASS < EXAMS . MARK -- SUB-QUERY referencing main table `exams` -- this referred to as `outer reference`. ); -- this query selects all successful exams: -- - for each row in exams it loops through all rows in subject. -- - and gets all SUBJECT_NO where this exam.mark is considered successful. -- - it lastly compare the SUBJECT_NO of this exam with the retrieved successful SUBJECT_NO(s) correlated sub-query is executed once for each row of the outer table. outer reference : sub-query (correlated sub-query) that reference a column from the outer (main) table . candidate row : the current outer query for which the sub-query is being executed. detailed example on correlated sub-query here: https://i.imgur.com/WdiL1eD.png EXISTS operator \u00b6 similar to IN operator . MUST have sub-query as its argument. returns true if the sub-query finds any value , false if no values returned by the sub-query. SELECT * FROM SUBJECTS WHERE EXISTS ( SELECT * FROM SUBJECTS WHERE PASS > 75 -- will be executed once (not correlated) ); -- retrieves ```all rows``` in the SUBJECTS table, if there exists ```one subject``` with pass > 75. SELECT DISTINCT A . STUDENT_NO FROM EXAMS A WHERE EXISTS ( SELECT * FROM EXAMS B WHERE B . STUDENT_NO = A . STUDENT_NO AND B . SUB_NO <> A . SUB_NO -- WILL executed foreach outer row (correlated) ); -- retrieves all students that have taken more than one exam in different subjects. SELECT DISTINCT A . STUDENT_NO , S . SURNAME FROM EXAMS A , STUDENTS S WHERE EXISTS ( SELECT * FROM EXAMS B WHERE B . STUDENT_NO = A . STUDENT_NO AND B . SUB_NO <> A . SUB_NO AND B . STUDENT_NO = S . STUDENT_NO ); -- same as the previous query, but we joined the students and exam tables to retrieve student name. ANY, ALL operators \u00b6 MUST be used in sub-queries ANY = SOME, can be used interchangeably. similar to IN operator . ANY (SOME) returns true if any of the values returned by the sub-query equals the outer query column value used in the predicate. ALL returns true if all the values returned by the sub-query matches the predicate of the main query. SELECT SURNAME , INITIAL , DEPT_NO FROM LECTURERS A WHERE A . DEPT_NO = ANY ( SELECT B . DEPT_NO FROM DEPARTMENTS B WHERE BUDGET > 3000 ); -- retrieves THE NAMES OF the lecturers who work in department that has budget > 3000 SELECT SURNAME , INITIAL , DEPT_NO FROM LECTURERS A WHERE A . DEPT_NO <> ALL ( SELECT B . DEPT_NO FROM DEPARTMENTS B WHERE BUDGET > 3000 ); UNION clause \u00b6 allows to combine the output of two or more individual queries. these queries can be independent from each other. example here: https://i.imgur.com/p1CYwC3.png restrictions on UNION operations: all columns selected by each SELECT statement must be compatible. each query must select the same number of columns and each corresponding columns must be of the same type. each corresponding columns must have the same NOT NULL feature. UNION clause can NOT be used in sub-queries. the individual SELECT statement in the UNION can NOT use aggregate functions. the individual SELECT statement in the UNION can NOT use ORDER BY clause. UNION eliminates the duplicate rows from final results by default . you can use UNION ALL to include duplicate rows in the final results. although you can not use ORDER BY on the individual select statements, you can use it on the UNION as whole SELECT SURNAME , DEPT_NO FROM LECTURERS WHERE DEPT_NO = ( SELECT DEPT_NO FROM DEPARTMENTS WHERE DEPT_NAME = 'ENGINEERING' ) UNION SELECT SURNAME , DEPT_NO FROM STUDENTS WHERE DEPT_NO = ( SELECT DEPT_NO FROM DEPARTMENTS WHERE DEPT_NAME = 'ENGINEERING' ) ORDER BY 1 ; -- name of columns can not be used since the UNION does not retrieve column names -- ORDER BY is applied on the outer UNION level and can not be applied on the individual select statements References \u00b6 [1] Din, A. I. (2006). Structured Query Language (SQL): A practical introduction. NCC Blackwell. chapter 4. Retrieved from http://www.managedtime.com/freesqlbook.php .","title":"Unit 6: Querying the Database using SQL"},{"location":"knowledge-base/cs2203-database1/unit6/#unit-6-querying-the-database-using-sql","text":"Unit 6: Querying the Database using SQL Useful Definitions SELECT statement Cartesian Product Example: Cartesian product cartesian product of 3 tables WHERE Clause BETWEEN Operator IN operator LIKE operator ESCAPE Clause NULL operator AND, OR, NOT Operators ORDER BY clause aggregate functions COUNT() aggregate function SUM() aggregate function AVG() aggregate function MIN() and MAX() aggregate functions GROUP BY clause HAVING clause SQL JOINS self join nested SELECT statements (sub-queries) linked SELECT statements (correlated sub-query) EXISTS operator ANY, ALL operators UNION clause References","title":"Unit 6: Querying the Database using SQL"},{"location":"knowledge-base/cs2203-database1/unit6/#useful-definitions","text":"projection in a SQL statement: selecting a subset of the whole attribute set of a relation. eg. selecting only ID of a full table (select subset of columns). scalar expressions : simple calculations can be performed on NUMERIC type column values. predicate : logical expression that can either be true or false. synonyms : permanent aliases for table names, different from the normal table name aliases which only lasts while the query is being executed. available in ORACLE .","title":"Useful Definitions"},{"location":"knowledge-base/cs2203-database1/unit6/#select-statement","text":"most complex statement of ANSI/ISO SQL. has 6 different clauses. 2 required information: COLUMN_NAME and TABLE_NAME. the six clauses are: DISTINCT : eliminate the duplicate rows form the query, useful to know values in the table regardless of how many times they have appeared, opposite is ALL . FROM WHERE ORDER BY GROUP BY HAVING: eliminate part of the results depending on a condition. field_expression (in the image above) can be: field name eg. ID. (field names (columns) in SQL are limited to 24 characters length). ANSI aggregate function as: SUM(), AVG(), MIN(), MAX(), COUNT(). when using * in SELECT statement, columns in the result appear in the order they defined in the table. when specifying the column to select, the column will appear in the result in the order that specified in the statement. select statement allows you to use scalar expressions and constant string to specify column names. scalar expressions allow for dynamic or calculated column names, but the column MUST have numeric type or it will throw an error. scalar expressions in select statements lateral strings will appear for each row, they can be enclosed in single or double quotes. constant (lateral) strings in the select statements","title":"SELECT statement"},{"location":"knowledge-base/cs2203-database1/unit6/#cartesian-product","text":"when you specify more than one table in the FROM clause, this will produce the cartesian product of the result. for every row from the left set, a new row matching every single row of the right set will be generated. if we have 2 rows in the left set, and 3 rows in the right set, the final result will contain 6 rows as in the example below. The cartesian product is not limited to 2 tables, but this can grow quickly with adding more . this is not very useful in real-life, but good for science or AI purposes.","title":"Cartesian Product"},{"location":"knowledge-base/cs2203-database1/unit6/#example-cartesian-product","text":"we have 4 cars in the cars table and 2 rows in the sale table when we execute SELECT YEAR, Tax FROM cars, sale; we got a result of 8 rows, where every YEAR from cars appears in a row with every Tax from sale Notice that YEAR 1983 appears twice one with Tax 1.298 and one with Tax 1.656. and the same is true for EVERY YEAR .","title":"Example: Cartesian product"},{"location":"knowledge-base/cs2203-database1/unit6/#cartesian-product-of-3-tables","text":"Notice how the final results gone up to 48 row, although we have 4, 2, 4 rows in every table respectively.","title":"cartesian product of 3 tables"},{"location":"knowledge-base/cs2203-database1/unit6/#where-clause","text":"lets you specify predicate that tells SQL which rows should appear on the results. when processing a query with a predicate, DBMS goes through all the rows and evaluate the condition to true or false, this process can be speed up using indexes . operators allowed in WHERE clause include: >, <, =,<=, >=, <> , <> means not equal . mathematical operators of WHERE clause can work on NUMERIC, CHAR types. those operators can be used in CHAR based column types, the operating system matters in this case where it uses ASCII or EBCDIC system. it is also case sensitive. in ASCII , uppercase chars known has lower values than lowercase chars. using comparison operators with CHAR based values","title":"WHERE Clause"},{"location":"knowledge-base/cs2203-database1/unit6/#between-operator","text":"known as range test operator and allows to define a predicate in the form of a range. the BETWEEN range test determines the lower boundary of the range, and should be followed by AND to specify the upper boundary of the range. value of BETWEEN should be lower than the value of AND. upper and lower boundaries are inclusive . using between operator in NUMERIC and CHAR based columns. between does not add functionality to SQL, as it can be rephrased using WHERE, AND operators, aas shown in the photo below rephrase BETWEEN operator to use AND.","title":"BETWEEN Operator"},{"location":"knowledge-base/cs2203-database1/unit6/#in-operator","text":"know as set membership test operator . returns true when the value is in the provided set. set values must be comma separated in between parentheses. IN operator can be rephrased using OR operator. IN operator, and rephrase of IN using OR operator","title":"IN operator"},{"location":"knowledge-base/cs2203-database1/unit6/#like-operator","text":"known as pattern matching test operator . uses characters like %, _ as wildcard characters. useful when you don't know the exact word that you are searching for. _ indicates one valid character , while % indicates multiple chars . _ and % can be combined. <img src=\"https://i.imgur.com/Ab5F2UC.png\" using % and _ as wild characters in LIKE operator","title":"LIKE operator"},{"location":"knowledge-base/cs2203-database1/unit6/#escape-clause","text":"if % or _ are part of the string you need to use ESCAPE clause when using LIKE operator. you need to define a char (mostly $) before the char that you want to escape, and then add that char to ESCAPE clause. ESCAPE clause is not widely supported. using ESCAPE clause with LIKE operator so _ and % can be treated as normal letters","title":"ESCAPE Clause"},{"location":"knowledge-base/cs2203-database1/unit6/#null-operator","text":"known as value test operator . checks if a value has been set or not, empty string and 0 are NOT NULL . checks for NULL in unknown column will return all rows in the table or throws an error(depending on SQL and DBMS versions). works as column = NULL or column IS NULL . the opposite statement would be column <> NULL or column IS NOT NULL . using NULL, IS NULL, NOT NULL, IS NOT NULL operators. Note that column <p> NULL and column != NULL did not work on this version of SQL.","title":"NULL operator"},{"location":"knowledge-base/cs2203-database1/unit6/#and-or-not-operators","text":"known as Logical operators logical operator links multiple predicates within a single WHERE clause. you can group expressions by using parentheses.","title":"AND, OR, NOT Operators"},{"location":"knowledge-base/cs2203-database1/unit6/#order-by-clause","text":"by default SQL orders the results arbitrarily depending on the order in which the rows have been found which in turn depends on the location of these rows on the disk and the location of the objects representing these rows in server's memory . you can use ORDER BY clause with column name to order on. you can use more than one column name, then SQL will ORDER BY will use first column as primary ordering column, the second column as secondary and so on. ANSI/ISO standards requires the columns that you ORDER BY to appear in the results, so they have to be included in the SELECT statement. you can always use the column index (starts from 1) instead of the column name as: SELECT ID , NAME FROM T ORDER BY NAME DESC ; -- is equal to this statement SELECT ID , NAME FROM T ORDER BY 2 DESC ; -- 2 is the index of column NAME in the select statement, ID has index 1","title":"ORDER BY clause"},{"location":"knowledge-base/cs2203-database1/unit6/#aggregate-functions","text":"5 functions that can be used to summarize data in tables as they operate on the table data and generate a single value as output. COUNT() : number of rows or columns the query selects, no rows are returned except the count. SUM() . AVG() . MIN() . MAX() . you can NOT nest the aggregate functions, or mix regular columns and aggregate functions in the same query.","title":"aggregate functions"},{"location":"knowledge-base/cs2203-database1/unit6/#count-aggregate-function","text":"it has 2 types: counts and lists the number of all non-NULL values in a particular column, using DISTINCT keyword before the columns name . counts and displays the number of rows that would be retrieved by the query. SELECT COUNT ( DEPT_NO ) FROM STUDENTS ; -- 16 -- count the rows, second type. will return the number of all rows that has DEPT_NO in them SELECT COUNT ( DISTINCT DEPT_NO ) FROM STUDENTS ; -- 5 -- count the occurrence of each DEPT_NO, it happens that we have 5 different departments. -- null values for DEPT_NO are ignored. -- each DEPT_NO might be occurred more than once, but it only appears in the count for once. -- this is useful to count `how many different departments that we have that are assigned to students` SELECT COUNT ( * ) FROM STUDENTS ; -- count all rows, including null and duplicate rows.","title":"COUNT() aggregate function"},{"location":"knowledge-base/cs2203-database1/unit6/#sum-aggregate-function","text":"calculates the total of the values in a NUMERIC type column. it take the name of the column, or the name of the column in a scalar expression as an argument. results of SUM() sometimes has greater precision than the column itself. SELECT SUM ( PRICE ) FROM ITEMS ; -- name of the column SELECT SUM ( PRICE + ( PRICE * 0 . 2 )) FROM ITEMS ; -- name of the column in a scalar expression","title":"SUM() aggregate function"},{"location":"knowledge-base/cs2203-database1/unit6/#avg-aggregate-function","text":"calculates the average or arithmetic mean of the values in a NUMERIC type column, adding all values then divide by the number of rows. SELECT AVG ( PRICE ) FROM ITEMS ; -- average of all rows = sum / count (all rows) SELECT AVG ( PRICE ) FROM ITEMS WHERE ITEM_ID = 5 ; -- average of selected set of rows = sum / count (selected rows)","title":"AVG() aggregate function"},{"location":"knowledge-base/cs2203-database1/unit6/#min-and-max-aggregate-functions","text":"MIN() returns the smallest value in a column. it can work on NUMERIC, STRING, and non-ANSI types (such date and time) column types. MAX() is the opposite, but works on the same column types. both allow the use of scalar expressions and column name as arguments. SELECT MIN ( JOIN_DATE ) FROM T ; -- 01-01-2020 SELECT MAX ( JOIN_DATE ) FROM T ; -- 01-01-2022 SELECT MIN ( MARK ) FROM EXAMS ; -- 18 SELECT MAX ( 100 - MARK ) FROM EXAMS ; -- 82, as 100-18","title":"MIN() and MAX() aggregate functions"},{"location":"knowledge-base/cs2203-database1/unit6/#group-by-clause","text":"allows to split the values in a column into subsets , then apply the aggregate functions on those subsets might be very useful. we can group by single column or multiple columns . SELECT STUDENT_NO , AVG ( MARK ) FROM EXAMS GROUP BY STUDENT_NO ; -- results https://i.imgur.com/AmLRt9k.png -- this query splits up all exams into sets by STUDENT_NO, then calculates the AVG of the marks in each set. -- so it is calculating the average score for each student in all exams. -- we can get an equal query as: SELECT STUDENT_NO , AVG ( MARK ) FROM EXAMS WHERE STUDENT_NO = 1 ; -- then use a query for each student id. SELECT SUB_NAME , DEP_NO , CREDITS , MAX ( PASS ) FROM STUDENTS GROUP BY DEP_NO , CREDITS ; -- results https://i.imgur.com/RUxPVet.png -- we got split sets into combinations of all DEP_NO and CREDITS.","title":"GROUP BY clause"},{"location":"knowledge-base/cs2203-database1/unit6/#having-clause","text":"you can NOT use aggregate functions in the WHERE clause , so you can NOT use WHERE clause to eliminate the results that don't interest you. HAVING clause allows for filtering the results that are produced by the aggregate functions and GROUP BY . it accepts only one single value SELECT STUDENT_NO , AVG ( MARK ) FROM EXAMS GROUP BY STUDENT_NO HAVING AVG ( MARK ) > 65 ;","title":"HAVING clause"},{"location":"knowledge-base/cs2203-database1/unit6/#sql-joins","text":"JOIN is the ability to retrieve data from several different related tables, that's the power of rational databases. the tables to be joined should be named in the FROM clause as comma separated. the relationships between tables should be defined in the predicate or WHERE clause . if the columns are unique between all the tables int he join , we don't have to prefix the column name by the table name. but it is good practice to always use tableName.columnName notation in the join statement. we don't care about how SQL will actually do the join, DBMS will take care of that. the results are what important to us. when processing the JOIN, SQL will look up all possible combinations of the rows in the join tables , then evaluate them and decide to add each row to the results or not. and the steps are: construct a list of all possible row combinations from the tables in the JOIN. check if the predicate is true for each combination of rows . if the predicate is true, that row will be added to the results. once all possible rows have been checked, the results will be returned by the query.","title":"SQL JOINS"},{"location":"knowledge-base/cs2203-database1/unit6/#self-join","text":"joining 2 copies of the same table. data retrieved by self-join can not be retrieved by any other query. we can use table name aliases to distinguish the 2 copies from each other. SELECT L . SURNAME , R . SURNAME , L . DEPT_NO FROM LECTURERS L , LECTURERS R -- USING table name aliases to reference tables WHERE L . DEPT_NO = R . DEPT_NO -- predicate AND L . SURNAME <> R . SURNAME ; -- <> means not-equals (!=), eliminate so duplicate records.","title":"self join"},{"location":"knowledge-base/cs2203-database1/unit6/#nested-select-statements-sub-queries","text":"the results of a query becomes part of the predicate of another query (WHERE clause or HAVING clause) . DBMS will execute the sub-query first, then the results of the sub-query is being populated into the predicate. when using = in a predicate with sub-query: make sure that sub-query returns only single value like: selecting one column, a result of an aggregate function , and the type of the sub-query result should be the same of the predicate column. example: https://i.imgur.com/aG4Ran4.png we have to make sure that the sub-query also returns a value and not just empty. example https://i.imgur.com/OVkRjxd.png aggregate functions are allowed unless they are using GROUP BY, HAVING , even if the GROUP BY, or HAVING returned one value it will be rejected, because they are returning a set -even if it has one item- . SELECT * FROM EXAMS WHERE STUDENT_NO = ( SELECT STUDENT_NO FROM STUDENTS WHERE NAME = 'AHMAD' AND LASTNAME = 'ALI' ); -- SUB-QUERY, executed first SELECT SURNAME , PAY FROM LECTURERS WHERE PAY < ( SELECT AVG ( PAY ) FROM LECTURERS ); -- aggregate function that returns a single value. -- GET ALL lecturers that are getting paid less than average. the sub-query should always appear after the comparison operator on the predicate, adding the sub-query before the operator will throw an error. example https://i.imgur.com/79k1fOk.png if the sub-query returns a set, you can use the IN operator SELECT * FROM EXAMS WHERE STUDENT_NO IN ( SELECT STUDENT_NO FROM STUDENTS WHERE DEPT_NO = 1 ); -- sub-query returns a set, so we used `IN` operator ANSI/ISO standards don't have a limit on the level of query nesting, but practically it is very low.","title":"nested SELECT statements (sub-queries)"},{"location":"knowledge-base/cs2203-database1/unit6/#linked-select-statements-correlated-sub-query","text":"another method of extracting data of multiple tables by linking them through a sub-query. correlated sub-query : is a sub-query that refers to columns of the main table in the main query. example here https://i.imgur.com/6h8mvh9.png SELECT * FROM EXAMS WHERE SUB_NO IN -- candidate row ( SELECT SUB_NO FROM SUBJECTS WHERE SUBJECTS . PASS < EXAMS . MARK -- SUB-QUERY referencing main table `exams` -- this referred to as `outer reference`. ); -- this query selects all successful exams: -- - for each row in exams it loops through all rows in subject. -- - and gets all SUBJECT_NO where this exam.mark is considered successful. -- - it lastly compare the SUBJECT_NO of this exam with the retrieved successful SUBJECT_NO(s) correlated sub-query is executed once for each row of the outer table. outer reference : sub-query (correlated sub-query) that reference a column from the outer (main) table . candidate row : the current outer query for which the sub-query is being executed. detailed example on correlated sub-query here: https://i.imgur.com/WdiL1eD.png","title":"linked SELECT statements (correlated sub-query)"},{"location":"knowledge-base/cs2203-database1/unit6/#exists-operator","text":"similar to IN operator . MUST have sub-query as its argument. returns true if the sub-query finds any value , false if no values returned by the sub-query. SELECT * FROM SUBJECTS WHERE EXISTS ( SELECT * FROM SUBJECTS WHERE PASS > 75 -- will be executed once (not correlated) ); -- retrieves ```all rows``` in the SUBJECTS table, if there exists ```one subject``` with pass > 75. SELECT DISTINCT A . STUDENT_NO FROM EXAMS A WHERE EXISTS ( SELECT * FROM EXAMS B WHERE B . STUDENT_NO = A . STUDENT_NO AND B . SUB_NO <> A . SUB_NO -- WILL executed foreach outer row (correlated) ); -- retrieves all students that have taken more than one exam in different subjects. SELECT DISTINCT A . STUDENT_NO , S . SURNAME FROM EXAMS A , STUDENTS S WHERE EXISTS ( SELECT * FROM EXAMS B WHERE B . STUDENT_NO = A . STUDENT_NO AND B . SUB_NO <> A . SUB_NO AND B . STUDENT_NO = S . STUDENT_NO ); -- same as the previous query, but we joined the students and exam tables to retrieve student name.","title":"EXISTS operator"},{"location":"knowledge-base/cs2203-database1/unit6/#any-all-operators","text":"MUST be used in sub-queries ANY = SOME, can be used interchangeably. similar to IN operator . ANY (SOME) returns true if any of the values returned by the sub-query equals the outer query column value used in the predicate. ALL returns true if all the values returned by the sub-query matches the predicate of the main query. SELECT SURNAME , INITIAL , DEPT_NO FROM LECTURERS A WHERE A . DEPT_NO = ANY ( SELECT B . DEPT_NO FROM DEPARTMENTS B WHERE BUDGET > 3000 ); -- retrieves THE NAMES OF the lecturers who work in department that has budget > 3000 SELECT SURNAME , INITIAL , DEPT_NO FROM LECTURERS A WHERE A . DEPT_NO <> ALL ( SELECT B . DEPT_NO FROM DEPARTMENTS B WHERE BUDGET > 3000 );","title":"ANY, ALL operators"},{"location":"knowledge-base/cs2203-database1/unit6/#union-clause","text":"allows to combine the output of two or more individual queries. these queries can be independent from each other. example here: https://i.imgur.com/p1CYwC3.png restrictions on UNION operations: all columns selected by each SELECT statement must be compatible. each query must select the same number of columns and each corresponding columns must be of the same type. each corresponding columns must have the same NOT NULL feature. UNION clause can NOT be used in sub-queries. the individual SELECT statement in the UNION can NOT use aggregate functions. the individual SELECT statement in the UNION can NOT use ORDER BY clause. UNION eliminates the duplicate rows from final results by default . you can use UNION ALL to include duplicate rows in the final results. although you can not use ORDER BY on the individual select statements, you can use it on the UNION as whole SELECT SURNAME , DEPT_NO FROM LECTURERS WHERE DEPT_NO = ( SELECT DEPT_NO FROM DEPARTMENTS WHERE DEPT_NAME = 'ENGINEERING' ) UNION SELECT SURNAME , DEPT_NO FROM STUDENTS WHERE DEPT_NO = ( SELECT DEPT_NO FROM DEPARTMENTS WHERE DEPT_NAME = 'ENGINEERING' ) ORDER BY 1 ; -- name of columns can not be used since the UNION does not retrieve column names -- ORDER BY is applied on the outer UNION level and can not be applied on the individual select statements","title":"UNION clause"},{"location":"knowledge-base/cs2203-database1/unit6/#references","text":"[1] Din, A. I. (2006). Structured Query Language (SQL): A practical introduction. NCC Blackwell. chapter 4. Retrieved from http://www.managedtime.com/freesqlbook.php .","title":"References"},{"location":"knowledge-base/cs2203-database1/unit7/","text":"Unit 7: Database Programming \u00b6 Table of contents \u00b6 Unit 7: Database Programming Table of contents objectives SQL in applications transactions Embedded SQL static SQL SQL communications area, SQLCODE and SQLSTATE pre-compiling of embedded SQL dynamic SQL Static vs. dynamic SQL Database APIs ODBC and the IBM Data Server CLI driver pureQuery Client Optimizer References objectives \u00b6 The concept of transaction Working with embedded SQL The differences between static and dynamic SQL Database APIs like ODBC, CLI and JDBC An introduction to pureQuery SQL in applications \u00b6 host language : a programming language that you embed SQL commands in, like c, c++, java.. some techniques allow to use the API of the database itself to execute SQL, like: ODBC, CLI, JDBC. transactions \u00b6 transaction is a unit of work or a set of database operations which all they need to be executed successfully in order to call the transaction successful. at the end of a transaction you can commit or roll back to maintain data integrity. Embedded SQL \u00b6 embed SQL commands inside a high-level programming language. compiling these applications needs 2 steps: pre-compilation: the DBMS will provide a pre-compiler compatible with the programming language that will compile embedded SQL into the DBMS run-time API calls. normal compilation of the host language where the pre-compiled code get linked to the host language development tools. in c, c++, COBOL the statement initializer EXEC SQL is used to identify embedded SQL, as: EXEC SQL UPDATE employee . details SET emp_desig = ' Mgr ' WHERE emp_desig = ' Asst Mgr ' ; in java, applications with embedded SQL called SQLJ and the statement initializer is #sql , as: # sql { UPDATE employee . details SET emp_desig = ' Mgr ' WHERE emp_desig = ' Asst Mgr ' }; static SQL \u00b6 Static SQL applications are designed for scenarios where the applications need to issue the same SQL statements every time it interacts with the database An embedded SQL application where the syntax of the SQL is fully known beforehand and where the SQL statements are hard-coded within the source code of the application is known as a static embedded SQL application . The only input(s) that can be fed to the SQL statements from the application are the actual data values that need to be inserted into the table or the predicate values of the SQL statements. These input values are provided to the SQL statements using host variables . host variables are programming language variables that should only be used for static SQL processing. These host variables need to be declared in the application prior to using them. a good practice is to append the host variable names with \u2018_hv\u2019 to differentiate them from other variables as well as from column names SQL communications area, SQLCODE and SQLSTATE \u00b6 The SQL Communications Area (SQLCA) is a data structure that is used as a communication medium between the database server and its clients. The SQLCA data structure comprises of a number of variables that are updated at the end of each SQL execution SQLCODE is one such variable in SQLCA which is set to 0 (zero) after every successful SQL execution. If the SQL statement completes with a warning, it is set with a positive, non-zero value; and if the SQL statement returns an error, it is set with a negative value. it correspond to either hardware-specific or operating system-specific issues; SQLSTATE is another variable in SQLCA which stores a return code as a string that also indicates the outcome of the most recently executed SQL statement. However, SQLSTATE provides a more generic message that is standardized across different database vendor products. pre-compiling of embedded SQL \u00b6 The pre-compiler checks for SQL statements within the source code, replaces them with equivalent runtime APIs supported by the host language and re-writes the entire output (with commented SQL statements) into a new file which can then be compiled and linked using the host language development tools. pre-compiler performs the following tasks: It validates the SQL syntax for each coded SQL statement and ensures that appropriate data types are used for host variables by comparing them with their respective column types. It also determines the data conversion methods to be used while fetching or inserting the data into the database. It evaluates references to database objects, creates access plans for them and stores them in a package in the database. An access plan of an SQL statement is the most optimized path to data objects that the SQL statement will reference each application is bound to its respective package residing on the database. so that every time the application is run, the access plan for the corresponding SQL statement is fetched from the package and used. This makes SQL execution very fast in the database, since the most optimized way of accessing the required database objects is already known in advance. we could redefine static SQL statements to be the ones whose access plan can be determined and known in advance and stored in the database for faster execution. Thus for an SQL statement to be embedded statically in an application, its syntax must be known at pre-compile time . source: [1] dynamic SQL \u00b6 Dynamic SQL statements include parameters whose values are not known until runtime, when they are supplied as input to the application. the access plans for dynamic SQL queries can only be generated at runtime . parameter marker is the question mark (?) used in a statement,used in place of predicate values. statements with parameter markers need access plan before they get executed , so we prepare the statement to get the access plan. the exact SQL that needs to be issued is generated only at execution time where we have to replace the marker with an actual value to the predicate. applications with dynamic SQL contains some static SQL statements needed for statement preparation, cursor declarations, and so on . This would mean that such applications would also require pre-compilation . If there is a way to replace all such static SQL statements by equivalent APIs, the pre-compilation/SQL translation of the application would not be required at all . Static vs. dynamic SQL \u00b6 Unlike static SQL statements, access plans for dynamic statements are generated only at runtime; hence, dynamic statements need to be prepared in the application. The time taken to generate an access plan at runtime makes dynamic SQL applications a little slower than static SQL. However, they offer much more flexibility to application developers and hence, are more robust than static SQL applications. Sometimes a dynamic SQL statement performs better than its static SQL counterpart, because it is able to exploit the latest statistics available in the database at the time of execution. The access plan generated for a static SQL statement is stored in advance and may become outdated in case certain database statistics change, which is not the case with dynamic SQL whenever the application is modified or upgraded, If the static SQL part of the application is modified, then regeneration of access plans would be needed. This means pre-compilation of the application and rebinding of packages would have to be done again. In the case of dynamic SQL execution, since the access plans are generated at runtime, pre-compilation and rebinding is not needed. Database APIs \u00b6 a connection to the database is required while pre-compiling the embedded SQL application, since in order to generate access plans, statistics information from the database catalog is required. Database APIs are a set of APIs exposed by database vendors pertaining to different programming languages like C, C++, Java and so on, which gives application developers a mechanism to interact with the database from within the application by just calling these SQL callable interfaces. database connectivity driver is tThe intermediate layer between the application and the database server, which makes this interaction possible. The database vendors themselves provide these drivers and once the driver libraries are linked with the source code libraries, the application source code can be easily compiled and then executed. Applications can now be developed independently of target databases, without the need for database connection at the compilation phase. ODBC and the IBM Data Server CLI driver \u00b6 CLI here means Call level Interface. the X/Open Company and the SQL Access Group jointly developed a specification for a callable SQL interface referred to as the X/Open Call Level Interface. which accepted as part of the ISO Call Level Interface International Standard in 1995 . Microsoft developed a callable SQL interface called Open Database Connectivity (ODBC) . The IBM Data Server CLI driver is the DB2 Call level Interface which is based on the Microsoft\u00ae ODBC specifications, and the International Standard for SQL/CLI. The DB2 CLI is a C and C++ is an API for relational database access that uses function calls to pass dynamic SQL statements as function arguments, means that there would be no need for any static EXEC SQL statements in the application. SQLCHAR * stmt = ( SQLCHAR * ) \"UPDATE employee.details SET emp_id = ? WHERE emp_name = ? \" ; /* prepare the statement */ int rc = SQLPrepare ( hstmt , stmt , SQL_NTS ); IBM CLI offers other callable interfaces like SQLConnect(), SQLFetch(), SQLExecute, and so on. The ODBC specifications also includes an operating environment, where database specific ODBC Drivers are dynamically loaded at runtime by a driver manager based on the data source (database name) provided on the connect request . source: [1] Java Database Connectivity (JDBC) : it is an SQL application programming interface similar to ODBC and CLI, but for Java applications. pureQuery : is a platform offered to Java developers that exploits the advantages of dynamic SQL without having to bother about preparation and object mapping overheads. //The name, country, street and province are variables which would be populated at runtime using user inputs. String sqlSel = \"select \u201c+name+\u201d, \u201d +country+\u201d, \u201c+street+\u201d, \u201c+province+\u201d, \u201c+zip+\u201d from CUSTOMER where Customer = ?\" ; Data data = DataFactory . getData ( con ); //execute the Select and get the list of customer List < Customer > customerList = data . queryList ( sqlSel , Customer . class , \"custCountry\" ); // pureQuery API queryList will execute the sqlSel statement with the predicate value \u201ccustCountry\u201d // and return the query results into cutomerList. pureQuery Client Optimizer \u00b6 Using the pureQuery Client Optimizer, the following steps need to be undertaken: When the dynamic application is first run, the pureQuery Client Optimizer captures the different SQL statements issued from an application into a pureQueryXml capture . The captured SQL statements in the XML file are then divided into packages , by running the command line tool Configure . Using the staticBinder , these packages are then created on the database server and the application is bound to these packages. Finally, the execution mode of the application needs to be set to static , which allows some of the SQL statements to run in static mode. each SQL issued from the application is matched with the SQL statements captured in the capture file. As soon as a match is found, the corresponding package details are fetched from the capture file and since the SQL is already bound to the corresponding package at the database server, the statement can be run statically. Each new SQL, which does not find a match, stills runs in dynamic execution mode. References \u00b6 [1] Sharma, N., Perniu, L., Chong, R. F., Iyer, A., Nandan, C., Mitea, A. C., Nonvinkere, M., & Danubianu, M. (2010). Databases fundamentals. chapter 7.","title":"Unit 7: Database Programming"},{"location":"knowledge-base/cs2203-database1/unit7/#unit-7-database-programming","text":"","title":"Unit 7: Database Programming"},{"location":"knowledge-base/cs2203-database1/unit7/#table-of-contents","text":"Unit 7: Database Programming Table of contents objectives SQL in applications transactions Embedded SQL static SQL SQL communications area, SQLCODE and SQLSTATE pre-compiling of embedded SQL dynamic SQL Static vs. dynamic SQL Database APIs ODBC and the IBM Data Server CLI driver pureQuery Client Optimizer References","title":"Table of contents"},{"location":"knowledge-base/cs2203-database1/unit7/#objectives","text":"The concept of transaction Working with embedded SQL The differences between static and dynamic SQL Database APIs like ODBC, CLI and JDBC An introduction to pureQuery","title":"objectives"},{"location":"knowledge-base/cs2203-database1/unit7/#sql-in-applications","text":"host language : a programming language that you embed SQL commands in, like c, c++, java.. some techniques allow to use the API of the database itself to execute SQL, like: ODBC, CLI, JDBC.","title":"SQL in applications"},{"location":"knowledge-base/cs2203-database1/unit7/#transactions","text":"transaction is a unit of work or a set of database operations which all they need to be executed successfully in order to call the transaction successful. at the end of a transaction you can commit or roll back to maintain data integrity.","title":"transactions"},{"location":"knowledge-base/cs2203-database1/unit7/#embedded-sql","text":"embed SQL commands inside a high-level programming language. compiling these applications needs 2 steps: pre-compilation: the DBMS will provide a pre-compiler compatible with the programming language that will compile embedded SQL into the DBMS run-time API calls. normal compilation of the host language where the pre-compiled code get linked to the host language development tools. in c, c++, COBOL the statement initializer EXEC SQL is used to identify embedded SQL, as: EXEC SQL UPDATE employee . details SET emp_desig = ' Mgr ' WHERE emp_desig = ' Asst Mgr ' ; in java, applications with embedded SQL called SQLJ and the statement initializer is #sql , as: # sql { UPDATE employee . details SET emp_desig = ' Mgr ' WHERE emp_desig = ' Asst Mgr ' };","title":"Embedded SQL"},{"location":"knowledge-base/cs2203-database1/unit7/#static-sql","text":"Static SQL applications are designed for scenarios where the applications need to issue the same SQL statements every time it interacts with the database An embedded SQL application where the syntax of the SQL is fully known beforehand and where the SQL statements are hard-coded within the source code of the application is known as a static embedded SQL application . The only input(s) that can be fed to the SQL statements from the application are the actual data values that need to be inserted into the table or the predicate values of the SQL statements. These input values are provided to the SQL statements using host variables . host variables are programming language variables that should only be used for static SQL processing. These host variables need to be declared in the application prior to using them. a good practice is to append the host variable names with \u2018_hv\u2019 to differentiate them from other variables as well as from column names","title":"static SQL"},{"location":"knowledge-base/cs2203-database1/unit7/#sql-communications-area-sqlcode-and-sqlstate","text":"The SQL Communications Area (SQLCA) is a data structure that is used as a communication medium between the database server and its clients. The SQLCA data structure comprises of a number of variables that are updated at the end of each SQL execution SQLCODE is one such variable in SQLCA which is set to 0 (zero) after every successful SQL execution. If the SQL statement completes with a warning, it is set with a positive, non-zero value; and if the SQL statement returns an error, it is set with a negative value. it correspond to either hardware-specific or operating system-specific issues; SQLSTATE is another variable in SQLCA which stores a return code as a string that also indicates the outcome of the most recently executed SQL statement. However, SQLSTATE provides a more generic message that is standardized across different database vendor products.","title":"SQL communications area, SQLCODE and SQLSTATE"},{"location":"knowledge-base/cs2203-database1/unit7/#pre-compiling-of-embedded-sql","text":"The pre-compiler checks for SQL statements within the source code, replaces them with equivalent runtime APIs supported by the host language and re-writes the entire output (with commented SQL statements) into a new file which can then be compiled and linked using the host language development tools. pre-compiler performs the following tasks: It validates the SQL syntax for each coded SQL statement and ensures that appropriate data types are used for host variables by comparing them with their respective column types. It also determines the data conversion methods to be used while fetching or inserting the data into the database. It evaluates references to database objects, creates access plans for them and stores them in a package in the database. An access plan of an SQL statement is the most optimized path to data objects that the SQL statement will reference each application is bound to its respective package residing on the database. so that every time the application is run, the access plan for the corresponding SQL statement is fetched from the package and used. This makes SQL execution very fast in the database, since the most optimized way of accessing the required database objects is already known in advance. we could redefine static SQL statements to be the ones whose access plan can be determined and known in advance and stored in the database for faster execution. Thus for an SQL statement to be embedded statically in an application, its syntax must be known at pre-compile time . source: [1]","title":"pre-compiling of embedded SQL"},{"location":"knowledge-base/cs2203-database1/unit7/#dynamic-sql","text":"Dynamic SQL statements include parameters whose values are not known until runtime, when they are supplied as input to the application. the access plans for dynamic SQL queries can only be generated at runtime . parameter marker is the question mark (?) used in a statement,used in place of predicate values. statements with parameter markers need access plan before they get executed , so we prepare the statement to get the access plan. the exact SQL that needs to be issued is generated only at execution time where we have to replace the marker with an actual value to the predicate. applications with dynamic SQL contains some static SQL statements needed for statement preparation, cursor declarations, and so on . This would mean that such applications would also require pre-compilation . If there is a way to replace all such static SQL statements by equivalent APIs, the pre-compilation/SQL translation of the application would not be required at all .","title":"dynamic SQL"},{"location":"knowledge-base/cs2203-database1/unit7/#static-vs-dynamic-sql","text":"Unlike static SQL statements, access plans for dynamic statements are generated only at runtime; hence, dynamic statements need to be prepared in the application. The time taken to generate an access plan at runtime makes dynamic SQL applications a little slower than static SQL. However, they offer much more flexibility to application developers and hence, are more robust than static SQL applications. Sometimes a dynamic SQL statement performs better than its static SQL counterpart, because it is able to exploit the latest statistics available in the database at the time of execution. The access plan generated for a static SQL statement is stored in advance and may become outdated in case certain database statistics change, which is not the case with dynamic SQL whenever the application is modified or upgraded, If the static SQL part of the application is modified, then regeneration of access plans would be needed. This means pre-compilation of the application and rebinding of packages would have to be done again. In the case of dynamic SQL execution, since the access plans are generated at runtime, pre-compilation and rebinding is not needed.","title":"Static vs. dynamic SQL"},{"location":"knowledge-base/cs2203-database1/unit7/#database-apis","text":"a connection to the database is required while pre-compiling the embedded SQL application, since in order to generate access plans, statistics information from the database catalog is required. Database APIs are a set of APIs exposed by database vendors pertaining to different programming languages like C, C++, Java and so on, which gives application developers a mechanism to interact with the database from within the application by just calling these SQL callable interfaces. database connectivity driver is tThe intermediate layer between the application and the database server, which makes this interaction possible. The database vendors themselves provide these drivers and once the driver libraries are linked with the source code libraries, the application source code can be easily compiled and then executed. Applications can now be developed independently of target databases, without the need for database connection at the compilation phase.","title":"Database APIs"},{"location":"knowledge-base/cs2203-database1/unit7/#odbc-and-the-ibm-data-server-cli-driver","text":"CLI here means Call level Interface. the X/Open Company and the SQL Access Group jointly developed a specification for a callable SQL interface referred to as the X/Open Call Level Interface. which accepted as part of the ISO Call Level Interface International Standard in 1995 . Microsoft developed a callable SQL interface called Open Database Connectivity (ODBC) . The IBM Data Server CLI driver is the DB2 Call level Interface which is based on the Microsoft\u00ae ODBC specifications, and the International Standard for SQL/CLI. The DB2 CLI is a C and C++ is an API for relational database access that uses function calls to pass dynamic SQL statements as function arguments, means that there would be no need for any static EXEC SQL statements in the application. SQLCHAR * stmt = ( SQLCHAR * ) \"UPDATE employee.details SET emp_id = ? WHERE emp_name = ? \" ; /* prepare the statement */ int rc = SQLPrepare ( hstmt , stmt , SQL_NTS ); IBM CLI offers other callable interfaces like SQLConnect(), SQLFetch(), SQLExecute, and so on. The ODBC specifications also includes an operating environment, where database specific ODBC Drivers are dynamically loaded at runtime by a driver manager based on the data source (database name) provided on the connect request . source: [1] Java Database Connectivity (JDBC) : it is an SQL application programming interface similar to ODBC and CLI, but for Java applications. pureQuery : is a platform offered to Java developers that exploits the advantages of dynamic SQL without having to bother about preparation and object mapping overheads. //The name, country, street and province are variables which would be populated at runtime using user inputs. String sqlSel = \"select \u201c+name+\u201d, \u201d +country+\u201d, \u201c+street+\u201d, \u201c+province+\u201d, \u201c+zip+\u201d from CUSTOMER where Customer = ?\" ; Data data = DataFactory . getData ( con ); //execute the Select and get the list of customer List < Customer > customerList = data . queryList ( sqlSel , Customer . class , \"custCountry\" ); // pureQuery API queryList will execute the sqlSel statement with the predicate value \u201ccustCountry\u201d // and return the query results into cutomerList.","title":"ODBC and the IBM Data Server CLI driver"},{"location":"knowledge-base/cs2203-database1/unit7/#purequery-client-optimizer","text":"Using the pureQuery Client Optimizer, the following steps need to be undertaken: When the dynamic application is first run, the pureQuery Client Optimizer captures the different SQL statements issued from an application into a pureQueryXml capture . The captured SQL statements in the XML file are then divided into packages , by running the command line tool Configure . Using the staticBinder , these packages are then created on the database server and the application is bound to these packages. Finally, the execution mode of the application needs to be set to static , which allows some of the SQL statements to run in static mode. each SQL issued from the application is matched with the SQL statements captured in the capture file. As soon as a match is found, the corresponding package details are fetched from the capture file and since the SQL is already bound to the corresponding package at the database server, the statement can be run statically. Each new SQL, which does not find a match, stills runs in dynamic execution mode.","title":"pureQuery Client Optimizer"},{"location":"knowledge-base/cs2203-database1/unit7/#references","text":"[1] Sharma, N., Perniu, L., Chong, R. F., Iyer, A., Nandan, C., Mitea, A. C., Nonvinkere, M., & Danubianu, M. (2010). Databases fundamentals. chapter 7.","title":"References"},{"location":"knowledge-base/cs2203-database1/unit8/","text":"Unit 8: Database Development Process \u00b6 table of contents \u00b6 Unit 8: Database Development Process table of contents definitions waterfall lifecycle Database Life Cycle 1. requirements gathering 2. Analysis 3. Logical Design 4. Implementation 5. Realizing the Design 6. Populating the Database References definitions \u00b6 software development life cycle (SDLC) : the collection of steps or phases in the product development, each step (phase) focuses on one aspect of the development. Database application development : is the process of obtaining real-world requirements, analyzing requirements, designing the data and functions of the system, and then implementing the operations in the system. Flexing : capture the simultaneous ideas of bending something for a different purpose and weakening aspects of it as it is bent. Bulk Load : The transfer of large quantities of existing data into a database, one table at a time, some DBMS facilities to postpone constraint checking until the end of the bulk loading . waterfall lifecycle \u00b6 strict sequence of steps where the output of one step is the input to the next and all of one step has to be completed before moving onto the next. seeps of waterfall model: Establishing requirements involves consultation with, and agreement among, stakeholders about what they want from a system, expressed as a statement of requirements. Analysis starts by considering the statement of requirements and finishes by producing a system specification. The specification is a formal representation of what a system should do, expressed in terms that are independent of how it may be realized. Design begins with a system specification, produces design documents and provides a detailed description of how the system should be conducted. Implementation is the construction of a computer system according to a given design document and taking into account the environment in which the system will be operating (e.g., specific hardware or software available for the development). Implementation may be staged, usually with an initial system that can be validated and tested before a final system is released for use. Testing compares the implemented system against the design documents and requirements specification and produces an acceptance report or, more usually, a list of errors and bugs that require a review of the analysis, design and implementation processes to correct (testing is usually the task that leads to the waterfall model iterating through the life cycle). Maintenance involves dealing with changes in the requirements or the implementation environment, bug fixing or porting of the system to new environments (e.g., migrating a system from a standalone PC to a UNIX workstation or a networked environment). Since maintenance involves the analysis of the changes required, design of a solution, implementation and testing of that solution over the lifetime of a maintained software system, the waterfall life cycle will be repeatedly revisited. Database Life Cycle \u00b6 We can separate the development of a database \u2013 that is, specification and creation of a schema to define data in a database \u2013 from the user processes that make use of the database. We can use the three-schema architecture as a basis for distinguishing the activities associated with a schema. We can represent the constraints to enforce the semantics of the data once within a database, rather than within every user process that uses the data. database life cycle source [1] requirements > conceptual data model > logical data model > physical data model > testing 1. requirements gathering \u00b6 interview the customer to get as much as info as you can. required agreement among all of the users about the information that will be stored in the DB. The data administrator plays a key role in this process as they overview the business, legal and ethical issues within the organization that impact on the data requirements. data requirements document is used to confirm the understanding of requirements with users. requirements should not describe how data is to be processed, but what the data items are?, their attributes?, any constraints or relationships? 2. Analysis \u00b6 Data analysis begins with the statement of data requirements and then produces a conceptual data model . The aim of analysis is to obtain a detailed description of the data that will suit user requirements so that both high and low level properties of data and their use are dealt with. conceptual data model is concerned with the meaning and structure of data, but not with the details affecting how they are implemented . 3. Logical Design \u00b6 Database design starts with a conceptual data model and produces a specification of a logical schema . this will determine the specific type of database system (network, relational, object-oriented) that is required. The relational representation is still independent of any specific DBMS; it is another conceptual data model . The output of this stage is a detailed relational specification, the logical schema, of all the tables and constraints needed to satisfy the description of the data in the conceptual data model. Database designers familiar with relational databases and SQL might be tempted to go directly to implementation after they have produced a conceptual data model. However, such a direct transformation of the relational representation to SQL tables does not necessarily result in a database that has all the desirable properties: completeness, integrity, flexibility, efficiency and usability . 4. Implementation \u00b6 Implementation involves the construction of a database according to the specification of a logical schema. This will include the specification of an appropriate storage schema, security enforcement, external schema and so on. implementation of the logical schema in a given DBMS requires a very detailed knowledge of the specific features and facilities that the DBMS has to offer. 5. Realizing the Design \u00b6 One way to achieve this is to write the appropriate SQL DDL statements into a file that can be executed by a DBMS so that there is an independent record, a text file, of the SQL statements defining the database. Another method is to work interactively using a database tool like SQL Server Management Studio or Microsoft Access. 6. Populating the Database \u00b6 After a database has been created, there are two ways of populating the tables either from existing data or through the use of the user applications developed for the database. when the first approach is used, the simplest approach to populate the database is to use the import and export facilities found in the DBMS. References \u00b6 [1] Watt, A., & Eng, N. (2014). Database design, 2 nd ed. BCcampus, BC Open Textbook Project. Retrieved from https://opentextbc.ca/dbdesign01/ .","title":"Unit 8: Database Development Process"},{"location":"knowledge-base/cs2203-database1/unit8/#unit-8-database-development-process","text":"","title":"Unit 8: Database Development Process"},{"location":"knowledge-base/cs2203-database1/unit8/#table-of-contents","text":"Unit 8: Database Development Process table of contents definitions waterfall lifecycle Database Life Cycle 1. requirements gathering 2. Analysis 3. Logical Design 4. Implementation 5. Realizing the Design 6. Populating the Database References","title":"table of contents"},{"location":"knowledge-base/cs2203-database1/unit8/#definitions","text":"software development life cycle (SDLC) : the collection of steps or phases in the product development, each step (phase) focuses on one aspect of the development. Database application development : is the process of obtaining real-world requirements, analyzing requirements, designing the data and functions of the system, and then implementing the operations in the system. Flexing : capture the simultaneous ideas of bending something for a different purpose and weakening aspects of it as it is bent. Bulk Load : The transfer of large quantities of existing data into a database, one table at a time, some DBMS facilities to postpone constraint checking until the end of the bulk loading .","title":"definitions"},{"location":"knowledge-base/cs2203-database1/unit8/#waterfall-lifecycle","text":"strict sequence of steps where the output of one step is the input to the next and all of one step has to be completed before moving onto the next. seeps of waterfall model: Establishing requirements involves consultation with, and agreement among, stakeholders about what they want from a system, expressed as a statement of requirements. Analysis starts by considering the statement of requirements and finishes by producing a system specification. The specification is a formal representation of what a system should do, expressed in terms that are independent of how it may be realized. Design begins with a system specification, produces design documents and provides a detailed description of how the system should be conducted. Implementation is the construction of a computer system according to a given design document and taking into account the environment in which the system will be operating (e.g., specific hardware or software available for the development). Implementation may be staged, usually with an initial system that can be validated and tested before a final system is released for use. Testing compares the implemented system against the design documents and requirements specification and produces an acceptance report or, more usually, a list of errors and bugs that require a review of the analysis, design and implementation processes to correct (testing is usually the task that leads to the waterfall model iterating through the life cycle). Maintenance involves dealing with changes in the requirements or the implementation environment, bug fixing or porting of the system to new environments (e.g., migrating a system from a standalone PC to a UNIX workstation or a networked environment). Since maintenance involves the analysis of the changes required, design of a solution, implementation and testing of that solution over the lifetime of a maintained software system, the waterfall life cycle will be repeatedly revisited.","title":"waterfall lifecycle"},{"location":"knowledge-base/cs2203-database1/unit8/#database-life-cycle","text":"We can separate the development of a database \u2013 that is, specification and creation of a schema to define data in a database \u2013 from the user processes that make use of the database. We can use the three-schema architecture as a basis for distinguishing the activities associated with a schema. We can represent the constraints to enforce the semantics of the data once within a database, rather than within every user process that uses the data. database life cycle source [1] requirements > conceptual data model > logical data model > physical data model > testing","title":"Database Life Cycle"},{"location":"knowledge-base/cs2203-database1/unit8/#1-requirements-gathering","text":"interview the customer to get as much as info as you can. required agreement among all of the users about the information that will be stored in the DB. The data administrator plays a key role in this process as they overview the business, legal and ethical issues within the organization that impact on the data requirements. data requirements document is used to confirm the understanding of requirements with users. requirements should not describe how data is to be processed, but what the data items are?, their attributes?, any constraints or relationships?","title":"1. requirements gathering"},{"location":"knowledge-base/cs2203-database1/unit8/#2-analysis","text":"Data analysis begins with the statement of data requirements and then produces a conceptual data model . The aim of analysis is to obtain a detailed description of the data that will suit user requirements so that both high and low level properties of data and their use are dealt with. conceptual data model is concerned with the meaning and structure of data, but not with the details affecting how they are implemented .","title":"2. Analysis"},{"location":"knowledge-base/cs2203-database1/unit8/#3-logical-design","text":"Database design starts with a conceptual data model and produces a specification of a logical schema . this will determine the specific type of database system (network, relational, object-oriented) that is required. The relational representation is still independent of any specific DBMS; it is another conceptual data model . The output of this stage is a detailed relational specification, the logical schema, of all the tables and constraints needed to satisfy the description of the data in the conceptual data model. Database designers familiar with relational databases and SQL might be tempted to go directly to implementation after they have produced a conceptual data model. However, such a direct transformation of the relational representation to SQL tables does not necessarily result in a database that has all the desirable properties: completeness, integrity, flexibility, efficiency and usability .","title":"3. Logical Design"},{"location":"knowledge-base/cs2203-database1/unit8/#4-implementation","text":"Implementation involves the construction of a database according to the specification of a logical schema. This will include the specification of an appropriate storage schema, security enforcement, external schema and so on. implementation of the logical schema in a given DBMS requires a very detailed knowledge of the specific features and facilities that the DBMS has to offer.","title":"4. Implementation"},{"location":"knowledge-base/cs2203-database1/unit8/#5-realizing-the-design","text":"One way to achieve this is to write the appropriate SQL DDL statements into a file that can be executed by a DBMS so that there is an independent record, a text file, of the SQL statements defining the database. Another method is to work interactively using a database tool like SQL Server Management Studio or Microsoft Access.","title":"5. Realizing the Design"},{"location":"knowledge-base/cs2203-database1/unit8/#6-populating-the-database","text":"After a database has been created, there are two ways of populating the tables either from existing data or through the use of the user applications developed for the database. when the first approach is used, the simplest approach to populate the database is to use the import and export facilities found in the DBMS.","title":"6. Populating the Database"},{"location":"knowledge-base/cs2203-database1/unit8/#references","text":"[1] Watt, A., & Eng, N. (2014). Database design, 2 nd ed. BCcampus, BC Open Textbook Project. Retrieved from https://opentextbc.ca/dbdesign01/ .","title":"References"},{"location":"knowledge-base/cs2203-database1/unit9/","text":"Unit 9: Data integrity, Views \u00b6 table of contents \u00b6 Unit 9: Data integrity, Views table of contents Data integrity NOT NULL columns data validity table (Entity) integrity referential integrity SQL TRIGGERS VIEWS updating VIEWS database security transactions deadlock problem DBMS catalog LABEL, COMMENT command References Data integrity \u00b6 ensure that all new data are compatible with the existing relationships. divided into 4 categories: NOT NULL columns: some columns must have values. data validity: ensures that the right values are inserted in columns. table integrity referential integrity. NOT NULL columns \u00b6 checks apply to all INSERT / UPDATE statements. data validity \u00b6 DBMS check that data type matches column type by default. ORACLE has data validity checking built into its data entry form package . it can also be at the application level. table (Entity) integrity \u00b6 primary key must be unique within a table. referential integrity \u00b6 all child rows must have a parent. the foreign key (child) which references a primary key (parent) from other table, when a changes happens on the parent side, this change should be populated to all its children. example https://i.imgur.com/iBBBxfX.png when defining a foreign key, if you don't define the column then it will be assumed to be the primary key . when defining the foreign key, you can choose what will happen when the parent get changed or deleted: prevent delete operation on the parent: https://i.imgur.com/SDHoCTt.png cascade changes to the children: https://i.imgur.com/jKVLlYt.png set children to NULL: https://i.imgur.com/qIVPWUS.png SQL TRIGGERS \u00b6 trigger : is a set of operations that the DBMS must execute whenever there is a change in the content of a table. CREATE TRIGGER UPDATE_BUDGET ON DEPARTMENTS FOR UPDATE AS UPDATE DEPARTMENTS SET PREVIOUS_BUDGET = BUDGET FROM DEPARTMENTS , INSERTED WHERE DEPARTMENTS . DEPT_NO = INSERTED . DEPT_NO -- CREATES a trigger to save the old budget values into previous_budget column whenever -- departments table gets updated. VIEWS \u00b6 the content of views are retrieved from other tables. benefits of views: database security. restrict users access to only parts of a table. data integrity. insert data into a view and validate it before inserting in the actual table . shield form change. when the actual table structure change, the VIEW remains constant. easier querying. save repetitive complex queries into a VIEW, and access it easily. UNION are not allowed in VIEW, since view can only have one query. VERTICAL VIEW : the view has access to few columns of all table rows . HORIZONTAL VIEW : the view have access to all columns of a few table rows. you can always mix horizontal and vertical VIEWS. CREATE VIEW FRESHMEN AS SELECT * FROM STUDENTS WHERE STUDENTS . YEAR = 1 ; -- vertical view CREATE VIEW FRESHMEN AS SELECT NAME , SURNAME FROM STUDENTS ; -- HORIZONTAL VIEW CREATE VIEW FRESHMEN AS SELECT NAME , SURNAME FROM STUDENTS WHERE STUDENTS . YEAR = 1 ; -- MIXED HORIZONTAL, VERTICAL VIEW SELECT * FROM FRESHMEN ; -- QUERYING A VIEW, ``DO QUERY ON A QUERY`` JOIN a VIEW to a base table is completely legal. VIEWS can be used with sub-queries. updating VIEWS \u00b6 updating data through a view can represent some issues. unlike base tables, VIEWS allow you to use aggregate functions in CREATE VIEW statements, these fields can not be updated later . for a VIEW to be updatable: the VIEW should specify one base table. VIEW should not have aggregate functions on its definition. VIEW should not use GROUP BY, or HAVING. VIEW should not use DISTINCT. VIEW should select only simple columns. scalar expressions or string constants are not allowed. VIEW must include all columns from the base table with NOT NULL modifier. WITH CHECK OPTION is a modifier that can be added to the CREATE VIEW statement, and it will instruct SQL to check INSERT/UPDATE operations on the VIEW against the VIEW predicate. if the values in the INSERT/UPDATE conflict with the VIEW predicate, the operation will be rejected. it is a good practice to use WITH CHECK OPTION modifier in updatable VIEWS . removal of a VIEW will npt affect the base table. database security \u00b6 ANSI/ISO standards define four privileges: SELECT, INSERT, UPDATE, DELETE. table owner can GRANT/REVOKE access to other users. GRANT statement REVOKE statement you can use VIEWS to limit users access to columns in a table, the user must have SELECT privileges on the base table. GRANT UPDATE allows you to specify individual columns that the granted user is allowed to update. WITH GRANT OPTION modifier can be added to the GRANT statement allowing the grantee to grant privileges to other users REVOKE only allows you to revoke privileges from users that you have granted, and it cascades down the chain to any users they have given privileges GRANT ALL PRIVILEGES ON TABLE_NAME TO PUBLIC ; -- GRANT ALL PRIVILEGES TO ALL USERS GRANT UPDATE ( COLUMN1 ) ON TABLE_NAME TO USER1 ; -- ALLOW USER1 TO UPDATE ONLY COLUMN1 transactions \u00b6 once a transaction started, it can only be ended by COMMIT, ROLLBACK, program termination, or system crash. problems that can be faced with multiple users updating sam DB (concurrency control): lost update problem: 2 transactions happens at the same time, one is failed and rollback to its previous value overwriting the update of the successful transaction. temporary update problem: 2 transactions started at the same time, one makes a temporary update then fail and roll back, then that temporary update is saved by the 2 nd transaction as its rollback point, if 2 nd transaction fails, it will rollback to that temporary value. incorrect summary problem: a read operation that reads a temporary value set by another transaction, this information and everything generated based on it is wrong. data lock can solve the previous issues. 2 types of data locks: share locks : allows multiple transactions to access te locked data, but NOT modify it . in read operations . exclusive locks : allows one transaction at a time, all other transactions are locked from it. in update operations . deadlock problem \u00b6 happens when you have 2 transactions that updates the same 2 (or more) tables , 1 st transactions might start updating t1 first and locks it. 2 nd transaction might start updating t2 first and locks it. now both tables are locked to both transactions and none of them can continue . when a deadlock is happen, DBMS will arbitrarily kill one of the transactions and roll it back, while the other one will continue. deadlock example https://i.imgur.com/WzN6QIW.png DBMS catalog \u00b6 DBMS catalog are tables created by the DBMS itself, DBMS does not allow modifying theses tables but users can query them. DBMS uses these tables to track tables, columns, views, users, authorization-ids, and privileges. system catalog is not in ANSI/ISO standards, so every commercial vendor implemented it in a different way. in IBM o2 it, there is a db called SYSIBM that holds system catalog. in ORACLE MYSQL this DB is called information_schema . LABEL, COMMENT command \u00b6 label command allows to add comments to tables, views, or columns in a table. comment command allows to add long description to tables and views only. LABEL ON TABLE EXAMS IS 'holds information about all user exams taken' ; LABEL ON COLUMN EXAMS . MARK IS 'the final results of the exam' . COMMENT ON EXAMS ( MARK IS 'the final mark' , DATE IS 'the date the exam taken' ); References \u00b6 [1] Din, A. I. (2006). Structured Query Language (SQL): A practical introduction. NCC Blackwell. chapters 6 & 7. Retrieved from http://www.managedtime.com/freesqlbook.php .","title":"Unit 9: Data integrity, Views"},{"location":"knowledge-base/cs2203-database1/unit9/#unit-9-data-integrity-views","text":"","title":"Unit 9: Data integrity, Views"},{"location":"knowledge-base/cs2203-database1/unit9/#table-of-contents","text":"Unit 9: Data integrity, Views table of contents Data integrity NOT NULL columns data validity table (Entity) integrity referential integrity SQL TRIGGERS VIEWS updating VIEWS database security transactions deadlock problem DBMS catalog LABEL, COMMENT command References","title":"table of contents"},{"location":"knowledge-base/cs2203-database1/unit9/#data-integrity","text":"ensure that all new data are compatible with the existing relationships. divided into 4 categories: NOT NULL columns: some columns must have values. data validity: ensures that the right values are inserted in columns. table integrity referential integrity.","title":"Data integrity"},{"location":"knowledge-base/cs2203-database1/unit9/#not-null-columns","text":"checks apply to all INSERT / UPDATE statements.","title":"NOT NULL columns"},{"location":"knowledge-base/cs2203-database1/unit9/#data-validity","text":"DBMS check that data type matches column type by default. ORACLE has data validity checking built into its data entry form package . it can also be at the application level.","title":"data validity"},{"location":"knowledge-base/cs2203-database1/unit9/#table-entity-integrity","text":"primary key must be unique within a table.","title":"table (Entity) integrity"},{"location":"knowledge-base/cs2203-database1/unit9/#referential-integrity","text":"all child rows must have a parent. the foreign key (child) which references a primary key (parent) from other table, when a changes happens on the parent side, this change should be populated to all its children. example https://i.imgur.com/iBBBxfX.png when defining a foreign key, if you don't define the column then it will be assumed to be the primary key . when defining the foreign key, you can choose what will happen when the parent get changed or deleted: prevent delete operation on the parent: https://i.imgur.com/SDHoCTt.png cascade changes to the children: https://i.imgur.com/jKVLlYt.png set children to NULL: https://i.imgur.com/qIVPWUS.png","title":"referential integrity"},{"location":"knowledge-base/cs2203-database1/unit9/#sql-triggers","text":"trigger : is a set of operations that the DBMS must execute whenever there is a change in the content of a table. CREATE TRIGGER UPDATE_BUDGET ON DEPARTMENTS FOR UPDATE AS UPDATE DEPARTMENTS SET PREVIOUS_BUDGET = BUDGET FROM DEPARTMENTS , INSERTED WHERE DEPARTMENTS . DEPT_NO = INSERTED . DEPT_NO -- CREATES a trigger to save the old budget values into previous_budget column whenever -- departments table gets updated.","title":"SQL TRIGGERS"},{"location":"knowledge-base/cs2203-database1/unit9/#views","text":"the content of views are retrieved from other tables. benefits of views: database security. restrict users access to only parts of a table. data integrity. insert data into a view and validate it before inserting in the actual table . shield form change. when the actual table structure change, the VIEW remains constant. easier querying. save repetitive complex queries into a VIEW, and access it easily. UNION are not allowed in VIEW, since view can only have one query. VERTICAL VIEW : the view has access to few columns of all table rows . HORIZONTAL VIEW : the view have access to all columns of a few table rows. you can always mix horizontal and vertical VIEWS. CREATE VIEW FRESHMEN AS SELECT * FROM STUDENTS WHERE STUDENTS . YEAR = 1 ; -- vertical view CREATE VIEW FRESHMEN AS SELECT NAME , SURNAME FROM STUDENTS ; -- HORIZONTAL VIEW CREATE VIEW FRESHMEN AS SELECT NAME , SURNAME FROM STUDENTS WHERE STUDENTS . YEAR = 1 ; -- MIXED HORIZONTAL, VERTICAL VIEW SELECT * FROM FRESHMEN ; -- QUERYING A VIEW, ``DO QUERY ON A QUERY`` JOIN a VIEW to a base table is completely legal. VIEWS can be used with sub-queries.","title":"VIEWS"},{"location":"knowledge-base/cs2203-database1/unit9/#updating-views","text":"updating data through a view can represent some issues. unlike base tables, VIEWS allow you to use aggregate functions in CREATE VIEW statements, these fields can not be updated later . for a VIEW to be updatable: the VIEW should specify one base table. VIEW should not have aggregate functions on its definition. VIEW should not use GROUP BY, or HAVING. VIEW should not use DISTINCT. VIEW should select only simple columns. scalar expressions or string constants are not allowed. VIEW must include all columns from the base table with NOT NULL modifier. WITH CHECK OPTION is a modifier that can be added to the CREATE VIEW statement, and it will instruct SQL to check INSERT/UPDATE operations on the VIEW against the VIEW predicate. if the values in the INSERT/UPDATE conflict with the VIEW predicate, the operation will be rejected. it is a good practice to use WITH CHECK OPTION modifier in updatable VIEWS . removal of a VIEW will npt affect the base table.","title":"updating VIEWS"},{"location":"knowledge-base/cs2203-database1/unit9/#database-security","text":"ANSI/ISO standards define four privileges: SELECT, INSERT, UPDATE, DELETE. table owner can GRANT/REVOKE access to other users. GRANT statement REVOKE statement you can use VIEWS to limit users access to columns in a table, the user must have SELECT privileges on the base table. GRANT UPDATE allows you to specify individual columns that the granted user is allowed to update. WITH GRANT OPTION modifier can be added to the GRANT statement allowing the grantee to grant privileges to other users REVOKE only allows you to revoke privileges from users that you have granted, and it cascades down the chain to any users they have given privileges GRANT ALL PRIVILEGES ON TABLE_NAME TO PUBLIC ; -- GRANT ALL PRIVILEGES TO ALL USERS GRANT UPDATE ( COLUMN1 ) ON TABLE_NAME TO USER1 ; -- ALLOW USER1 TO UPDATE ONLY COLUMN1","title":"database security"},{"location":"knowledge-base/cs2203-database1/unit9/#transactions","text":"once a transaction started, it can only be ended by COMMIT, ROLLBACK, program termination, or system crash. problems that can be faced with multiple users updating sam DB (concurrency control): lost update problem: 2 transactions happens at the same time, one is failed and rollback to its previous value overwriting the update of the successful transaction. temporary update problem: 2 transactions started at the same time, one makes a temporary update then fail and roll back, then that temporary update is saved by the 2 nd transaction as its rollback point, if 2 nd transaction fails, it will rollback to that temporary value. incorrect summary problem: a read operation that reads a temporary value set by another transaction, this information and everything generated based on it is wrong. data lock can solve the previous issues. 2 types of data locks: share locks : allows multiple transactions to access te locked data, but NOT modify it . in read operations . exclusive locks : allows one transaction at a time, all other transactions are locked from it. in update operations .","title":"transactions"},{"location":"knowledge-base/cs2203-database1/unit9/#deadlock-problem","text":"happens when you have 2 transactions that updates the same 2 (or more) tables , 1 st transactions might start updating t1 first and locks it. 2 nd transaction might start updating t2 first and locks it. now both tables are locked to both transactions and none of them can continue . when a deadlock is happen, DBMS will arbitrarily kill one of the transactions and roll it back, while the other one will continue. deadlock example https://i.imgur.com/WzN6QIW.png","title":"deadlock problem"},{"location":"knowledge-base/cs2203-database1/unit9/#dbms-catalog","text":"DBMS catalog are tables created by the DBMS itself, DBMS does not allow modifying theses tables but users can query them. DBMS uses these tables to track tables, columns, views, users, authorization-ids, and privileges. system catalog is not in ANSI/ISO standards, so every commercial vendor implemented it in a different way. in IBM o2 it, there is a db called SYSIBM that holds system catalog. in ORACLE MYSQL this DB is called information_schema .","title":"DBMS catalog"},{"location":"knowledge-base/cs2203-database1/unit9/#label-comment-command","text":"label command allows to add comments to tables, views, or columns in a table. comment command allows to add long description to tables and views only. LABEL ON TABLE EXAMS IS 'holds information about all user exams taken' ; LABEL ON COLUMN EXAMS . MARK IS 'the final results of the exam' . COMMENT ON EXAMS ( MARK IS 'the final mark' , DATE IS 'the date the exam taken' );","title":"LABEL, COMMENT command"},{"location":"knowledge-base/cs2203-database1/unit9/#references","text":"[1] Din, A. I. (2006). Structured Query Language (SQL): A practical introduction. NCC Blackwell. chapters 6 & 7. Retrieved from http://www.managedtime.com/freesqlbook.php .","title":"References"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/","text":"unit 1 \u00b6 Abbreviations \u00b6 ATM: Asynchronous Transfer Mode. ISP: Internet Service Provider. MAC: Media Access Control. definitions \u00b6 LAN \u00b6 Local Area Network physical, link layer . connect multiple machines within a local are. in charge of actually delivering packets. subdivided to physical (down) , logical(above) . the physical layer deals with the analog, electrical or radio signaling. the logical layer describes all logical (non-analog) operations on the packets. physical layer is the LAN hardware. logical layer is the kernel software interface to the LAN. examples: Ethernet, Token Ring, ATM. IP \u00b6 Internet Protocol Internet network layer . connect multiple LANs. TCP \u00b6 Transport and Connection Protocol. transport layer . actually sending data. OverView of networks \u00b6 1.1 Layers \u00b6 layer is a library or interface to that can work directly (and only) with the layers (interfaces) immediately above and below it. layer can only interact with its direct sibling four-layer model: link: LAN network: IP. transport: TCP. application: your software. so a message travels: application > TCP > IP > LAN. application can talk to TCP only, not IP, not LAN. and so fourth. five-layer model : link: LAN physical. link: LAN logical. network: IP. transport: TCP. application: your software. five layers model: https://www.loom.com/i/402dee371e9344c1921523e46d4c709d 1.2 Data Rate, Throughput and Bandwidth \u00b6 data rate is the number of bytes per second. Throughput is the overall effective transmission putting into account things like transmission overhead,protocol ineffectiveness and competing traffic. throughput used with higher network layers than data rate. bandwidth is a synonym for data rate. goodput: used with TCP, refers to application-layer throughput . the amount of usable data delivered to the receiving application. 1.3 Packets \u00b6 Packets are modest-sized buffers of data, transmitted as a unit through some shared set of links. prefixed with a header containing delivery information. in datagram forewarning: the header contains destination address. headers use virtual-circuit forwarding contain instead of an identifier for the connection . in LAN: packets are buffer and address on top of low-level serial lines. sometimes called frames . in transport layer: packets are called segments . maximum supported packet size by a LAN is an intrinsic attribute of this LAN. Ethernet has a maximum size of 1500 bytes per packet. 14 bytes headers. TCP/IP packets are 512 bytes per packet. TCP header is 20 bytes. IP header is 20 bytes. Token Ring has a maximum size of 4KB per packet. ATM has a maximum size of 48 bytes per packet. issues happen on how to forward large-packet LAN to small-packet LAN. Internal nodes of the network called routers or switches will then try to ensure that the packet is delivered to the requested destination. The concept of packets and packet switching was first introduced by Paul Baran in 1962. packets are buffers built of 8-bit bytes which is universal and understandable by hardwares. The early Internet specifications introduced the term octet (an 8-bit byte) and required that packets be sequences of octets; 1.4 Datagram Forwarding \u00b6 is a model of packed delivery that uses stateless forwarding . packet headers contain destination address, routers and switches are in charge of delivering the packet to its destination. each switch has a forwarding table of pairs. when a packet arrives at the switch, the switch looks up its destination and forward it to the right next switch closer to the destination. and so on. the forward table does not need to match the exact destination, a prefix of the IP address is enough. switch forwarding example: https://www.loom.com/i/469bf66565bd4cc1a178e2b942cae3fe each packet is forwarded in isolation of the other packets , the switches are not aware of the higher-level connections between packets or its endpoints. alternative for datagram forwarding is virtual circuits in which the router maintains a state about each connection passes through it. datagram forwarding might still get some other information beside the destination address, which are quality of service information . switches act on the LAN layer and forward packets based on the LAN address. routers acts on the IP layer and forward packets based on the IP address. 1.5 Topology \u00b6 many LANs (in particular Ethernet) prefer \u201ctree\u201d networks with no redundancy (no loops), while IP has complex protocols in support of redundancy traffic engineering : route selection of a specific route over another. in datagram forwarding the path is only determined by the packet's destination. At the LAN layer, traffic-engineering mechanisms are historically limited. At the IP layer, more strategies are available. 1.6 Routing loops \u00b6 potential drawback in datagram forwarding is the possibility of routing loops : where a packet circulates back endlessly between 2 switches. loops are caused by bad forwarding tables. Ethernet fights loops by: relying on a linear routing loop. use TTl (time-to-live) where a packet is simply discarded if its TTL reaches 0. initial TTL is 64. prevent switches from sending a packet pack to its sender. 1.7 Congestion \u00b6 congestion happens at a switch when: arriving packets to this switch are more than the switch can forward. the previous switch has a higher bandwidth, so they can send packets more than its next switch can handle. multiple switches send packets to the same destination through this same switch. on case of congestion, a queue will be formed at the switch interface, and when the queue is full, all next packets will be dropped . a congestion might be referred to when the queue starts to build (knee, contention) or when the queue is full and the packets starts to get lost (cliff) In the Internet, most packet losses are due to congestion. congestion is simply the network\u2019s feedback that the maximum transmission rate has been reached . 1.8 packets \u00b6 small size packets are useful for: represent the maximum size a sender can send at a time so it allows more senders to send at the same time. large packets size will leave the network unavailable for other senders while this sender is sending his large packet. if the packet is corrupted, the whole large packet needs to be transmitted versus if its on a small size, the process can be interrupted between packets. the switch usually reads the whole packet before reading its headers, if the packet is too large then this process will take longer time before forwarding to the next switch and cause forwarding delay . total packet delay from the sender to the receiver is the sum if the following: bandwidth delay: forwarding from a machine with a higher bandwidth to another with lower bandwidth. propagation delay: the time consumed for the packet travelling through the wires. store-and-forward delay: the delay on each switch * number of switches. queuing delay: waiting in line on a busy routers. 1.9 LANs and Ethernet \u00b6 LAN consists of: physical links that are serial lines. hardware to connect the links to the hosts. protocols that make everything work together. Ethernet is a LAN that described in 1976 and has a bandwidth of 10 Mbps. Ethernet started as unswitched , every host has a long wire that connects all of its stations (clients) and when a station (client) sends a message it will delivered to all other stations and its up to the receiver host to detect if this is addressed to him or not. and collisions could happen when 2 stations send at the same time. Ethernet today are all full switched . switched network advantages over unswitched network: every packet is only delivered to the host in which it is addressed to. collisions are uncommon is switched networks. queuing issue arises. prevents host-based eavesdropping although encryption is a better solution. Ethernet is addresses are 6 bytes long . each Ethernet card (network interface) gets assigned a unique address at the time of manufacturing . that is been saved to its ROM. and called physical address, hardware address, or MAC the first 3 bytes refers to the manufacturers, and the latest 3 bytes are serial number for the network interface. IP addresses get assigned administratively by the local site . The network interface continually monitors all arriving packets; if it sees any packet containing a destination address that matches its own physical address, it grabs the packet and forwards it to the attached CPU (via a CPU interrupt). broadcast address allows one host to send a message to every other host that has the same broadcast address. traffic to a single host called unicast . Ethernet does not scale well to large network sizes . (up to 100k devices is fine). Ethernet switches use passive learning algorithm to build its forwarding tables. (IP routers use active protocols ). Ethernet switches has flooding mechanism as backup in case of emergency or if the switch does not have information about the destination in its forward table. the flooding includes sending the packet to every other switch that this switch connects to (like broadcasting message). and then it can be sorted from there. 1.10 IP Protocol \u00b6 IP was developed to solve the scaling issue of Ethernet by allowing LANs to connect to each other in point-to-point links. IP scales fine to up to 10 10 connected devices. IP addresses are 4 bytes (32 bits) and are part of the IP header which follows the Ethernet headers. Ethernet headers lives for one switch and changes on every switch containing the address of the next switch . while the IP address header stay with the packet for its whole journey . IP addresses can be divided into network part(prefix) and host address the rest of the address. types of IP addresses https://www.loom.com/i/45aea69eb9fd496381344094f45647d4 IP address is identifier and locator while the Ethernet address (MAC) is only identifier . all hosts with the same IP network address and must be located together on the same LAN. IP must also support fragmentation , divide large packets to smaller size packets that can be supported by any switches with low bandwidth. IP protocol: we ship the packet off, and hope it gets there. most of the times it does . IP is connectionless connectionless (IP) vs connection oriented protocols(TCP): connectionless does not hold information about the connection state. connectionless are more reliable , since they don't have connection state, they cannot lose it. in connectionless networks, the packet routes might dynamically change according ot the network status. connectionless makes it harder for providers to bill by connection. connection-oriented protocols have better quality of service. The most common form of IP packet loss is router queue overflows, 1.10.1 IP forwarding \u00b6 IP routers use datagram forwarding , and the destination values are network prefixed that represent an entire LAB instead of individual host. so the IP mission is to deliver to the right LAN , then another process will deliver to the exact host inside the LAN. network prefixing minimized the length of forward tables of thr routers, which make everything faster, saves bandwidth, minimizing overhead. Internet backbone are IP routers that specialize in large-scale routing on the commercial Internet, and which generally have forwarding-table entries covering all public IP addresses.","title":"unit 1"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#unit-1","text":"","title":"unit 1"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#abbreviations","text":"ATM: Asynchronous Transfer Mode. ISP: Internet Service Provider. MAC: Media Access Control.","title":"Abbreviations"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#definitions","text":"","title":"definitions"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#lan","text":"Local Area Network physical, link layer . connect multiple machines within a local are. in charge of actually delivering packets. subdivided to physical (down) , logical(above) . the physical layer deals with the analog, electrical or radio signaling. the logical layer describes all logical (non-analog) operations on the packets. physical layer is the LAN hardware. logical layer is the kernel software interface to the LAN. examples: Ethernet, Token Ring, ATM.","title":"LAN"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#ip","text":"Internet Protocol Internet network layer . connect multiple LANs.","title":"IP"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#tcp","text":"Transport and Connection Protocol. transport layer . actually sending data.","title":"TCP"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#overview-of-networks","text":"","title":"OverView of networks"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#11-layers","text":"layer is a library or interface to that can work directly (and only) with the layers (interfaces) immediately above and below it. layer can only interact with its direct sibling four-layer model: link: LAN network: IP. transport: TCP. application: your software. so a message travels: application > TCP > IP > LAN. application can talk to TCP only, not IP, not LAN. and so fourth. five-layer model : link: LAN physical. link: LAN logical. network: IP. transport: TCP. application: your software. five layers model: https://www.loom.com/i/402dee371e9344c1921523e46d4c709d","title":"1.1 Layers"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#12-data-rate-throughput-and-bandwidth","text":"data rate is the number of bytes per second. Throughput is the overall effective transmission putting into account things like transmission overhead,protocol ineffectiveness and competing traffic. throughput used with higher network layers than data rate. bandwidth is a synonym for data rate. goodput: used with TCP, refers to application-layer throughput . the amount of usable data delivered to the receiving application.","title":"1.2 Data Rate, Throughput and Bandwidth"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#13-packets","text":"Packets are modest-sized buffers of data, transmitted as a unit through some shared set of links. prefixed with a header containing delivery information. in datagram forewarning: the header contains destination address. headers use virtual-circuit forwarding contain instead of an identifier for the connection . in LAN: packets are buffer and address on top of low-level serial lines. sometimes called frames . in transport layer: packets are called segments . maximum supported packet size by a LAN is an intrinsic attribute of this LAN. Ethernet has a maximum size of 1500 bytes per packet. 14 bytes headers. TCP/IP packets are 512 bytes per packet. TCP header is 20 bytes. IP header is 20 bytes. Token Ring has a maximum size of 4KB per packet. ATM has a maximum size of 48 bytes per packet. issues happen on how to forward large-packet LAN to small-packet LAN. Internal nodes of the network called routers or switches will then try to ensure that the packet is delivered to the requested destination. The concept of packets and packet switching was first introduced by Paul Baran in 1962. packets are buffers built of 8-bit bytes which is universal and understandable by hardwares. The early Internet specifications introduced the term octet (an 8-bit byte) and required that packets be sequences of octets;","title":"1.3 Packets"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#14-datagram-forwarding","text":"is a model of packed delivery that uses stateless forwarding . packet headers contain destination address, routers and switches are in charge of delivering the packet to its destination. each switch has a forwarding table of pairs. when a packet arrives at the switch, the switch looks up its destination and forward it to the right next switch closer to the destination. and so on. the forward table does not need to match the exact destination, a prefix of the IP address is enough. switch forwarding example: https://www.loom.com/i/469bf66565bd4cc1a178e2b942cae3fe each packet is forwarded in isolation of the other packets , the switches are not aware of the higher-level connections between packets or its endpoints. alternative for datagram forwarding is virtual circuits in which the router maintains a state about each connection passes through it. datagram forwarding might still get some other information beside the destination address, which are quality of service information . switches act on the LAN layer and forward packets based on the LAN address. routers acts on the IP layer and forward packets based on the IP address.","title":"1.4 Datagram Forwarding"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#15-topology","text":"many LANs (in particular Ethernet) prefer \u201ctree\u201d networks with no redundancy (no loops), while IP has complex protocols in support of redundancy traffic engineering : route selection of a specific route over another. in datagram forwarding the path is only determined by the packet's destination. At the LAN layer, traffic-engineering mechanisms are historically limited. At the IP layer, more strategies are available.","title":"1.5 Topology"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#16-routing-loops","text":"potential drawback in datagram forwarding is the possibility of routing loops : where a packet circulates back endlessly between 2 switches. loops are caused by bad forwarding tables. Ethernet fights loops by: relying on a linear routing loop. use TTl (time-to-live) where a packet is simply discarded if its TTL reaches 0. initial TTL is 64. prevent switches from sending a packet pack to its sender.","title":"1.6 Routing loops"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#17-congestion","text":"congestion happens at a switch when: arriving packets to this switch are more than the switch can forward. the previous switch has a higher bandwidth, so they can send packets more than its next switch can handle. multiple switches send packets to the same destination through this same switch. on case of congestion, a queue will be formed at the switch interface, and when the queue is full, all next packets will be dropped . a congestion might be referred to when the queue starts to build (knee, contention) or when the queue is full and the packets starts to get lost (cliff) In the Internet, most packet losses are due to congestion. congestion is simply the network\u2019s feedback that the maximum transmission rate has been reached .","title":"1.7 Congestion"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#18-packets","text":"small size packets are useful for: represent the maximum size a sender can send at a time so it allows more senders to send at the same time. large packets size will leave the network unavailable for other senders while this sender is sending his large packet. if the packet is corrupted, the whole large packet needs to be transmitted versus if its on a small size, the process can be interrupted between packets. the switch usually reads the whole packet before reading its headers, if the packet is too large then this process will take longer time before forwarding to the next switch and cause forwarding delay . total packet delay from the sender to the receiver is the sum if the following: bandwidth delay: forwarding from a machine with a higher bandwidth to another with lower bandwidth. propagation delay: the time consumed for the packet travelling through the wires. store-and-forward delay: the delay on each switch * number of switches. queuing delay: waiting in line on a busy routers.","title":"1.8 packets"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#19-lans-and-ethernet","text":"LAN consists of: physical links that are serial lines. hardware to connect the links to the hosts. protocols that make everything work together. Ethernet is a LAN that described in 1976 and has a bandwidth of 10 Mbps. Ethernet started as unswitched , every host has a long wire that connects all of its stations (clients) and when a station (client) sends a message it will delivered to all other stations and its up to the receiver host to detect if this is addressed to him or not. and collisions could happen when 2 stations send at the same time. Ethernet today are all full switched . switched network advantages over unswitched network: every packet is only delivered to the host in which it is addressed to. collisions are uncommon is switched networks. queuing issue arises. prevents host-based eavesdropping although encryption is a better solution. Ethernet is addresses are 6 bytes long . each Ethernet card (network interface) gets assigned a unique address at the time of manufacturing . that is been saved to its ROM. and called physical address, hardware address, or MAC the first 3 bytes refers to the manufacturers, and the latest 3 bytes are serial number for the network interface. IP addresses get assigned administratively by the local site . The network interface continually monitors all arriving packets; if it sees any packet containing a destination address that matches its own physical address, it grabs the packet and forwards it to the attached CPU (via a CPU interrupt). broadcast address allows one host to send a message to every other host that has the same broadcast address. traffic to a single host called unicast . Ethernet does not scale well to large network sizes . (up to 100k devices is fine). Ethernet switches use passive learning algorithm to build its forwarding tables. (IP routers use active protocols ). Ethernet switches has flooding mechanism as backup in case of emergency or if the switch does not have information about the destination in its forward table. the flooding includes sending the packet to every other switch that this switch connects to (like broadcasting message). and then it can be sorted from there.","title":"1.9 LANs and Ethernet"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#110-ip-protocol","text":"IP was developed to solve the scaling issue of Ethernet by allowing LANs to connect to each other in point-to-point links. IP scales fine to up to 10 10 connected devices. IP addresses are 4 bytes (32 bits) and are part of the IP header which follows the Ethernet headers. Ethernet headers lives for one switch and changes on every switch containing the address of the next switch . while the IP address header stay with the packet for its whole journey . IP addresses can be divided into network part(prefix) and host address the rest of the address. types of IP addresses https://www.loom.com/i/45aea69eb9fd496381344094f45647d4 IP address is identifier and locator while the Ethernet address (MAC) is only identifier . all hosts with the same IP network address and must be located together on the same LAN. IP must also support fragmentation , divide large packets to smaller size packets that can be supported by any switches with low bandwidth. IP protocol: we ship the packet off, and hope it gets there. most of the times it does . IP is connectionless connectionless (IP) vs connection oriented protocols(TCP): connectionless does not hold information about the connection state. connectionless are more reliable , since they don't have connection state, they cannot lose it. in connectionless networks, the packet routes might dynamically change according ot the network status. connectionless makes it harder for providers to bill by connection. connection-oriented protocols have better quality of service. The most common form of IP packet loss is router queue overflows,","title":"1.10 IP Protocol"},{"location":"knowledge-base/cs2204-netwroks1/unit1/unit1/#1101-ip-forwarding","text":"IP routers use datagram forwarding , and the destination values are network prefixed that represent an entire LAB instead of individual host. so the IP mission is to deliver to the right LAN , then another process will deliver to the exact host inside the LAN. network prefixing minimized the length of forward tables of thr routers, which make everything faster, saves bandwidth, minimizing overhead. Internet backbone are IP routers that specialize in large-scale routing on the commercial Internet, and which generally have forwarding-table entries covering all public IP addresses.","title":"1.10.1 IP forwarding"},{"location":"knowledge-base/cs2204-netwroks1/unit6/da6/","text":"Unit 6: Discussion assignment \u00b6 question \u00b6 Consider a simple application-level protocol built on top of UDP that allows a client to retrieve a file from a remote server residing at a well-known address. The client first sends a request with a file name, and the server responds with a sequence of data packets containing different parts of the requested file. To ensure reliability and sequenced delivery, client and server use a stop-and-wait protocol. Ignoring the obvious performance issue, do you see a problem with this protocol? Think carefully about the possibility of processes crashing. answer \u00b6 The system mentioned above is a simple TFTP system. stop-and-wait protocol requires the receiver (client) to send acknowledgements signals, after every successful receiving of a packet, failing to send the ACK, will stop the server from sending subsequent packets, and causes the lost packet to be retransmitted. There are multiple things that can go wrong in this implementation. some issues are generic to this type of implementations, and some are very specific to certain cases. as we mentioned earlier, packet loss is a very generic problem, the mentioned system uses stop-and-wait, so if a data packet[N] get lost, the receiver(client) won't be able to send ACK[N], and the server needs to retransmit packet[N] again. if packet[N] arrived to the client, but the client's ACK[N] get lost, the sender will retransmit the packet[N] again, then a duplicate packet[N] will arrive to the client, if the packets are not numbered properly, the received file will be corrupted. if -for any reason- delivering packet[N] keeps failing, after the timeout period, the client will retransmit the ACK[N-1] again, which may cause all the subsequent packets to be duplicated, this is known as Sorcerer\u2019s Apprentice Bug, and the solution is to stop retransmit-on-timeout on the client side. if -for any reason- the client aborts the current transfer between a packet delivery, and started a new one; if the new connection happens to use the same port -on client and server side- the packet in-delivery from the aborted connection may arrive as a legitimate packet of the current connection, then the actual packet of the current connection will be discarded, the transfer will continue as normal. but the delivered file will be corrupted; this can be solved by choosing the port number of the new connection randomly and involve the OS in choosing the new port to prevent reissuing the same ports for the successive connections. if the bits used in the numbering block are not enough, that causes the number of packets to overflow and overlaps with a previous packet number; this can be solved by choosing enough bits to build the block number eg.16 bits which limits the file size to 32 MB. if the client's final ACK get lost, the server will keep retransmitting the final packet; the solution is for both the client and server to enter DALLY state. if the initial request from the client get retransmitted, 2 processes will start and 2 transfers will be established. references \u00b6 Dordal, P. (2019). An introduction to computer networks. https://eng.libretexts.org/Bookshelves/Computer_Science/Networks/Book%3A_An_Introduction_to_Computer_Networks_(Dordal)/07%3A_IP_version_4","title":"Unit 6: Discussion assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit6/da6/#unit-6-discussion-assignment","text":"","title":"Unit 6: Discussion assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit6/da6/#question","text":"Consider a simple application-level protocol built on top of UDP that allows a client to retrieve a file from a remote server residing at a well-known address. The client first sends a request with a file name, and the server responds with a sequence of data packets containing different parts of the requested file. To ensure reliability and sequenced delivery, client and server use a stop-and-wait protocol. Ignoring the obvious performance issue, do you see a problem with this protocol? Think carefully about the possibility of processes crashing.","title":"question"},{"location":"knowledge-base/cs2204-netwroks1/unit6/da6/#answer","text":"The system mentioned above is a simple TFTP system. stop-and-wait protocol requires the receiver (client) to send acknowledgements signals, after every successful receiving of a packet, failing to send the ACK, will stop the server from sending subsequent packets, and causes the lost packet to be retransmitted. There are multiple things that can go wrong in this implementation. some issues are generic to this type of implementations, and some are very specific to certain cases. as we mentioned earlier, packet loss is a very generic problem, the mentioned system uses stop-and-wait, so if a data packet[N] get lost, the receiver(client) won't be able to send ACK[N], and the server needs to retransmit packet[N] again. if packet[N] arrived to the client, but the client's ACK[N] get lost, the sender will retransmit the packet[N] again, then a duplicate packet[N] will arrive to the client, if the packets are not numbered properly, the received file will be corrupted. if -for any reason- delivering packet[N] keeps failing, after the timeout period, the client will retransmit the ACK[N-1] again, which may cause all the subsequent packets to be duplicated, this is known as Sorcerer\u2019s Apprentice Bug, and the solution is to stop retransmit-on-timeout on the client side. if -for any reason- the client aborts the current transfer between a packet delivery, and started a new one; if the new connection happens to use the same port -on client and server side- the packet in-delivery from the aborted connection may arrive as a legitimate packet of the current connection, then the actual packet of the current connection will be discarded, the transfer will continue as normal. but the delivered file will be corrupted; this can be solved by choosing the port number of the new connection randomly and involve the OS in choosing the new port to prevent reissuing the same ports for the successive connections. if the bits used in the numbering block are not enough, that causes the number of packets to overflow and overlaps with a previous packet number; this can be solved by choosing enough bits to build the block number eg.16 bits which limits the file size to 32 MB. if the client's final ACK get lost, the server will keep retransmitting the final packet; the solution is for both the client and server to enter DALLY state. if the initial request from the client get retransmitted, 2 processes will start and 2 transfers will be established.","title":"answer"},{"location":"knowledge-base/cs2204-netwroks1/unit6/da6/#references","text":"Dordal, P. (2019). An introduction to computer networks. https://eng.libretexts.org/Bookshelves/Computer_Science/Networks/Book%3A_An_Introduction_to_Computer_Networks_(Dordal)/07%3A_IP_version_4","title":"references"},{"location":"knowledge-base/cs2204-netwroks1/unit6/journal/","text":"unit 6: Journal \u00b6 Q1 \u00b6 In the TFTP protocol: Q1.1 \u00b6 If the client changes its port number on a subsequent connection, but the server does not, what prevents an old-duplicate data packet sent by the server from being accepted by the new client? the old duplicate packets will still have the old port number; assuming that the port numbers are being issued correctly, then the new connection will -almost- never uses the same old port number; the new connection will use different port number, that the old packet does not know about. Q1.2 \u00b6 If the server changes its port number on a subsequent connection, but the client does not, what prevents an old-duplicate data packet sent by the server from being accepted by the new client? once the new connection has been established on the new port number, the client will only accept packets sent from the server through that same new port number. the packet with the old port number, will reach the client, but will be discarded. Q2 \u00b6 In an RPC-like protocol in which multiple requests can be outstanding, and replies can be sent in any order: Assume that requests are numbered and that ACK[N] acknowledges reply[N]. Should ACKs be cumulative?If not,what should happen if an ACK is lost? since the replies can be sent in any order, then the acknowledges can not be cumulative. the replies will arrive at any order, and ACKs will arrive at any order. the server needs to cache all replies that have been sent. once the transfer has been completed, the server should enter DALLY state, waiting for any late ACKs; then if there are missing ACKs, the server should consult its cache and retransmit requests with the missing ACKs, then re-enter DALLY state till all replies have been acknowledged.","title":"unit 6: Journal"},{"location":"knowledge-base/cs2204-netwroks1/unit6/journal/#unit-6-journal","text":"","title":"unit 6: Journal"},{"location":"knowledge-base/cs2204-netwroks1/unit6/journal/#q1","text":"In the TFTP protocol:","title":"Q1"},{"location":"knowledge-base/cs2204-netwroks1/unit6/journal/#q11","text":"If the client changes its port number on a subsequent connection, but the server does not, what prevents an old-duplicate data packet sent by the server from being accepted by the new client? the old duplicate packets will still have the old port number; assuming that the port numbers are being issued correctly, then the new connection will -almost- never uses the same old port number; the new connection will use different port number, that the old packet does not know about.","title":"Q1.1"},{"location":"knowledge-base/cs2204-netwroks1/unit6/journal/#q12","text":"If the server changes its port number on a subsequent connection, but the client does not, what prevents an old-duplicate data packet sent by the server from being accepted by the new client? once the new connection has been established on the new port number, the client will only accept packets sent from the server through that same new port number. the packet with the old port number, will reach the client, but will be discarded.","title":"Q1.2"},{"location":"knowledge-base/cs2204-netwroks1/unit6/journal/#q2","text":"In an RPC-like protocol in which multiple requests can be outstanding, and replies can be sent in any order: Assume that requests are numbered and that ACK[N] acknowledges reply[N]. Should ACKs be cumulative?If not,what should happen if an ACK is lost? since the replies can be sent in any order, then the acknowledges can not be cumulative. the replies will arrive at any order, and ACKs will arrive at any order. the server needs to cache all replies that have been sent. once the transfer has been completed, the server should enter DALLY state, waiting for any late ACKs; then if there are missing ACKs, the server should consult its cache and retransmit requests with the missing ACKs, then re-enter DALLY state till all replies have been acknowledged.","title":"Q2"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/","text":"Unit 6 \u00b6 definition \u00b6 bottleneck : the slowest link in delivering packets. RTT : round-trip time, the time that a packet consumed from its sending till its arrival. 6. ABSTRACT SLIDING WINDOWS \u00b6 Build reliable data-transport layers on top of unreliable lower layers. Can be achieved by used retransmit-on-timeout policy. Retransmit-on-timeout (achieve reliability ) If packet is transmitted and no acknowledgment received during the timeout period, the packet will be resent. Protocols use this called: ARQ protocols: Automatic Repeat reQuest. Sliding windows algorithm is used to manage congestions and not giving the network more packets than it can handle. End-to-End principle suggest that building reliability on the lower layers is wrong, instead reliability should be built on the endpoints of a connection, as ARQ and retransmit-on-timeout do. 6.1 Building Reliable transport: Stop-and-Wait \u00b6 Retransmit-on-timeout requires sequence numbering for the packets . if the network guarantees that the packets won't be reordered we can omit the sequence numbers. but no-reordering is not guaranteed on the Internet so we have to use sequence numbers. Data[N] wil be the packet number N, and ACK[N] will be the acknowledgment number N. stop-and-wait is a version of retransmit-on-timeout where the sender sends only one outstanding packet (Data[N]) at a time and waits for ACK[N] to be received. if ACK[N] is not received, Data[N] will be retransmitted. Data[N+1] will never be retransmitted until ACK[N] is received. 6.1.1 Packet Loss \u00b6 in case of packet loss, the receiver won't receive packet[N] so the sender will retransmit Data[N] again. if the receiver receives a packet and send ACK, but the ACK get lost and never received by the sender, so the sender will transmit Data[N] again and the receiver will receive duplicate copies of Data[N] the receiver usually implements retransmit-on-duplicate policy, so in the case of receiving a duplicate packets it will sent the same ACK[N]. at least one side must implement retransmit-on-timeout policy, either the receiver or the sender; otherwise a lost packet will lead to deadlock as the server and the sender will wait forever. AND the other end must implement either retransmit-on-timeout OR transmit-on-duplicate policy. it is fine for the sender and receiver to implement retransmit-on-timeout policy with different timeout value . 6.1.2 Sorcerer\u2019s Apprentice Bug \u00b6 A strange thing happens if one side implements retransmit-on-timeout but both sides implement retransmit-on-duplicate. if the ACK[N] delayed then all of the later packets will be sent twice . thee transfer will still work normally, but it will take double the bandwidth. Fix: have one side (usually the sender) to implement retransmit-on-timeout only policy as in TCP protocol . 6.1.3 Flow Control \u00b6 stop-and-wait has a flow control to prevent data from arriving at the receiver faster than it can be handled . if the receiver needs less or slightly larger time than RTT (round-trip time) everything will work fine with stop-and-wait. if the receiver needs much more time than RTT (round-trip time) then the packet might be timed out and retransmitted from the sender since the receiver won't send the ACK until packet is completely received and transmitted. one solution is have 2 types of Acknowledgments: ACK Wait [N] which means that data has been received but receiver is not ready yet for Data[N +1] ACK GO [N] which means that data has been received and processed by receiver and it is ready for the next packet. a problem may arise if the ACK GO [N] is lost, then the connection is deadlock since the receiver and the sender keep waiting for each other. 6.2 Sliding Windows \u00b6 stop-and-wait is very reliable but not very efficient . most links with multi-hop stop-and-wait path will remain idle most of the time, which is a thing that we don't want. we can work around by allowing the sender to send few packets without waiting for their acknowledgments, but this bad since the packets may: end up in waiting queues on switches a long the way. packets get lost (discarded if the queues are full). packets get lost (discarded if the receiver does not have enough bandwidth). sliding windows : the sender picks a window size of packets that can be transmitted without waiting for their acknowledgments. acknowledgments are accumulative which means that ACK[N] will never been sent until the receiver has received all acknowledgment ACK[k] for k <= N . the sender keeps tracking of a variable representing the last ACK received. sender will send multiple packets equals to the window size and waits for the ACK of the last packet. with every new ACK received, the sender updates the last ACK received variable, and the window slides by [last ACK + window size] . sliding-windows is self-clocking so that it automatically reduces the sender rate whenever the available fraction of the bottleneck bandwidth is reduced. self-clocking property happens since the sliding will happen according to the pace of the slowest link (bottleneck). example https://vimeo.com/150452468 6.2.1 Bandwidth * Delay \u00b6 BDP (bandwidth delay product) = bandwidth * delay BDP0 ( transit capacity = amount of data can be sent before the first response) = bandwidth * RTT BDP0 is the optimal value for the window size if the chosen windowSize > BDP0 then RTT] will grow due to the queues that will form on the links, so the new RTT[actual] is different from the RTT[no-load] in the beginning. choosing windowSize less than BDP0 means that we are not using all network power. chosen windowSize less than BDP0 means that we are introducing queues somewhere in the network. in practice neither the bandwidth or RTT is constant, so that the windowSize keep changing according to the other traffic in the network (which changes the available bandwidth then BDP). 6.2.2 The receiver side \u00b6 if the transmission happens through a single link, then the packets will never be reordered and windowSize of 1 works perfectly. with the user of switches, no-reordering is not guaranteed, so the receiver has to use the same windowSize as the sender. if the windowSize is [11,12,13,14] and the receiver received 12,13,14 packets but 11 is delayed, then the receiver must buffer [12,13,14] packets waiting for the 11 to arrive. receiver also keeps track of lastSentACK , and at any time the receiver is willing to accept the packet Data[lastSentACK + 1] and when this packet arrived the receiver needs to check its buffers to see if subsequent packets are in the buffer , so the receiver process them and send the the largest accumulative ACK for the latest received packet 6.2.3 Loss Recovery with sliding-windows \u00b6 if windowSize is 4 and packet[5] is lost, then the sender will only receive ACK[4]. sender will only retransmit packet[5] then waits for the next ACK. if the receiver has received packets, 6,7,8 then it will send ACK[8] and the receiver will not retransmit the packets 6,7,8. and window will slide [9-13]. if the packets 6,7,8 have also lost , then the sender will receive ACK[5] and sender will retransmit the packets 6,7,8 and the window will slide [5-9]. the packets 6,7,8 will timeout shortly after packet[5] timeout. in this case, sliding-windows protocol will suppress further retransmission until recovery is complete . pipeline drain happens when the sliding-windows goes into halt after full timeout , after recovery from the full timeout, the sliding-windows needs to startup again . most TCP implementations has fast recovery for early detection of packets loss before the pipeline has fully drained. 6.3 Linear bottlenecks \u00b6 bottlenecks are the slowest links (eg. R2-R3, R3-R4 in the image above), they are most likely where the queues will form. when using slide-windows, the sending rate will adjusted according to the bottleneck rate. 6.3.1 Simple Fixed-window-size analysis \u00b6 we will analyze the windowSize on overall throughput on RTT. in the diagram above, bottleneck is R1-R2. any queues will form at R1. the bottleneck bandwidth is 1 packet/sec so the path bandwidth is 1 packet/sec. there are 4 links, each of them consumes 1 sec. so the RTT = 4 secs . (assuming the ACN path is infinitely fast so that can be omitted). bottleneck is S1-S2. any queues will form at S1. the path bandwidth is 1 packet/sec (equals to the bottleneck bandwidth). RTT = 4 secs. (0 (C-S1, infinitely fast) + 1 sec (S1-S2) + 1 sec (S2-D) + 1 sec (D-S2, for ACK) + 1 sec (S2-S1, for ACK) + 0 sec (S1-C, for ACK), infinitely fast ). NOTE 1 : in both example the propagation delay is assumed to be zero and omitted. NOTE 2 : we assume a single connection is made. BDP = bandwidth * RTT = 4. lets analyze different scenarios with different windowSize: 6.3.1.1 case 1: windowSize = 2 \u00b6 windowSize < BDP. every ACK will take BDP = 4 to arrive. throughput: 2 packets in 4 seconds = 2/4 = 0.5 packet/sec. (50% of the bandwidth is utilized). notice the brief pile-up (queue) on the bottleneck on startup. in the steady state, no queues are formed. notice that during each second: 2 of the 4 links remain idle . RTT for packet[4] is 9 secs. steady state achieved at time 9. 6.3.1.2 case 2: windowSize = 4 \u00b6 windowSize = BDP. every ACK will take BDP = 4 to arrive. throughput: 4 packets in 4 seconds = 4/4 = 1 packet/sec. (100% of the bandwidth is utilized). notice the queue on R1 lasts longer on startup, but after that no queues are formed. RTT fro packet[4] is 7 seconds. steady state achieved at time T = 4. this is the best possible throughput. at every second, all links are busy. this case is the congestion knee . 6.3.1.3 case 3: windowSize = 6 \u00b6 windowSize > BDP. every ACK will take BDP = 4 to arrive. throughput: 4 packets in 4 seconds = 4/4 = 1 packet/sec. (100% of the bandwidth is utilized). notice the heavy queue on R1. 6.3.2 RTT calculations \u00b6 RTT no-load is physical travel time that is subjective to the limitations of delay; any tine excess RTT no-load is spent waiting in queues. so queueTime = RTT actual - RTT no-load . When the bottleneck link is saturated, that is, is always busy, the number of packets actually in transit (not queued) somewhere along the path will always be bandwidth \u02c6 RTTnoLoad. and throughput = windowSize/RTTactual where \u201cthroughput\u201d is the rate at which the connection is sending packets. In the sliding windows steady state, where throughput and RTTactual are reasonably constant, the average number of packets in the queue is just throughput * queue_time (where throughput is measured in packets/sec): so queue_usage = throughput * (RTTactual \u2013 RTTnoLoad) = winSize * (1 \u2013 RTTnoLoad/RTTactual) and RTTactual = winSize/bottleneck_bandwidth queue_usage = winSize \u2013 bandwidth * RTTnoLoad RTTactual/RTTnoLoad = winSize/transit_capacity = (transit_capacity + queue_usage) / transit_capacity 6.3.3 Graphs at the Congestion Knee \u00b6 The critical windowSize value is equal to bandwidth \u02c6 RTTnoLoad; this is known as the congestion knee. For winsize below this, we have: throughput is proportional to winsize delay is constant queue utilization in the steady state is zero connection power is proportional to the windowSize. For winsize larger than the knee, we have throughput is constant (equal to the bottleneck bandwidth) delay increases linearly with winsize queue utilization increases linearly with winsize connection power is proportional to the 1/windowSize. Ideally, winsize will be at the critical knee. However, the exact value varies with time: available bandwidth changes due to the starting and stopping of competing traffic, and RTT changes due to queuing. Standard TCP makes an effort to stay well above the knee much of the time, presumably on the theory that maximizing throughput is more important than minimizing queue use. connection power = throughput / RTT 6.3.4 Simple Packet-Based Sliding-Windows Implementation \u00b6 pseudo code outline of the receiver side of a sliding-windows implementation, ignoring lost packets and timeouts. Receiver side: W: winsize LA: last_ACKed = 0 The next packet expected is LA+1 Window is [LA+1, . . . , LA+W] use EarlyArrivals data structure: EarlyArrivals is an array of objects of size W. we always out Data[M] at the index of of M % W Upon arrival of Data[M]: if M <= LA or M > LA+W, ignore the packet if M > LA+1, put the packet into EarlyArrivals. if M == LA+1: deliver the packet (that is, Data[LA+1]) to the application LA = LA+1 (slide window forward by 1) while (Data[LA+1] is in EarlyArrivals) { output Data[LA+1] LA = LA+1 } send ACK[LA] pseudo code outline of the sender side of a sliding-windows implementation, ignoring lost packets and timeouts. Sender side: W: winsize LA: last_ACKed = 0 start by sending a full windowFull of packets. Data[1, ..., W] Upon arrival of ACK[M]: if M <= LA or M > LA+W, ignore the packet otherwise: set K = LA + W + 1, the first packet just above the old window set LA = M, just below the bottom of the new window for (i=K; i <= LA+W; i++) send Data[i] 11. UDP transport \u00b6 TCP and UDP works above IP layer. UDP provides simple datagram delivery to remote sockets \\<host, port>. TCP provides much more richer functionality for sending data, but requires the remote socket first to be connected. in this chapter: introduce UDP. introduce UDP-based Trivial File Transfer Protocol. fundamental issues any transport protocol must address: lost final packets late arriving packets. 11.1 User DataGram Protocol UDP \u00b6 it is in RFC 1122 . UDP is almost a null protocol , since it is very basic. UDP adds 2 functionalities above IP layer: port numbers. checksum. UDP header: using port number, an application can connect to a an individual server process that owns this port number instead of connecting to the host as whole. UDP is unreliable : no reattempt-at-timeout, acknowledgments, or retransmission. socket = UDP UDP is unconnected (stateless) : if an application opens a port on a host; any other host on the internet can deliver packets to the socket with no questions asked. UDP packets uses 16-bit Internet checksum : checksum can be disabled by setting all-16-bits to 0. UDP checksum contains: UDP header. UDP data. pseudo-IP header: source and destination IP addresses. UDP packets can be can be dropped due to: queue overflow** on an intermediate router. queue overflow on the host: data is arriving faster than the receiver Higher level protocol use acknowledgment to form flow control that prevents packets from being dropped. UDP is popular for local transport into the same LAN. UDP is the base transport basis for Remote Procedure Call, RCP : a host invokes a procedure in another host, where the parameters and the return value of the procedure are transported back and forth through UDP. UDP is good for request-reply connections : TCP can be used, but it requires additional overhead of creating and destroying the connection. DNS uses UDP over TCP for the above reason, but if we have a sequence of request-reply operations, then TCP is worth the overhead . UDP is popular for real-time transport : with TCP if a packet is lost, the receiving host has no other option but to queue the subsequent packets till the lost packet arrives, which may tack several RTTs. UDP is better for real-time transport, since it give the receiving host the option to just ignore the lost packets, so that UDP is good for voice and video transport which are loss-tolerant but delay-intolerant . Real-time Transfer Protocol, RTP is built on top of UDP because of its loss-tolerance. VoIP uses UDP over TCP . UDP is used in flooding attacks : since it is easy to send UDP packets with spoofed IP addresses; with TCP is not hard to send TCP connection-request (SYN) packets with spoofed IP addresses, but the connection won't last till the malicious packets have been delivered to the application process. UDP enables traffic amplification attacks : the attacker sends a small message to a server, with spoofed source address, and the server then responds to the spoofed address with a much larger response message. One approach is for the server to limit the size of its response \u2013 ideally to the size of the client\u2019s request until it has been able to verify that the client actually receives packets sent to its claimed IP address. 11.1.1 QUIC \u00b6 UDP allows new protocols to run as user-space applications, no kernel updates are required. QUIC Quick UDP Internet Connection is a protocol created by google to support HTTPS (HTTP + TLS). QUIC allows supporting multiplexed streams in a single connection : a lost packet will block its stream until retransmits, while other streams continue without waiting. QUIC supports error-correcting codes . QUIC eliminates the initial RTT needed to create the TCP connection. QUIC provides support for advanced congestion control (on the application layer, new versions do this entirely on the server end). One downside of QUIC is its nonstandard programming interface. 11.1.2 DCCP \u00b6 DCCP Datagram Congestion Control Protocol outlined in RCF 4340. build on top of UDP. adds number of TCP-like features to UDP. DCCP packets are delivered to the application in *the order of arrival rather than the order of sequence numbers. DCCP uses Acknowledgments, but for congestion control . DCCP does support reliable delivery of control packets, used for connection setup and teardown, and option negotiation. DCCP packets includes: application-specific UDP prot number 32-bit service code : allows finer-grained packet handling which identifies the incoming packets and prevent conflicts. DCCP run on the operating system kernel (opposite to QUIC which runs on the user-space): because the ECN congestion-feedback mechanism requires setting flag bits in the IP header, and most kernels do not allow user-space applications to do this. 11.1.3 UDP Simplex-Talk \u00b6 One of the early standard examples for socket programming. the client side reads text from the user's terminal and sends them through the network to the server, which put them in the server's terminal. the server don't send any response or ACKs , so it is one way flow . simplex refers to the one-way flow (opposite to duplex-talk used in Instant Messaging IM ). the server side must select a port number, which with the server\u2019s IP address will form the socket address to which clients connect. On the server side, simplex-talk must do the following: ask for a designated port number create a socket, the sending/receiving endpoint bind the socket to the socket address, if this is not done at the point of socket creation receive packets sent to the socket for each packet received, print its sender and its content The client side has a similar list: look up the server\u2019s IP address, using DNS create an \u201canonymous\u201d socket; we don\u2019t care what the client\u2019s port number is read a line from the terminal, and send it to the socket address 11.1.4 netcat \u00b6 netcat or nc is a utility that enables sending and receiving UDP and TCP packets. 11.1.5 binary data \u00b6 when sending a binary data packets through UDP, the client and server must agree on encoding . big-endian encoding is used by IBM. little-endian encoding is used by Intel. big-endian encoding is the most used, and known as network byte order . converting from host byte order to network byte order is language-dependent. but should always assume big-endian encoding In java, the client needs to convert int[] to byte[] , then the receiving server needs to convert the packet back from byte[] to int[] . the DataOutputStream and DataInputStream help in the converting operations. In the C language, we can simply allocate a char[] of the appropriate size and write the network-byte order values directly into it. 11.2 Trivial File Transport Protocol TFTP \u00b6 TFTP supports file transfer in both directions . TFTP does not support authentication : all files are available to everyone. TFTP is UDP-based, well-suited for downloading startup files TFTP uses stop-and-wait : with fixed timeout interval. TFTP used in internal LANs because its limited security . TFTP documented in RFC 783, and in RFC 1350. TFTP has 5 packet types: Read ReQuest, RRQ: contains filename and text/binary indication. Write ReQuest, WRQ. Data: 16-bit block number then up to 512 bytes of data. ACK, 16-bit block number. Error, All errors other than \u201cUnknown Transfer ID\u201d are cause for sender termination. All blocks of data contain 512 bytes except the final block, which is identified as the final block ; if the data divisible by 512, the final block will be sent containing 0 bytes of data . TFTP must take care of packetization (UDP, opposed to TCP which takes care of packetization), TFTP must use small block size to avoid fragmentation . The TFTP server listens on UDP port 69 for arriving RRQ packets, the server will create separate process or thread for every requested file as child process ; the new child process will have entirely new UDP port , and it will be used for further communication with this client. the new port will: prevent old-duplicate packets. child process will be responsible for handling one client only, all packets are now arriving to the port of the child process. downside: f preventing the use of TFTP through NAT firewalls without port change, handling multiple clients will be very complicated, as the server would have to sort out, for each arriving packet, which transfer it belonged to. Each transfer would have its own state information including block number, open file, and the time of the last successful packet. TFTP file requests work as follows: The client sends a RRQ to server port 69. The server creates a child process, which obtains a new port, s_port, from the operating system. The server child process sends Data[1] from s_port. The client receives Data[1], and thus learns the value of s_port. The client will verify that each future Data[N] arrives from this same port. The client sends ACK[1] (and all future ACKs) to the server\u2019s s_port. The server child process sends Data[2], etc, each time waiting for the client ACK[N] before sending Data[N+1]. The transfer process stops when the server sends its final block, of size less than 512 bytes, and the client sends the corresponding ACK. 11.3 Fundamental Transfer issues \u00b6 These issues includes: old duplicate packets lost final ACK duplicated connection request reboots 11.3.1 old duplicate packets \u00b6 packets from the past arriving very late, but being accepted as current . external old duplicate : the previous connection has been closed, then a new connection has been opened (same socket and ports), then a delayed packet of the previous connection appears in the right order of the new connection, so it is being accepted as a legitimate packet of the current connection => file transfer will be corrupted. internal old duplicate : happens in the same connection instance, . For example, if TFTP allowed its 16-bit block numbers, then a very old Data[3] might be accepted in lieu of Data[3+(2^16)]. these are usually prevented by numbering the data,and use sufficiently enough bits to prevent number overlapping. to prevent internal duplicates due to number overlapping , and since it uses 16-bit numbering, the maximum file size (51B * max number of packets numbering) = 512B = ((2^16) - 1) = 32 MB . port numbers should be chosen randomly, so that the probability that the same number is chosen twice in immediate succession is very low. to prevent external duplicates , TFTP requires a new port number for each new connection (separate transfer): if ports chosen randomly, there are 1/(2^32) that the same port number will used twice. if ports were chosen by OS, we assume that OS won't reissue the same port twice in rapid succession . if one side (client or server) choses the same port, the probability the same 2 ports will be chosen is 1/(2^16). reasons why the packet from the old connection arriving late: A first copy of the old duplicate was sent A routing error occurs; the packet is stuck in a routing loop An alternative path between the original hosts is restored, and the packet is retransmitted successfully Some time later, the packet stuck in the routing loop is released, and reaches its final destination TCP officially once has a limit of 60 seconds, (now is 30 seconds) to assume that all old packets are now discarded and it is safe to reopen the connection with the same ports again. IP considers 255 seconds as the safe time. It is also possible to prevent external old duplicates by including a connection count parameter in the transport or application header. For each consecutive connection, the connection count is incremented by (at least) 1. A separate connection-count value must be maintained by each side; if a connection-count value is ever lost, a suitable backup mechanism based on delay might be used. 11.3.2 lost final ACK \u00b6 The final packet is alway an ACK, which can not be Acknowledged . TFTP recommend the sender to enter the DALLY state after sending the final ACK. in the DALLY state, the receiver responds to duplicated final ACK , by retransmitting the final ACK back to the sender . receiver does not respond to the single final ACK, and the sender will exit the DALLY state shortly. The time of the DALLY state should be twice the timeout of the receiver at least . so the sender can retry sending the final ACK 3 times . Note also that dallying only provides increased assurance, not certainty: it is possible that all final ACKs were lost. The TCP analogue of dallying is the TIMEWAIT state, which also prevents old duplicates. 11.3.3 Duplicated connection requests \u00b6 we need to distinguish between duplicated connection requests (same connection) and requests to open a new connection. scenario: client send RRQ('foo') > client aborts connection foo > client sends RRQ('bar') > server responds of Data[1] of foo connection. if the client starts the new connection from a new port, it is fine, the data of the previous connection will be sent to the old port . TFTP does not have cancellation message . if -for unexpected reason- the client sends the new connection from the same port, then it is a problem. in case of duplicate connection requests, the server will start 2 different processes , then the receiver should accept one successfully, and responds with ERROR of Unknown Transfer ID to the second process , transfer ID refers to the port number. the ERROR state from the receiver causes the associated process to shutdown , and the other process to continue normally. It is theoretically possible for a malicious actor on the LAN to take advantage of this TFTP \u201clatching on\u201d behavior to hijack anticipated RRQs, and sends malicious data to all ports of the victim (the client). 11.3.4 reboots \u00b6 one side might reboot between receiving the messages form the other side, the other side must detect the reboot and close the old connection. if the receiver (client) reboots , the sender will keep sending packets but no further ACK will be received. rebooted side will lose all its memory, which may lead to the possibility of reusing the same port in the post-reboot connection. In practical terms, this scenario seems to be of limited importance, though \u201cdiskless\u201d devices often do use TFTP to request their boot image file when restarting, and so might be potential candidates. 11.4 other TFTP notes \u00b6 11.4.1 TFTP and the sorcerer \u00b6 TFTP uses stop-and-wait, its ACK includes the block number of the packet being acknowledged. TFTP was vulnerable to the Sorcerer\u2019s Apprentice bug. fix: the sender (ie, the side originating the DATA packets) must never resend the current DATA packet on receipt of a duplicate ACK. 11.4.2 TFTP states \u00b6 DALLY state. UNLATCHED: when the client-receiver sends RRQ, it does not know the port number to latch on yet . ESTABLISHED: when the client-receiver receives DATA[1], the port number is now known, and the connection is ESTABLISHED. ERROR. 11.4.3 TFTP throughput \u00b6 on a single ETHERNET , sender and receiver will alternate using the same channel, so the throughput is optimal . As soon as the store-and-forward delays of switches and routers are introduced ( multiple ETHERNETs ), though, stop-and-wait becomes a performance bottleneck. 11.5 Remote Procedure Call RPC \u00b6 usually implemented on to of UDP. example is DNS : a host sends a DNS look up request to the DNS server and receives a reply. RPC is also quite successful as the mechanism for interprocess communication within CPU clusters, perhaps its most time-sensitive application. requests and replies must be numbered , so the client knows which reply is to which request, and the reply can serve as acknowledgment to the request. Request[N] timeout When the server creates reply[N] and sends it to the client, it must also keep a cached copy of the reply, until such time as ACK[N] is received. After sending reply[N], the server may receive ACK[N], indicating all is well, or may receive request[N] again, indicating that reply[N] was lost, or may experience a timeout, indicating that either reply[N] or ACK[N] was lost. In the latter two cases, the server should retransmit reply[N] and wait again for ACK[N]. exactly-once semantics : RPC connection between client and server. neither client nor server crashed. no packet reordering. every request, reply, ACK arrives to its destination (no data loss). 11.5.1 Network File System \u00b6 the application making the greatest use of early RPC was Sun's Network File System, NFS allowed to the file system of the server to be available for clients. client opens a file => server responds with file handle, that includes the file inode number . client sends: read(dataBlockNumber) => server responds with Data of 8KB packets . client sends: write(dataBlockNumber, dataToBeWritten) => server responds with ACK. Usually an 8 kB block of data would be sent as a single UDP/IPv4 packet, using IPv4 fragmentation by the sender for transmission over Ethernet. 11.5.2 Sun RPC \u00b6 developed by Sun Microsystems, documented in RFC 1831. now officially known as Open Network Computing ONC . ACKs was omitted . server stops caching replies, since ACK was omitted. so the server will re-execute the requests on duplicated requests. at-least-once semantics : client sent a request, and received a reply. the client is sure that the request has been executed at least once. if the reply got lost, the client will retransmit the request and the request will be re-executed bu the server. idempotent request : is a request that has the same results and the same side effects on the server wether executed once or twice or more. at-least-once semantics allow the server to be stateless The lack of file-locking and other non-idempotent I/O operations, along with the rise of cheap client workstation storage (and, for that matter, more-reliable servers), eventually led to the decline of NFS over RPC, though it has not disappeared. NFS can, if desired, also be run (shamefully) over TCP. 11.5.3 Serial Execution \u00b6 serial execution is automatic if request[N+1] serves as implicit ACK[N]. Disk drives commonly use the elevator algorithm to process requests. 11.5.4 RPC Refinements \u00b6 One basic network-level improvement to RPC concerns the avoidance of IP-level fragmentation","title":"Unit 6"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#unit-6","text":"","title":"Unit 6"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#definition","text":"bottleneck : the slowest link in delivering packets. RTT : round-trip time, the time that a packet consumed from its sending till its arrival.","title":"definition"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#6-abstract-sliding-windows","text":"Build reliable data-transport layers on top of unreliable lower layers. Can be achieved by used retransmit-on-timeout policy. Retransmit-on-timeout (achieve reliability ) If packet is transmitted and no acknowledgment received during the timeout period, the packet will be resent. Protocols use this called: ARQ protocols: Automatic Repeat reQuest. Sliding windows algorithm is used to manage congestions and not giving the network more packets than it can handle. End-to-End principle suggest that building reliability on the lower layers is wrong, instead reliability should be built on the endpoints of a connection, as ARQ and retransmit-on-timeout do.","title":"6. ABSTRACT SLIDING WINDOWS"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#61-building-reliable-transport-stop-and-wait","text":"Retransmit-on-timeout requires sequence numbering for the packets . if the network guarantees that the packets won't be reordered we can omit the sequence numbers. but no-reordering is not guaranteed on the Internet so we have to use sequence numbers. Data[N] wil be the packet number N, and ACK[N] will be the acknowledgment number N. stop-and-wait is a version of retransmit-on-timeout where the sender sends only one outstanding packet (Data[N]) at a time and waits for ACK[N] to be received. if ACK[N] is not received, Data[N] will be retransmitted. Data[N+1] will never be retransmitted until ACK[N] is received.","title":"6.1 Building Reliable transport: Stop-and-Wait"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#611-packet-loss","text":"in case of packet loss, the receiver won't receive packet[N] so the sender will retransmit Data[N] again. if the receiver receives a packet and send ACK, but the ACK get lost and never received by the sender, so the sender will transmit Data[N] again and the receiver will receive duplicate copies of Data[N] the receiver usually implements retransmit-on-duplicate policy, so in the case of receiving a duplicate packets it will sent the same ACK[N]. at least one side must implement retransmit-on-timeout policy, either the receiver or the sender; otherwise a lost packet will lead to deadlock as the server and the sender will wait forever. AND the other end must implement either retransmit-on-timeout OR transmit-on-duplicate policy. it is fine for the sender and receiver to implement retransmit-on-timeout policy with different timeout value .","title":"6.1.1 Packet Loss"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#612-sorcerers-apprentice-bug","text":"A strange thing happens if one side implements retransmit-on-timeout but both sides implement retransmit-on-duplicate. if the ACK[N] delayed then all of the later packets will be sent twice . thee transfer will still work normally, but it will take double the bandwidth. Fix: have one side (usually the sender) to implement retransmit-on-timeout only policy as in TCP protocol .","title":"6.1.2 Sorcerer\u2019s Apprentice Bug"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#613-flow-control","text":"stop-and-wait has a flow control to prevent data from arriving at the receiver faster than it can be handled . if the receiver needs less or slightly larger time than RTT (round-trip time) everything will work fine with stop-and-wait. if the receiver needs much more time than RTT (round-trip time) then the packet might be timed out and retransmitted from the sender since the receiver won't send the ACK until packet is completely received and transmitted. one solution is have 2 types of Acknowledgments: ACK Wait [N] which means that data has been received but receiver is not ready yet for Data[N +1] ACK GO [N] which means that data has been received and processed by receiver and it is ready for the next packet. a problem may arise if the ACK GO [N] is lost, then the connection is deadlock since the receiver and the sender keep waiting for each other.","title":"6.1.3 Flow Control"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#62-sliding-windows","text":"stop-and-wait is very reliable but not very efficient . most links with multi-hop stop-and-wait path will remain idle most of the time, which is a thing that we don't want. we can work around by allowing the sender to send few packets without waiting for their acknowledgments, but this bad since the packets may: end up in waiting queues on switches a long the way. packets get lost (discarded if the queues are full). packets get lost (discarded if the receiver does not have enough bandwidth). sliding windows : the sender picks a window size of packets that can be transmitted without waiting for their acknowledgments. acknowledgments are accumulative which means that ACK[N] will never been sent until the receiver has received all acknowledgment ACK[k] for k <= N . the sender keeps tracking of a variable representing the last ACK received. sender will send multiple packets equals to the window size and waits for the ACK of the last packet. with every new ACK received, the sender updates the last ACK received variable, and the window slides by [last ACK + window size] . sliding-windows is self-clocking so that it automatically reduces the sender rate whenever the available fraction of the bottleneck bandwidth is reduced. self-clocking property happens since the sliding will happen according to the pace of the slowest link (bottleneck). example https://vimeo.com/150452468","title":"6.2 Sliding Windows"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#621-bandwidth-delay","text":"BDP (bandwidth delay product) = bandwidth * delay BDP0 ( transit capacity = amount of data can be sent before the first response) = bandwidth * RTT BDP0 is the optimal value for the window size if the chosen windowSize > BDP0 then RTT] will grow due to the queues that will form on the links, so the new RTT[actual] is different from the RTT[no-load] in the beginning. choosing windowSize less than BDP0 means that we are not using all network power. chosen windowSize less than BDP0 means that we are introducing queues somewhere in the network. in practice neither the bandwidth or RTT is constant, so that the windowSize keep changing according to the other traffic in the network (which changes the available bandwidth then BDP).","title":"6.2.1 Bandwidth * Delay"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#622-the-receiver-side","text":"if the transmission happens through a single link, then the packets will never be reordered and windowSize of 1 works perfectly. with the user of switches, no-reordering is not guaranteed, so the receiver has to use the same windowSize as the sender. if the windowSize is [11,12,13,14] and the receiver received 12,13,14 packets but 11 is delayed, then the receiver must buffer [12,13,14] packets waiting for the 11 to arrive. receiver also keeps track of lastSentACK , and at any time the receiver is willing to accept the packet Data[lastSentACK + 1] and when this packet arrived the receiver needs to check its buffers to see if subsequent packets are in the buffer , so the receiver process them and send the the largest accumulative ACK for the latest received packet","title":"6.2.2 The receiver side"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#623-loss-recovery-with-sliding-windows","text":"if windowSize is 4 and packet[5] is lost, then the sender will only receive ACK[4]. sender will only retransmit packet[5] then waits for the next ACK. if the receiver has received packets, 6,7,8 then it will send ACK[8] and the receiver will not retransmit the packets 6,7,8. and window will slide [9-13]. if the packets 6,7,8 have also lost , then the sender will receive ACK[5] and sender will retransmit the packets 6,7,8 and the window will slide [5-9]. the packets 6,7,8 will timeout shortly after packet[5] timeout. in this case, sliding-windows protocol will suppress further retransmission until recovery is complete . pipeline drain happens when the sliding-windows goes into halt after full timeout , after recovery from the full timeout, the sliding-windows needs to startup again . most TCP implementations has fast recovery for early detection of packets loss before the pipeline has fully drained.","title":"6.2.3 Loss Recovery with sliding-windows"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#63-linear-bottlenecks","text":"bottlenecks are the slowest links (eg. R2-R3, R3-R4 in the image above), they are most likely where the queues will form. when using slide-windows, the sending rate will adjusted according to the bottleneck rate.","title":"6.3 Linear bottlenecks"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#631-simple-fixed-window-size-analysis","text":"we will analyze the windowSize on overall throughput on RTT. in the diagram above, bottleneck is R1-R2. any queues will form at R1. the bottleneck bandwidth is 1 packet/sec so the path bandwidth is 1 packet/sec. there are 4 links, each of them consumes 1 sec. so the RTT = 4 secs . (assuming the ACN path is infinitely fast so that can be omitted). bottleneck is S1-S2. any queues will form at S1. the path bandwidth is 1 packet/sec (equals to the bottleneck bandwidth). RTT = 4 secs. (0 (C-S1, infinitely fast) + 1 sec (S1-S2) + 1 sec (S2-D) + 1 sec (D-S2, for ACK) + 1 sec (S2-S1, for ACK) + 0 sec (S1-C, for ACK), infinitely fast ). NOTE 1 : in both example the propagation delay is assumed to be zero and omitted. NOTE 2 : we assume a single connection is made. BDP = bandwidth * RTT = 4. lets analyze different scenarios with different windowSize:","title":"6.3.1 Simple Fixed-window-size analysis"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#6311-case-1-windowsize-2","text":"windowSize < BDP. every ACK will take BDP = 4 to arrive. throughput: 2 packets in 4 seconds = 2/4 = 0.5 packet/sec. (50% of the bandwidth is utilized). notice the brief pile-up (queue) on the bottleneck on startup. in the steady state, no queues are formed. notice that during each second: 2 of the 4 links remain idle . RTT for packet[4] is 9 secs. steady state achieved at time 9.","title":"6.3.1.1 case 1: windowSize = 2"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#6312-case-2-windowsize-4","text":"windowSize = BDP. every ACK will take BDP = 4 to arrive. throughput: 4 packets in 4 seconds = 4/4 = 1 packet/sec. (100% of the bandwidth is utilized). notice the queue on R1 lasts longer on startup, but after that no queues are formed. RTT fro packet[4] is 7 seconds. steady state achieved at time T = 4. this is the best possible throughput. at every second, all links are busy. this case is the congestion knee .","title":"6.3.1.2 case 2: windowSize = 4"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#6313-case-3-windowsize-6","text":"windowSize > BDP. every ACK will take BDP = 4 to arrive. throughput: 4 packets in 4 seconds = 4/4 = 1 packet/sec. (100% of the bandwidth is utilized). notice the heavy queue on R1.","title":"6.3.1.3 case 3: windowSize = 6"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#632-rtt-calculations","text":"RTT no-load is physical travel time that is subjective to the limitations of delay; any tine excess RTT no-load is spent waiting in queues. so queueTime = RTT actual - RTT no-load . When the bottleneck link is saturated, that is, is always busy, the number of packets actually in transit (not queued) somewhere along the path will always be bandwidth \u02c6 RTTnoLoad. and throughput = windowSize/RTTactual where \u201cthroughput\u201d is the rate at which the connection is sending packets. In the sliding windows steady state, where throughput and RTTactual are reasonably constant, the average number of packets in the queue is just throughput * queue_time (where throughput is measured in packets/sec): so queue_usage = throughput * (RTTactual \u2013 RTTnoLoad) = winSize * (1 \u2013 RTTnoLoad/RTTactual) and RTTactual = winSize/bottleneck_bandwidth queue_usage = winSize \u2013 bandwidth * RTTnoLoad RTTactual/RTTnoLoad = winSize/transit_capacity = (transit_capacity + queue_usage) / transit_capacity","title":"6.3.2 RTT calculations"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#633-graphs-at-the-congestion-knee","text":"The critical windowSize value is equal to bandwidth \u02c6 RTTnoLoad; this is known as the congestion knee. For winsize below this, we have: throughput is proportional to winsize delay is constant queue utilization in the steady state is zero connection power is proportional to the windowSize. For winsize larger than the knee, we have throughput is constant (equal to the bottleneck bandwidth) delay increases linearly with winsize queue utilization increases linearly with winsize connection power is proportional to the 1/windowSize. Ideally, winsize will be at the critical knee. However, the exact value varies with time: available bandwidth changes due to the starting and stopping of competing traffic, and RTT changes due to queuing. Standard TCP makes an effort to stay well above the knee much of the time, presumably on the theory that maximizing throughput is more important than minimizing queue use. connection power = throughput / RTT","title":"6.3.3 Graphs at the Congestion Knee"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#634-simple-packet-based-sliding-windows-implementation","text":"pseudo code outline of the receiver side of a sliding-windows implementation, ignoring lost packets and timeouts. Receiver side: W: winsize LA: last_ACKed = 0 The next packet expected is LA+1 Window is [LA+1, . . . , LA+W] use EarlyArrivals data structure: EarlyArrivals is an array of objects of size W. we always out Data[M] at the index of of M % W Upon arrival of Data[M]: if M <= LA or M > LA+W, ignore the packet if M > LA+1, put the packet into EarlyArrivals. if M == LA+1: deliver the packet (that is, Data[LA+1]) to the application LA = LA+1 (slide window forward by 1) while (Data[LA+1] is in EarlyArrivals) { output Data[LA+1] LA = LA+1 } send ACK[LA] pseudo code outline of the sender side of a sliding-windows implementation, ignoring lost packets and timeouts. Sender side: W: winsize LA: last_ACKed = 0 start by sending a full windowFull of packets. Data[1, ..., W] Upon arrival of ACK[M]: if M <= LA or M > LA+W, ignore the packet otherwise: set K = LA + W + 1, the first packet just above the old window set LA = M, just below the bottom of the new window for (i=K; i <= LA+W; i++) send Data[i]","title":"6.3.4 Simple Packet-Based Sliding-Windows Implementation"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#11-udp-transport","text":"TCP and UDP works above IP layer. UDP provides simple datagram delivery to remote sockets \\<host, port>. TCP provides much more richer functionality for sending data, but requires the remote socket first to be connected. in this chapter: introduce UDP. introduce UDP-based Trivial File Transfer Protocol. fundamental issues any transport protocol must address: lost final packets late arriving packets.","title":"11. UDP transport"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#111-user-datagram-protocol-udp","text":"it is in RFC 1122 . UDP is almost a null protocol , since it is very basic. UDP adds 2 functionalities above IP layer: port numbers. checksum. UDP header: using port number, an application can connect to a an individual server process that owns this port number instead of connecting to the host as whole. UDP is unreliable : no reattempt-at-timeout, acknowledgments, or retransmission. socket = UDP UDP is unconnected (stateless) : if an application opens a port on a host; any other host on the internet can deliver packets to the socket with no questions asked. UDP packets uses 16-bit Internet checksum : checksum can be disabled by setting all-16-bits to 0. UDP checksum contains: UDP header. UDP data. pseudo-IP header: source and destination IP addresses. UDP packets can be can be dropped due to: queue overflow** on an intermediate router. queue overflow on the host: data is arriving faster than the receiver Higher level protocol use acknowledgment to form flow control that prevents packets from being dropped. UDP is popular for local transport into the same LAN. UDP is the base transport basis for Remote Procedure Call, RCP : a host invokes a procedure in another host, where the parameters and the return value of the procedure are transported back and forth through UDP. UDP is good for request-reply connections : TCP can be used, but it requires additional overhead of creating and destroying the connection. DNS uses UDP over TCP for the above reason, but if we have a sequence of request-reply operations, then TCP is worth the overhead . UDP is popular for real-time transport : with TCP if a packet is lost, the receiving host has no other option but to queue the subsequent packets till the lost packet arrives, which may tack several RTTs. UDP is better for real-time transport, since it give the receiving host the option to just ignore the lost packets, so that UDP is good for voice and video transport which are loss-tolerant but delay-intolerant . Real-time Transfer Protocol, RTP is built on top of UDP because of its loss-tolerance. VoIP uses UDP over TCP . UDP is used in flooding attacks : since it is easy to send UDP packets with spoofed IP addresses; with TCP is not hard to send TCP connection-request (SYN) packets with spoofed IP addresses, but the connection won't last till the malicious packets have been delivered to the application process. UDP enables traffic amplification attacks : the attacker sends a small message to a server, with spoofed source address, and the server then responds to the spoofed address with a much larger response message. One approach is for the server to limit the size of its response \u2013 ideally to the size of the client\u2019s request until it has been able to verify that the client actually receives packets sent to its claimed IP address.","title":"11.1 User DataGram Protocol UDP"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1111-quic","text":"UDP allows new protocols to run as user-space applications, no kernel updates are required. QUIC Quick UDP Internet Connection is a protocol created by google to support HTTPS (HTTP + TLS). QUIC allows supporting multiplexed streams in a single connection : a lost packet will block its stream until retransmits, while other streams continue without waiting. QUIC supports error-correcting codes . QUIC eliminates the initial RTT needed to create the TCP connection. QUIC provides support for advanced congestion control (on the application layer, new versions do this entirely on the server end). One downside of QUIC is its nonstandard programming interface.","title":"11.1.1 QUIC"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1112-dccp","text":"DCCP Datagram Congestion Control Protocol outlined in RCF 4340. build on top of UDP. adds number of TCP-like features to UDP. DCCP packets are delivered to the application in *the order of arrival rather than the order of sequence numbers. DCCP uses Acknowledgments, but for congestion control . DCCP does support reliable delivery of control packets, used for connection setup and teardown, and option negotiation. DCCP packets includes: application-specific UDP prot number 32-bit service code : allows finer-grained packet handling which identifies the incoming packets and prevent conflicts. DCCP run on the operating system kernel (opposite to QUIC which runs on the user-space): because the ECN congestion-feedback mechanism requires setting flag bits in the IP header, and most kernels do not allow user-space applications to do this.","title":"11.1.2 DCCP"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1113-udp-simplex-talk","text":"One of the early standard examples for socket programming. the client side reads text from the user's terminal and sends them through the network to the server, which put them in the server's terminal. the server don't send any response or ACKs , so it is one way flow . simplex refers to the one-way flow (opposite to duplex-talk used in Instant Messaging IM ). the server side must select a port number, which with the server\u2019s IP address will form the socket address to which clients connect. On the server side, simplex-talk must do the following: ask for a designated port number create a socket, the sending/receiving endpoint bind the socket to the socket address, if this is not done at the point of socket creation receive packets sent to the socket for each packet received, print its sender and its content The client side has a similar list: look up the server\u2019s IP address, using DNS create an \u201canonymous\u201d socket; we don\u2019t care what the client\u2019s port number is read a line from the terminal, and send it to the socket address","title":"11.1.3 UDP Simplex-Talk"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1114-netcat","text":"netcat or nc is a utility that enables sending and receiving UDP and TCP packets.","title":"11.1.4 netcat"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1115-binary-data","text":"when sending a binary data packets through UDP, the client and server must agree on encoding . big-endian encoding is used by IBM. little-endian encoding is used by Intel. big-endian encoding is the most used, and known as network byte order . converting from host byte order to network byte order is language-dependent. but should always assume big-endian encoding In java, the client needs to convert int[] to byte[] , then the receiving server needs to convert the packet back from byte[] to int[] . the DataOutputStream and DataInputStream help in the converting operations. In the C language, we can simply allocate a char[] of the appropriate size and write the network-byte order values directly into it.","title":"11.1.5 binary data"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#112-trivial-file-transport-protocol-tftp","text":"TFTP supports file transfer in both directions . TFTP does not support authentication : all files are available to everyone. TFTP is UDP-based, well-suited for downloading startup files TFTP uses stop-and-wait : with fixed timeout interval. TFTP used in internal LANs because its limited security . TFTP documented in RFC 783, and in RFC 1350. TFTP has 5 packet types: Read ReQuest, RRQ: contains filename and text/binary indication. Write ReQuest, WRQ. Data: 16-bit block number then up to 512 bytes of data. ACK, 16-bit block number. Error, All errors other than \u201cUnknown Transfer ID\u201d are cause for sender termination. All blocks of data contain 512 bytes except the final block, which is identified as the final block ; if the data divisible by 512, the final block will be sent containing 0 bytes of data . TFTP must take care of packetization (UDP, opposed to TCP which takes care of packetization), TFTP must use small block size to avoid fragmentation . The TFTP server listens on UDP port 69 for arriving RRQ packets, the server will create separate process or thread for every requested file as child process ; the new child process will have entirely new UDP port , and it will be used for further communication with this client. the new port will: prevent old-duplicate packets. child process will be responsible for handling one client only, all packets are now arriving to the port of the child process. downside: f preventing the use of TFTP through NAT firewalls without port change, handling multiple clients will be very complicated, as the server would have to sort out, for each arriving packet, which transfer it belonged to. Each transfer would have its own state information including block number, open file, and the time of the last successful packet. TFTP file requests work as follows: The client sends a RRQ to server port 69. The server creates a child process, which obtains a new port, s_port, from the operating system. The server child process sends Data[1] from s_port. The client receives Data[1], and thus learns the value of s_port. The client will verify that each future Data[N] arrives from this same port. The client sends ACK[1] (and all future ACKs) to the server\u2019s s_port. The server child process sends Data[2], etc, each time waiting for the client ACK[N] before sending Data[N+1]. The transfer process stops when the server sends its final block, of size less than 512 bytes, and the client sends the corresponding ACK.","title":"11.2 Trivial File Transport Protocol TFTP"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#113-fundamental-transfer-issues","text":"These issues includes: old duplicate packets lost final ACK duplicated connection request reboots","title":"11.3 Fundamental Transfer issues"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1131-old-duplicate-packets","text":"packets from the past arriving very late, but being accepted as current . external old duplicate : the previous connection has been closed, then a new connection has been opened (same socket and ports), then a delayed packet of the previous connection appears in the right order of the new connection, so it is being accepted as a legitimate packet of the current connection => file transfer will be corrupted. internal old duplicate : happens in the same connection instance, . For example, if TFTP allowed its 16-bit block numbers, then a very old Data[3] might be accepted in lieu of Data[3+(2^16)]. these are usually prevented by numbering the data,and use sufficiently enough bits to prevent number overlapping. to prevent internal duplicates due to number overlapping , and since it uses 16-bit numbering, the maximum file size (51B * max number of packets numbering) = 512B = ((2^16) - 1) = 32 MB . port numbers should be chosen randomly, so that the probability that the same number is chosen twice in immediate succession is very low. to prevent external duplicates , TFTP requires a new port number for each new connection (separate transfer): if ports chosen randomly, there are 1/(2^32) that the same port number will used twice. if ports were chosen by OS, we assume that OS won't reissue the same port twice in rapid succession . if one side (client or server) choses the same port, the probability the same 2 ports will be chosen is 1/(2^16). reasons why the packet from the old connection arriving late: A first copy of the old duplicate was sent A routing error occurs; the packet is stuck in a routing loop An alternative path between the original hosts is restored, and the packet is retransmitted successfully Some time later, the packet stuck in the routing loop is released, and reaches its final destination TCP officially once has a limit of 60 seconds, (now is 30 seconds) to assume that all old packets are now discarded and it is safe to reopen the connection with the same ports again. IP considers 255 seconds as the safe time. It is also possible to prevent external old duplicates by including a connection count parameter in the transport or application header. For each consecutive connection, the connection count is incremented by (at least) 1. A separate connection-count value must be maintained by each side; if a connection-count value is ever lost, a suitable backup mechanism based on delay might be used.","title":"11.3.1 old duplicate packets"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1132-lost-final-ack","text":"The final packet is alway an ACK, which can not be Acknowledged . TFTP recommend the sender to enter the DALLY state after sending the final ACK. in the DALLY state, the receiver responds to duplicated final ACK , by retransmitting the final ACK back to the sender . receiver does not respond to the single final ACK, and the sender will exit the DALLY state shortly. The time of the DALLY state should be twice the timeout of the receiver at least . so the sender can retry sending the final ACK 3 times . Note also that dallying only provides increased assurance, not certainty: it is possible that all final ACKs were lost. The TCP analogue of dallying is the TIMEWAIT state, which also prevents old duplicates.","title":"11.3.2 lost final ACK"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1133-duplicated-connection-requests","text":"we need to distinguish between duplicated connection requests (same connection) and requests to open a new connection. scenario: client send RRQ('foo') > client aborts connection foo > client sends RRQ('bar') > server responds of Data[1] of foo connection. if the client starts the new connection from a new port, it is fine, the data of the previous connection will be sent to the old port . TFTP does not have cancellation message . if -for unexpected reason- the client sends the new connection from the same port, then it is a problem. in case of duplicate connection requests, the server will start 2 different processes , then the receiver should accept one successfully, and responds with ERROR of Unknown Transfer ID to the second process , transfer ID refers to the port number. the ERROR state from the receiver causes the associated process to shutdown , and the other process to continue normally. It is theoretically possible for a malicious actor on the LAN to take advantage of this TFTP \u201clatching on\u201d behavior to hijack anticipated RRQs, and sends malicious data to all ports of the victim (the client).","title":"11.3.3 Duplicated connection requests"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1134-reboots","text":"one side might reboot between receiving the messages form the other side, the other side must detect the reboot and close the old connection. if the receiver (client) reboots , the sender will keep sending packets but no further ACK will be received. rebooted side will lose all its memory, which may lead to the possibility of reusing the same port in the post-reboot connection. In practical terms, this scenario seems to be of limited importance, though \u201cdiskless\u201d devices often do use TFTP to request their boot image file when restarting, and so might be potential candidates.","title":"11.3.4 reboots"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#114-other-tftp-notes","text":"","title":"11.4 other TFTP notes"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1141-tftp-and-the-sorcerer","text":"TFTP uses stop-and-wait, its ACK includes the block number of the packet being acknowledged. TFTP was vulnerable to the Sorcerer\u2019s Apprentice bug. fix: the sender (ie, the side originating the DATA packets) must never resend the current DATA packet on receipt of a duplicate ACK.","title":"11.4.1 TFTP and the sorcerer"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1142-tftp-states","text":"DALLY state. UNLATCHED: when the client-receiver sends RRQ, it does not know the port number to latch on yet . ESTABLISHED: when the client-receiver receives DATA[1], the port number is now known, and the connection is ESTABLISHED. ERROR.","title":"11.4.2 TFTP states"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1143-tftp-throughput","text":"on a single ETHERNET , sender and receiver will alternate using the same channel, so the throughput is optimal . As soon as the store-and-forward delays of switches and routers are introduced ( multiple ETHERNETs ), though, stop-and-wait becomes a performance bottleneck.","title":"11.4.3 TFTP throughput"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#115-remote-procedure-call-rpc","text":"usually implemented on to of UDP. example is DNS : a host sends a DNS look up request to the DNS server and receives a reply. RPC is also quite successful as the mechanism for interprocess communication within CPU clusters, perhaps its most time-sensitive application. requests and replies must be numbered , so the client knows which reply is to which request, and the reply can serve as acknowledgment to the request. Request[N] timeout When the server creates reply[N] and sends it to the client, it must also keep a cached copy of the reply, until such time as ACK[N] is received. After sending reply[N], the server may receive ACK[N], indicating all is well, or may receive request[N] again, indicating that reply[N] was lost, or may experience a timeout, indicating that either reply[N] or ACK[N] was lost. In the latter two cases, the server should retransmit reply[N] and wait again for ACK[N]. exactly-once semantics : RPC connection between client and server. neither client nor server crashed. no packet reordering. every request, reply, ACK arrives to its destination (no data loss).","title":"11.5 Remote Procedure Call RPC"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1151-network-file-system","text":"the application making the greatest use of early RPC was Sun's Network File System, NFS allowed to the file system of the server to be available for clients. client opens a file => server responds with file handle, that includes the file inode number . client sends: read(dataBlockNumber) => server responds with Data of 8KB packets . client sends: write(dataBlockNumber, dataToBeWritten) => server responds with ACK. Usually an 8 kB block of data would be sent as a single UDP/IPv4 packet, using IPv4 fragmentation by the sender for transmission over Ethernet.","title":"11.5.1 Network File System"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1152-sun-rpc","text":"developed by Sun Microsystems, documented in RFC 1831. now officially known as Open Network Computing ONC . ACKs was omitted . server stops caching replies, since ACK was omitted. so the server will re-execute the requests on duplicated requests. at-least-once semantics : client sent a request, and received a reply. the client is sure that the request has been executed at least once. if the reply got lost, the client will retransmit the request and the request will be re-executed bu the server. idempotent request : is a request that has the same results and the same side effects on the server wether executed once or twice or more. at-least-once semantics allow the server to be stateless The lack of file-locking and other non-idempotent I/O operations, along with the rise of cheap client workstation storage (and, for that matter, more-reliable servers), eventually led to the decline of NFS over RPC, though it has not disappeared. NFS can, if desired, also be run (shamefully) over TCP.","title":"11.5.2 Sun RPC"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1153-serial-execution","text":"serial execution is automatic if request[N+1] serves as implicit ACK[N]. Disk drives commonly use the elevator algorithm to process requests.","title":"11.5.3 Serial Execution"},{"location":"knowledge-base/cs2204-netwroks1/unit6/unit6/#1154-rpc-refinements","text":"One basic network-level improvement to RPC concerns the avoidance of IP-level fragmentation","title":"11.5.4 RPC Refinements"},{"location":"knowledge-base/cs2204-netwroks1/unit6/wa6/","text":"Unit 6: written assignment \u00b6 Assuming standard 1500 byte Ethernet max payloads: how many IPv4 fragments will be needed to transfer 2000 bytes of user data with a single UDP send? And, how do the 2000 bytes get split over the frags? IP header size = 20 bytes. UDP header size = 8 byes. available space from the Ethernet packet: 1500 - (20 + 8) = 1472 bytes. The first fragment will ship 1472 bytes of user data. The second fragment (last) will ship 2000 - 1742 = 528 bytes. it will be fragmented into 2 fragments, 1472 of data in the first; 528 bytes in the last. Despite its conceptual elegance, RPC (Remote Procedure Call) has a few problems. Discuss any 3 of those in brief. IP-level fragmentation: if the data is so big, it needs to be fragmented at the IP level so that it can be transmitted successfully; this fragmentation may slow down the transfer; possibility that the client or the server might crash and reboot: because this type of communication requires 2 machines to stays in sync, at the start of the transfer and during it; the fact that the 2 machines may be different puts more possibility for failure. if the client is single threaded, then it will stay freeze till it get the full reply from the server. Why is timestamps needed in real-time applications? This is in the context of Real-time Transport Protocol (RTP) . timestamps are used as block numbers in the real-time connections, which saving the sender from tracking the last received or sent packet; and also eliminates the need for caching packets; if a disruption happens on the link (eg.delay due to queueing), and a few packets got lost; the receiver can easily recover by putting the last received packet as the current (depending on its timestamp) and ignore all missing packets behind its timestamp. after recovery, if any of the missing packets arrived very late; its timestamp will be compared to the current timestamp, and if its from the past (eg. more than N RTTs) it can be discarded, knowing that injecting it this timestamp will corrupt data. Why does UDP exist? Would it not have been enough to just let user processes send raw IP packets? well, UDP is on the layer 4, while IP is on the layer 3; the only responsibility of the IP packet is to deliver the raw data to the receiver machine; it does not -and it should not- worry about the content of the packet or how it should be presented to the internal processes at the receiver machine. UDP (or TCP) takes charge of picking up the data from the processes in the sender machine, do the necessary processing (marshaling), prepare the packet to be sent through IP, if fragmentation needed, UDP will handle that with th IP, putting this functions away from the processes themselves. on the receiver side, UDP will receive IP fragments; connect them back into one UDP packet (if necessary), then un-marshal them back into data that is suitable for the receiver processes and so on. The fact that IP packets might be in different sizes, and IP does not have the idea of ports; makes the use of UDP more convenient. Explain how QUIC eliminates a couple of RTTs usually needed at the start of a secure web connection. QUIC uses UDP over TCP, adding the missing security standard to it. since QUIC is using UDP, it eliminates the need of establishing a secure connection before hand; the connection negotiation along with the first data request are to start transmitting with the first packet. QUIC eliminates the Handshake delay (Adam, 2017). References \u00b6 Langley Adam et al. (2017) The QUIC Transport Protocol: Design and Internet-Scale Deployment. http://www.audentia-gestion.fr/Recherche-Research-Google/46403.pdf","title":"Unit 6: written assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit6/wa6/#unit-6-written-assignment","text":"Assuming standard 1500 byte Ethernet max payloads: how many IPv4 fragments will be needed to transfer 2000 bytes of user data with a single UDP send? And, how do the 2000 bytes get split over the frags? IP header size = 20 bytes. UDP header size = 8 byes. available space from the Ethernet packet: 1500 - (20 + 8) = 1472 bytes. The first fragment will ship 1472 bytes of user data. The second fragment (last) will ship 2000 - 1742 = 528 bytes. it will be fragmented into 2 fragments, 1472 of data in the first; 528 bytes in the last. Despite its conceptual elegance, RPC (Remote Procedure Call) has a few problems. Discuss any 3 of those in brief. IP-level fragmentation: if the data is so big, it needs to be fragmented at the IP level so that it can be transmitted successfully; this fragmentation may slow down the transfer; possibility that the client or the server might crash and reboot: because this type of communication requires 2 machines to stays in sync, at the start of the transfer and during it; the fact that the 2 machines may be different puts more possibility for failure. if the client is single threaded, then it will stay freeze till it get the full reply from the server. Why is timestamps needed in real-time applications? This is in the context of Real-time Transport Protocol (RTP) . timestamps are used as block numbers in the real-time connections, which saving the sender from tracking the last received or sent packet; and also eliminates the need for caching packets; if a disruption happens on the link (eg.delay due to queueing), and a few packets got lost; the receiver can easily recover by putting the last received packet as the current (depending on its timestamp) and ignore all missing packets behind its timestamp. after recovery, if any of the missing packets arrived very late; its timestamp will be compared to the current timestamp, and if its from the past (eg. more than N RTTs) it can be discarded, knowing that injecting it this timestamp will corrupt data. Why does UDP exist? Would it not have been enough to just let user processes send raw IP packets? well, UDP is on the layer 4, while IP is on the layer 3; the only responsibility of the IP packet is to deliver the raw data to the receiver machine; it does not -and it should not- worry about the content of the packet or how it should be presented to the internal processes at the receiver machine. UDP (or TCP) takes charge of picking up the data from the processes in the sender machine, do the necessary processing (marshaling), prepare the packet to be sent through IP, if fragmentation needed, UDP will handle that with th IP, putting this functions away from the processes themselves. on the receiver side, UDP will receive IP fragments; connect them back into one UDP packet (if necessary), then un-marshal them back into data that is suitable for the receiver processes and so on. The fact that IP packets might be in different sizes, and IP does not have the idea of ports; makes the use of UDP more convenient. Explain how QUIC eliminates a couple of RTTs usually needed at the start of a secure web connection. QUIC uses UDP over TCP, adding the missing security standard to it. since QUIC is using UDP, it eliminates the need of establishing a secure connection before hand; the connection negotiation along with the first data request are to start transmitting with the first packet. QUIC eliminates the Handshake delay (Adam, 2017).","title":"Unit 6: written assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit6/wa6/#references","text":"Langley Adam et al. (2017) The QUIC Transport Protocol: Design and Internet-Scale Deployment. http://www.audentia-gestion.fr/Recherche-Research-Google/46403.pdf","title":"References"},{"location":"knowledge-base/cs2204-netwroks1/unit7/","text":"CS2204: Unit7: The Transport Layer (TCP) \u00b6 standard transport protocols riding above IP layer, are TCP,UDP . UDP provides simple datagram delivery to remote sockets, that is, to pairs. TCP provides a much richer functionality for sending data to (connected) sockets. TCP is stream-oriented : application can write data in small or large chunks and TCP layer will do the packetization . TCP transmit stream of bytes, not messages or records. TCP is connection-oriented . TCP is reliable : TCP uses sequence numbers to ensure the correct order of delivery and a timeout/retransmission mechanism to make sure no data is lost short of massive network failure. TCP uses sliding windows by default, to achieve throughput relatively close to the maximum available. TCP works fine for: transferring large files. interactive applications, each end is sending and receive streams of small packets. ssh, telnet, db connections as examples of interactive applications. request/reply protocols: sender sends one request and receives replies, then the connection is closed. not desirable due to the overhead of creating the connection. if UDP opens a socket, then any client on the internet can transmit to that socket; so the UDP application needs to have mechanism to check that the socket is receiving messages from the right client for every packet . with TCP all data coming into the connected socket must be sent form the legitimate client who opened the socket. when the socket is not connected: it is ine LISTENING state. to transmit to a TCP, client must open a connection defined by the socketPair as on both ends of the connection. for this connection a new child socket S c is created, that includes creating a new thread or process . 12.1 End-to-End principle \u00b6 transport issues are the responsibility of the endpoints in question, and not the core network. this addresses: data corruption, and congestion . data corruption: all links on the internet have link-layer-checksums to protect against corruption; but TCP still have to add its own checksum . TCP is the only layer has congestion management . 12.2 TCP Header source and destination ports are 16-bits 4-bits data offset specifies the number of 32-bits words in the header. default value is 5 . checksum covers: TCP header TCP data IP pseudo header that includes source and destination IP addresses. the checksum must be updated by a NAT router that modifies any header values. The sequence and acknowledgment numbers are for numbering the data, at the byte level. TCP can send 1024-byte blocks of data, incrementing the sequence number by 1024. TCP can also send 1-byte telnet packets, incrementing the sequence number by 1. There is no distinguish between the data and acknowledgements packets : all packets from A to B are sending the most current acknowledgment from B to A. and vice versa. packets in TCP are called segments sequence numbers represent the position of the first byte of data. the acknowledgment number is sequenceNumber + number of data bytes + 1 which represents the position of the first byte of data in the next segment. The sequence and acknowledgment numbers, as sent, represent these relative values plus an Initial Sequence Number , or ISN , that is fixed for the lifetime of the connection . Each direction of a connection has its own ISN; TCP acknowledgments are cumulative : when an endpoint sends a packet with an acknowledgment number of N, it is acknowledging receipt of all data bytes numbered less than N. TCP header flags : SYN : for SYNchronize; marks packets that are part of the new-connection handshake. ACK : indicates that the header Acknowledgment field is valid; that is, all but the first packet. FIN : for FINish; marks packets involved in the connection closing. PSH : for PuSH; marks \u201cnon-full\u201d packets that should be delivered promptly at the far end. RST : for ReSeT; indicates various error conditions. URG : for URGent; part of a now-seldom-used mechanism for high-priority data CWR and ECE : part of the Explicit Congestion Notification mechanism. 12.3 TCP connection establishment. \u00b6 established with three-way handshake that involves three steps: C send a packet with SYN bit set to the S. S responds a SYN packet of its own; ACK flag is set. C acknowledges the S's SYN with its own ACK . the three-way handshake is triggered by client; and any exchange of data should happen after the handshake completes. which means one-RTT delay . three-way handshake is vulnerable to SYN flooding : client A sends large number of SYN packets to a server B; B must allocate resources for each SYN packet as if it is a legitimate connection request; then the server resources will be exhausted. SYN flooding can be done with spoofed IP address, or large number of real clients trying to connect. SCTP is an alternative to three-way handshake, to handle SYN flooding attacks, but it is not in TCP yet. routine for closing the connection: client A send a packet with FIN set, announcing that it has finished sending data. server B acknowledges the FIN packet with an ACK . B continues sending any left data to the client A. When B finishes sending data; it sends a FIN packet. client B acknowledges the A's FIN ; and this is the final packet. and the connection flow would be: If B had not been LISTENing at the port to which A sent its SYN, its response would have been RST (\u201creset\u201d), meaning in this context \u201cconnection refused\u201d. Similarly, if A sent data to B before the SYN packet, the response would have been RST. a RST can be sent by either side at any time to abort the connection. sometimes external attackers are able to tear down a TCP connection with a spoofed RST; this requires brute-force guessing the endpoint port numbers and the current SYN value In the days of 4kB**window sizes, guessing a valid SYN was a one-in-a-million chance, but window sizes have steadily increased; with **4MB window size makes SYN guessing quite feasible. If A sends a series of small packets to B, then B has the option of assembling them into a full-sized I/O buffer before releasing them to the receiving application. f A sets the PSH bit on each packet, then B should release each packet immediately to the receiving application. if A has sent to B large amount of data, that B is busy processing it; the application may want to abort the connection immediately (eg. pressing Ctrl-C in an ssh session ); this can be achieved by setting the URG bit(flag) , the TCP protocol then sends an interrupt command to the socket process. 12.5 TCP offloading \u00b6 TCP checksum offloading, TCO : used to have the network-interface card to do the actual checksum calculations; this permits a modest amount of parallelism. TCP segment offloading, TSO : hand the segmentation process to the LAN hardware , this requires TCO . TSO can be divided into: LSO: large send offloading: the host system transfers to the network card a large buffer of data (perhaps 64 kB), together with information about the headers. The network card then divides the buffer into 1500-byte packets, with proper TCP/IP headers, and sends them off. LRO: large receive offloading: the network card accumulates multiple inbound packets that are part of the same TCP connection, and consolidates them in proper sequence to one much larger packet. This means that the network card, upon receiving one packet, must wait to see if there will be more. This wait is very short, however, at most a few milliseconds. TSO is important in very high bandwidth systems. At 10 Gbps, a system can send or receive close to a million packets per second, and offloading some of the packet processing to the network card can be essential to maintaining high throughput. TSO allows a host system to behave as if it were reading or writing very large packets, and yet the actual packet size on the wire remains at the standard 1500 bytes. 12.7 TCP state diagram \u00b6 a ladder diagram: client stats: CLOSED > SYNC_SENT > ESTABLISHED > FIN_WAIT_1 > FIN_WAIT_2 > TIME_WAIT. server states: LISTEN > SYNC_RECT > ESTABLISHED > CLOSE_WAIT > LAST_ACK > CLOSED. 12.7.1 closing a connection \u00b6 the first party to send FIN follows active CLOSE path; the other side will enter passive CLOSE path. active CLOSE: A and B is ESTABLISHED. after sending the FIN, A enters FIN_WAIT_1 upon receiving the the B's ACK of FIN, A enters FIN_WAIT_2 waiting for data to be sent from B. after receiving the FIN of B; A enters TIME_WAIT . A sends ACK of B's FIN; and enters CLOSED passive CLOSE: A and B is ESTABLISHED. after receiving the A's FIN; B enters CLOSE_WAIT . B sends all of its data to A; B sends its FIN; and enters LAST_ACK where it waits for A's ACK. B enters CLOSED . A TCP endpoint is half-closed if it has sent its FIN (thus promising not to send any more data) and is waiting for the other side\u2019s FIN; this corresponds to A in the diagram above in states FIN_WAIT_1 and FIN_WAIT_2. A TCP endpoint is half-open if it is in the ESTABLISHED state, but during a lull in the exchange of packets the other side has rebooted; this has nothing to do with the close protocol. As soon as the ESTABLISHED side sends a packet, the rebooted side will respond with RST and the connection will be fully closed. A simultaneous close \u2013 having both sides send each other FINs before receiving the other side\u2019s FIN 12.7.2 calling close() \u00b6 Most network programming interfaces provide a close() method for ending a connection; it usually closes bidirectionally and so models the TCP closure protocol rather imperfectly. A\u2019s application calls shutdown(), thereby promising not to send any more data. A\u2019s FIN is sent to B. A\u2019s application is expected to continue reading, however. The connection is now half-closed .Onreceipt of A\u2019s FIN,B\u2019s TCP layer knows this. If B\u2019s application attempts to read more data, it will receive an end-of-file indication (this is typically a read() or recv() operation that returns immediately with 0 bytes received). B\u2019s application is now done reading data, but it may or may not have more data to send. When B\u2019s application is done sending, it calls close(), at which point B\u2019s FIN is sent to A. Because the connection is already half-closed, B\u2019s close() is really a second half-close, ending further transmission by B. A\u2019s application keeps reading until it too receives an end-of-file indication, corresponding to B\u2019s FIN. The connection is now fully closed. No data has been lost. 12.8 TCP old duplicates \u00b6 the most serious threat facing the integrity of TCP data is external old duplicates . very late packets from a previous instance of the connection. Suppose a TCP connection is opened between A and B. One packet from A to B is duplicated and unduly delayed, with sequence number N. The connection is closed, and then another instance is reopened, that is, a connection is created using the same ports. At some point in the second connection, when an arriving packet with seq=N would be acceptable at B, the old duplicate shows up. Later, of course, B is likely to receive a seq=N packet from the new instance of the connection, but that packet will be seen by B as a duplicate (even though the data does not match), and (we will assume) be ignored. solution: include connection count. TCP is also vulnerable to sequence-number wraparound: arrival of an old duplicates from the same instance of the connection. However, if we take the MSL to be 60 seconds, sequence-number wrap requires sending 232 bytes in 60 seconds, which requires a data-transfer rate in excess of 500 Mbps. TCP offers a fix for this (Protection Against Wrapped Segments, or PAWS), but it was introduced relatively late; 12.9 TIME_WAIT \u00b6 The TIMEWAIT state is entered by whichever side initiates the connection close; in the event of a simultaneous close , both sides enter TIMEWAIT. TIMEWAIT lasts for a time 2 * MSL, where MSL = Maximum Segment Lifetime is an agreed-upon value for the maximum lifetime on the Internet of an IP packet. TIMEWAIT was 60 seconds, but now is 30 seconds . functions of TIMEWAIT: to solve the external-old-duplicates problem. address lost final-ack problem TIMEWAIT requires that between closing and reopening a connection, a long enough interval must pass that any packets from the first instance will disappear. After the expiration of the TIMEWAIT interval, an old duplicate cannot arrive. If host A sends its final ACK to host B and this is lost, then B will eventually retransmit its final packet, which will be its FIN. As long as A remains in state TIMEWAIT, it can appropriately reply to a retransmitted FIN from B with a duplicate final ACK TIMEWAIT only blocks re-connections for which both sides reuse the same port they used before. If A connects to B and closes the connection, A is free to connect again to B using a different port at A\u2019s end. the host must thus maintain for each of its ports a list of all the remote sockets currently in TIMEWAIT for that port. If a host is connecting as a client, this list likely will amount to a list of recently used ports; no port is likely to have been used twice within the TIMEWAIT interval. If a host is a server, however, accepting connections on a standardized port, and happens to be the side that initiates the active close and thus later goes into TIMEWAIT, then its TIMEWAIT list for that port can grow quite long. Generally, busy servers prefer to be free from these bookkeeping requirements of TIMEWAIT, so many protocols are designed so that it is the client that initiates the active close . In an environment in which many short-lived connections are made from host A to the same port on server B, port exhaustion \u2013 having all ports tied up in TIMEWAIT \u2013 is a theoretical possibility. early Berkeley-Unix TCP implementations often made only about 4,000 ports available to clients; with a 120-second TIMEWAIT interval, port exhaustion would occur with only 33 connections per second. 12.10 three-way handshake revisited \u00b6 the original TCP specification, for the ISN to be determined by a special clock, incremented by 1 every 4 microseconds. ISN is used to detect duplicated packets. if A sends a SYN, restarts, and sends the SYN again as part of a reopening the same connection, the arrival of a second SYN with a new ISN means that the original connection cannot proceed, because that ISN is now wrong. The receiver of the duplicate SYN should drop any connection state it has recorded so far, and restart processing the second SYN from scratch . The clock-driven ISN also originally added a second layer of protection against external old duplicates. Suppose that A opens a connection to B, and chooses a clock-based ISN N1. A then transfers M bytes of data, closed the connection, and reopens it with ISN N2. If N1 + M < N2, then the old-duplicates problem cannot occur: all of the absolute sequence numbers used in the first instance of the connection are less than or equal to N1 + M, and all of the absolute sequence numbers used in the second instance will be greater than N2. In fact, early Berkeley-Unix implementations of the socket library often allowed a second connection meeting this ISN requirement to be reopened before TIMEWAIT would have expired; 12.10.1 ISN and spoofing \u00b6 The clock-based ISN proved to have a significant weakness: it often allowed an attacker to guess the ISN a remote host might use. early version of Berkeley Unix, instead of incrementing the ISN 250,000 times a second, incremented it once a second, by 250,000 (plus something for each connection). By guessing the ISN a remote host would choose, an attacker might be able to mimic a local, trusted host, and thus gain privileged access. The IP-spoofing technique was used in the 1994 Christmas Day attack against UCSD, launched from Loyola\u2019s own apollo.it.luc.edu; the attack was associated with Kevin Mitnick though apparently not actually carried out by him. Mitnick was arrested a few months later. in May 1996, introduced a technique for introducing a degree of randomization in ISN selection, that insures: the same ISN won't be used twice in a row for the same connection. ISN = C(t) + hash(local_addr, local_port, remote_addr, remote_port, key) where C(t) is 4-microseconds clock; key is a random value chosen by the host. RFC 5925 addresses spoofing and related attacks by introducing an optional TCP authentication mechanism: the TCP header includes an option containing a secure hash of the rest of the TCP header; and a shared secret key . 12.11 Anomalous TCP scenarios \u00b6 TCP addresses the Duplicate Connection Request (Duplicate SYN) issue by noting whether the ISN has changed. This is handled at the kernel level by TCP, versus TFTP\u2019s application-level (and rather desultory) approach to handing Duplicate RRQs. TCP addresses Loss of Final ACK through TIMEWAIT: as long as the TIMEWAIT period has not expired, if the final ACK is lost and the other side re-sends its final FIN, TCP will still be able to reissue that final ACK. TIMEWAIT in this sense serves a similar function to TFTP\u2019s DALLY state. External Old Duplicates, arriving as part of a previous instance of the connection, are prevented by TIME- WAIT, and may also be prevented by the use of a clock-driven ISN. Internal Old Duplicates, from the same instance of the connection, that is, sequence number wraparound, is only an issue for bandwidths exceeding 500 Mbps: only at bandwidths above that can 4 GB be sent in one 60-second MSL; solution is PAWS: Protection Against Wrapped Segments ; PAWS adds a 32-bit \u201ctimestamp option\u201d to the TCP header. The granularity of the timestamp clock is left unspecified; ne tick must be small enough that sequence numbers cannot wrap in that interval (eg less than 3 seconds for 10,000 Mbps), and large enough that the timestamps cannot wrap in time MSL. The PAWS mechanism also requires ACK packets to echo back the sender\u2019s timestamp, in addition to including their own. This allows senders to accurately measure round-trip times. Reboots are a potential problem as the host presumably has no record of what aborted connections need to remain in TIMEWAIT. TCP addresses this on paper by requiring hosts to implement Quiet Time on Startup: no new connections are to be accepted for 1*MSL. 12.12 TCP faster opening \u00b6 There have been periodic calls to allow TCP clients to include data with the first SYN packet and have it be delivered immediately upon arrival \u2013 this is known as accelerated open. If there will be a series of requests and replies, the simplest fix is to pipeline all the requests and replies over one persistent connection; the one-RTT delay then applies only to the first request. If the pipeline connection is idle for a long-enough interval, it may be closed, and then reopened later if necessary. T/TCP, or TCP for Transactions, : introduced a connection count TCP option, called CC; each participant would include a 32-bit CC value in its SYN; each participant\u2019s own CC values were to be monotonically increasing. Accelerated open was allowed if the server side had the client\u2019s previous CC in a cache, and the new CC value was strictly greater than this cached value. This ensured that the new SYN was not a duplicate of an older SYN. The recent TCP Fast Open proposal , described in RFC 7413, involves a secure \u201ccookie\u201d sent by the client as a TCP option; if a SYN+Data packet has a valid cookie, then the client has proven its identity and the data may be released immediately to the receiving application. Cookies are cryptographically secure, and are requested ahead of time from the server. Because cookies have an expiration date and must be requested ahead of time, TCP Fast Open is not fundamentally faster from the connection-pipeline option, except that holding a TCP connection open uses more resources than simply storing a cookie. The likely application for TCP Fast Open is in accessing web servers. Web clients and servers already keep a persistent connection open for a while, but often \u201ca while\u201d here amounts only to several seconds; TCP Fast Open cookies could remain active for much longer. 12.13 Path MTU discovery \u00b6 CP connections are more efficient if they can keep large packets flowing between the endpoints. Once upon a time, TCP endpoints included just 512 bytes of data in each packet that was not destined for local delivery, to avoid fragmentation. TCP endpoints now typically engage in Path MTU Discovery which almost always allows them to send larger packets; backbone ISPs are now usually able to carry 1500-byte packets . The Path MTU is the largest packet size that can be sent along a path without fragmentation . The IPv4 strategy is to send an initial data packet with the IPv4 DONT_FRAG bit set. If the ICMP message Frag_Required/DONT_FRAG_Set comes back, or if the packet times out, the sender tries a smaller size. If the sender receives a TCP ACK for the packet, t might try a larger size. Usually, the size range of 512-1500 bytes is covered by less than a dozen discrete values; the point is not to find the exact Path MTU but to determine a reasonable approximation rapidly. IPv6 has no DONT_FRAG bit. Path MTU Discovery over IPv6 involves the periodic sending of larger packets; if the ICMPv6 message Packet Too Big is received,a smaller packet size must be used. RFC 1981 has details. 12.14 TCP sliding windows \u00b6 Window sizes are measured in terms of bytes rather than packets; this leaves TCP free to packetize the data in whatever segment size it elects. In the initial three-way handshake, each side specifies the maximum window size it is willing to accept, in the Window Size field of the TCP header. This 16-bit field can only go to 64 kB, and a 1 Gbps * 100 ms bandwidth * delay product is 12 MB ; as a result, there is a TCP Window Scale option that can also be negotiated in the opening handshake. The scale option specifies a power of 2 that is to be multiplied by the actual Window Size value. TCP may either transmit a bulk stream of data, using sliding windows fully, or it may send slowly generated interactive data; in the latter case, TCP may never have even one full segment outstanding. the window size included in the TCP header is known as the Advertised Window Size . On startup, TCP does not send a full window all at once; it uses a mechanism called \u201cslow start\u201d . 12.15 TCP delayed ACKs \u00b6 TCP receivers are allowed briefly to delay their ACK responses to new data. This offers perhaps the most benefit for interactive applications that exchange small packets, such as ssh and telnet. If A sends a data packet to B and expects an immediate response, delaying B\u2019s ACK allows the receiving application on B time to wake up and generate that application-level response, which can then be sent together with B\u2019s ACK. Without delayed ACKs, the kernel layer on B may send its ACK before the receiving application on B has even been scheduled to run. If response packets are small, that doubles the total traffic. The maximum ACK delay is 500 ms, according to RFC 1122 and RFC 2581. acknowledging too many data packets with one ACK can interfere with the self-clocking aspect of sliding windows; the arrival of that ACK will then trigger a burst of additional data packets, which would otherwise have been transmitted at regular intervals. ACK be sent, at a minimum, for every other data packet . 12.16 Nagle Algorithm \u00b6 attempts to improve the behavior of interactive small-packet applications. It specifies that a TCP endpoint generating small data segments should queue them until either it accumulates a full segment\u2019s worth or receives an ACK for the previous batch of small segments. If the full-segment threshold is not reached, this means that only one (consolidated) segment will be sent per RTT. As an example, suppose A wishes to send to B packets containing consecutive letters, starting with \u201ca\u201d. The application on A generates these every 100 ms, but the RTT is 501 ms. At T=0, A transmits \u201ca\u201d. The application on A continues to generate \u201cb\u201d, \u201cc\u201d, \u201cd\u201d, \u201ce\u201d and \u201cf\u201d at times 100 ms through 500 ms, but A does not send them immediately. At T=501 ms, ACK(\u201ca\u201d) arrives; at this point A transmits its backlogged \u201cbcdef\u201d. The ACK for this arrives at T=1002, by which point A has queued \u201cghijk\u201d. The end result is that A sends a fifth as many packets as it would without the Nagle algorithm. If these letters are generated by a user typing them with telnet, and the ACKs also include the echoed responses, then if the user pauses the echoed responses will very soon catch up. 12.17 TCP flow control \u00b6 It is possible for a TCP sender to send data faster than the receiver can process it. When this happens, a TCP receiver may reduce the advertised Window Size value of an open connection, thus informing the sender to switch to a smaller window size. This provides support for flow control. The window-size reduction appears in the ACKs sent back by the receiver. A given ACK is not supposed to reduce the window size by so much that the upper end of the window gets smaller. A window might shrink from the byte range [20,000..28,000] to [22,000..28,000] but never to [20,000..26,000]. If a TCP receiver uses this technique to shrink the advertised window size to 0, this means that the sender may not send data. The receiver has thus informed the sender that, yes, the data was received, but that, no, more may not yet be sent. This corresponds to the ACK WAIT Eventually, when the receiver is ready to receive data, it will send an ACK increasing the advertised window size again. 12.18 silly window syndrome \u00b6 TCP transfers only small amounts of data at a time. Because TCP/IP packets have a minimum fixed header size of 40 bytes, sending small packets uses the network inefficiently. The silly-window syndrome can occur when either by the receiving application consuming data slowly or when the sending application generating data slowly. As an example involving a slow-consuming receiver, suppose a TCP connection has a window size of 1000 bytes, but the receiving application consumes data only 10 bytes at a time, at intervals about equal to the RTT. The following can then happen: The sender sends bytes 1-1000. The receiving application consumes 10 bytes, numbered 1-10. The receiving TCP buffers the remaining 990 bytes and sends an ACK reducing the window size to 10. Upon receipt of the ACK, the sender sends 10 bytes numbered 1001-1010, the most it is permitted. In the meantime, the receiving application has consumed bytes 11-20. The window size therefore remains at 10 in the next ACK. the sender sends bytes 1011-1020 while the application consumes bytes 21-30. The window size remains at 10. The sender may end up sending 10 bytes at a time indefinitely. This is of no benefit to either side; the sender might as well send larger packets less often. The standard fix, set forth in RFC 1122, is for the receiver to use its ACKs to keep the window at 0 until it has consumed one full packet\u2019s worth (or half the window, for small window sizes). At that point the sender is invited \u2013 by an appropriate window-size advertisement in the next ACK \u2013 to send another full packet of data. The silly-window syndrome can also occur if the sender is generating data slowly, say 10 bytes at a time. The Nagle algorithm, above, can be used to prevent this, though for interactive applications sending small amounts of data in separate but closely spaced packets may actually be useful. 12.19 TCP timeouts and retransmits \u00b6 When TCP sends a packet containing user data (this excludes ACK-only packets), it sets a timeout. If that timeout expires before the packet data is acknowledged, it is retransmitted. Acknowledgments are sent for every arriving data packet (unless Delayed ACKs are implemented). this amounts to receiver-side retransmit-on-duplicate. Because ACKs are cumulative, and so a later ACK can replace an earlier one, lost ACKs are seldom a problem. For TCP to work well for both intra-server-room and trans-global connections, with RTTs ranging from well under 1 ms to close to 1 second, the length of the timeout interval must adapt. TCP manages this by maintaining a running estimate of the RTT, EstRTT. In the original version, TCP then set TimeOut = 2\u02c6EstRTT (in the literature, the TCP TimeOut value is often known as RTO, for Retransmission TimeOut). EstRTT itself was a running average of periodically measured SampleRTT values, according to EstRTT = \ud835\udefc\u02c6EstRTT + (1-\ud835\udefc)\u02c6SampleRTT 12.20 KeepAlive \u00b6 There is no reason that a TCP connection should not be idle for a long period of time; TCP supports an optional KeepAlive mechanism: each side \u201cpolls\u201d the other with a data-less packet. KeepAlive timeout was 2 hours, but this could be reduced to 15 minutes. If a connection failed the KeepAlive test, it would be closed. 12.21 TCP timers \u00b6 TimeOut : a per-segment timer; TimeOut values vary widely 2 *MSL TIMEWAIT : a per-connection timer Persist : the timer used to poll the receiving end when winsize = 0 KeepAlive;","title":"CS2204: Unit7: The Transport Layer (TCP)"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#cs2204-unit7-the-transport-layer-tcp","text":"standard transport protocols riding above IP layer, are TCP,UDP . UDP provides simple datagram delivery to remote sockets, that is, to pairs. TCP provides a much richer functionality for sending data to (connected) sockets. TCP is stream-oriented : application can write data in small or large chunks and TCP layer will do the packetization . TCP transmit stream of bytes, not messages or records. TCP is connection-oriented . TCP is reliable : TCP uses sequence numbers to ensure the correct order of delivery and a timeout/retransmission mechanism to make sure no data is lost short of massive network failure. TCP uses sliding windows by default, to achieve throughput relatively close to the maximum available. TCP works fine for: transferring large files. interactive applications, each end is sending and receive streams of small packets. ssh, telnet, db connections as examples of interactive applications. request/reply protocols: sender sends one request and receives replies, then the connection is closed. not desirable due to the overhead of creating the connection. if UDP opens a socket, then any client on the internet can transmit to that socket; so the UDP application needs to have mechanism to check that the socket is receiving messages from the right client for every packet . with TCP all data coming into the connected socket must be sent form the legitimate client who opened the socket. when the socket is not connected: it is ine LISTENING state. to transmit to a TCP, client must open a connection defined by the socketPair as on both ends of the connection. for this connection a new child socket S c is created, that includes creating a new thread or process .","title":"CS2204: Unit7: The Transport Layer (TCP)"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#121-end-to-end-principle","text":"transport issues are the responsibility of the endpoints in question, and not the core network. this addresses: data corruption, and congestion . data corruption: all links on the internet have link-layer-checksums to protect against corruption; but TCP still have to add its own checksum . TCP is the only layer has congestion management . 12.2 TCP Header source and destination ports are 16-bits 4-bits data offset specifies the number of 32-bits words in the header. default value is 5 . checksum covers: TCP header TCP data IP pseudo header that includes source and destination IP addresses. the checksum must be updated by a NAT router that modifies any header values. The sequence and acknowledgment numbers are for numbering the data, at the byte level. TCP can send 1024-byte blocks of data, incrementing the sequence number by 1024. TCP can also send 1-byte telnet packets, incrementing the sequence number by 1. There is no distinguish between the data and acknowledgements packets : all packets from A to B are sending the most current acknowledgment from B to A. and vice versa. packets in TCP are called segments sequence numbers represent the position of the first byte of data. the acknowledgment number is sequenceNumber + number of data bytes + 1 which represents the position of the first byte of data in the next segment. The sequence and acknowledgment numbers, as sent, represent these relative values plus an Initial Sequence Number , or ISN , that is fixed for the lifetime of the connection . Each direction of a connection has its own ISN; TCP acknowledgments are cumulative : when an endpoint sends a packet with an acknowledgment number of N, it is acknowledging receipt of all data bytes numbered less than N. TCP header flags : SYN : for SYNchronize; marks packets that are part of the new-connection handshake. ACK : indicates that the header Acknowledgment field is valid; that is, all but the first packet. FIN : for FINish; marks packets involved in the connection closing. PSH : for PuSH; marks \u201cnon-full\u201d packets that should be delivered promptly at the far end. RST : for ReSeT; indicates various error conditions. URG : for URGent; part of a now-seldom-used mechanism for high-priority data CWR and ECE : part of the Explicit Congestion Notification mechanism.","title":"12.1 End-to-End principle"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#123-tcp-connection-establishment","text":"established with three-way handshake that involves three steps: C send a packet with SYN bit set to the S. S responds a SYN packet of its own; ACK flag is set. C acknowledges the S's SYN with its own ACK . the three-way handshake is triggered by client; and any exchange of data should happen after the handshake completes. which means one-RTT delay . three-way handshake is vulnerable to SYN flooding : client A sends large number of SYN packets to a server B; B must allocate resources for each SYN packet as if it is a legitimate connection request; then the server resources will be exhausted. SYN flooding can be done with spoofed IP address, or large number of real clients trying to connect. SCTP is an alternative to three-way handshake, to handle SYN flooding attacks, but it is not in TCP yet. routine for closing the connection: client A send a packet with FIN set, announcing that it has finished sending data. server B acknowledges the FIN packet with an ACK . B continues sending any left data to the client A. When B finishes sending data; it sends a FIN packet. client B acknowledges the A's FIN ; and this is the final packet. and the connection flow would be: If B had not been LISTENing at the port to which A sent its SYN, its response would have been RST (\u201creset\u201d), meaning in this context \u201cconnection refused\u201d. Similarly, if A sent data to B before the SYN packet, the response would have been RST. a RST can be sent by either side at any time to abort the connection. sometimes external attackers are able to tear down a TCP connection with a spoofed RST; this requires brute-force guessing the endpoint port numbers and the current SYN value In the days of 4kB**window sizes, guessing a valid SYN was a one-in-a-million chance, but window sizes have steadily increased; with **4MB window size makes SYN guessing quite feasible. If A sends a series of small packets to B, then B has the option of assembling them into a full-sized I/O buffer before releasing them to the receiving application. f A sets the PSH bit on each packet, then B should release each packet immediately to the receiving application. if A has sent to B large amount of data, that B is busy processing it; the application may want to abort the connection immediately (eg. pressing Ctrl-C in an ssh session ); this can be achieved by setting the URG bit(flag) , the TCP protocol then sends an interrupt command to the socket process.","title":"12.3 TCP connection establishment."},{"location":"knowledge-base/cs2204-netwroks1/unit7/#125-tcp-offloading","text":"TCP checksum offloading, TCO : used to have the network-interface card to do the actual checksum calculations; this permits a modest amount of parallelism. TCP segment offloading, TSO : hand the segmentation process to the LAN hardware , this requires TCO . TSO can be divided into: LSO: large send offloading: the host system transfers to the network card a large buffer of data (perhaps 64 kB), together with information about the headers. The network card then divides the buffer into 1500-byte packets, with proper TCP/IP headers, and sends them off. LRO: large receive offloading: the network card accumulates multiple inbound packets that are part of the same TCP connection, and consolidates them in proper sequence to one much larger packet. This means that the network card, upon receiving one packet, must wait to see if there will be more. This wait is very short, however, at most a few milliseconds. TSO is important in very high bandwidth systems. At 10 Gbps, a system can send or receive close to a million packets per second, and offloading some of the packet processing to the network card can be essential to maintaining high throughput. TSO allows a host system to behave as if it were reading or writing very large packets, and yet the actual packet size on the wire remains at the standard 1500 bytes.","title":"12.5 TCP offloading"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#127-tcp-state-diagram","text":"a ladder diagram: client stats: CLOSED > SYNC_SENT > ESTABLISHED > FIN_WAIT_1 > FIN_WAIT_2 > TIME_WAIT. server states: LISTEN > SYNC_RECT > ESTABLISHED > CLOSE_WAIT > LAST_ACK > CLOSED.","title":"12.7 TCP state diagram"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1271-closing-a-connection","text":"the first party to send FIN follows active CLOSE path; the other side will enter passive CLOSE path. active CLOSE: A and B is ESTABLISHED. after sending the FIN, A enters FIN_WAIT_1 upon receiving the the B's ACK of FIN, A enters FIN_WAIT_2 waiting for data to be sent from B. after receiving the FIN of B; A enters TIME_WAIT . A sends ACK of B's FIN; and enters CLOSED passive CLOSE: A and B is ESTABLISHED. after receiving the A's FIN; B enters CLOSE_WAIT . B sends all of its data to A; B sends its FIN; and enters LAST_ACK where it waits for A's ACK. B enters CLOSED . A TCP endpoint is half-closed if it has sent its FIN (thus promising not to send any more data) and is waiting for the other side\u2019s FIN; this corresponds to A in the diagram above in states FIN_WAIT_1 and FIN_WAIT_2. A TCP endpoint is half-open if it is in the ESTABLISHED state, but during a lull in the exchange of packets the other side has rebooted; this has nothing to do with the close protocol. As soon as the ESTABLISHED side sends a packet, the rebooted side will respond with RST and the connection will be fully closed. A simultaneous close \u2013 having both sides send each other FINs before receiving the other side\u2019s FIN","title":"12.7.1 closing a connection"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1272-calling-close","text":"Most network programming interfaces provide a close() method for ending a connection; it usually closes bidirectionally and so models the TCP closure protocol rather imperfectly. A\u2019s application calls shutdown(), thereby promising not to send any more data. A\u2019s FIN is sent to B. A\u2019s application is expected to continue reading, however. The connection is now half-closed .Onreceipt of A\u2019s FIN,B\u2019s TCP layer knows this. If B\u2019s application attempts to read more data, it will receive an end-of-file indication (this is typically a read() or recv() operation that returns immediately with 0 bytes received). B\u2019s application is now done reading data, but it may or may not have more data to send. When B\u2019s application is done sending, it calls close(), at which point B\u2019s FIN is sent to A. Because the connection is already half-closed, B\u2019s close() is really a second half-close, ending further transmission by B. A\u2019s application keeps reading until it too receives an end-of-file indication, corresponding to B\u2019s FIN. The connection is now fully closed. No data has been lost.","title":"12.7.2 calling close()"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#128-tcp-old-duplicates","text":"the most serious threat facing the integrity of TCP data is external old duplicates . very late packets from a previous instance of the connection. Suppose a TCP connection is opened between A and B. One packet from A to B is duplicated and unduly delayed, with sequence number N. The connection is closed, and then another instance is reopened, that is, a connection is created using the same ports. At some point in the second connection, when an arriving packet with seq=N would be acceptable at B, the old duplicate shows up. Later, of course, B is likely to receive a seq=N packet from the new instance of the connection, but that packet will be seen by B as a duplicate (even though the data does not match), and (we will assume) be ignored. solution: include connection count. TCP is also vulnerable to sequence-number wraparound: arrival of an old duplicates from the same instance of the connection. However, if we take the MSL to be 60 seconds, sequence-number wrap requires sending 232 bytes in 60 seconds, which requires a data-transfer rate in excess of 500 Mbps. TCP offers a fix for this (Protection Against Wrapped Segments, or PAWS), but it was introduced relatively late;","title":"12.8 TCP old duplicates"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#129-time_wait","text":"The TIMEWAIT state is entered by whichever side initiates the connection close; in the event of a simultaneous close , both sides enter TIMEWAIT. TIMEWAIT lasts for a time 2 * MSL, where MSL = Maximum Segment Lifetime is an agreed-upon value for the maximum lifetime on the Internet of an IP packet. TIMEWAIT was 60 seconds, but now is 30 seconds . functions of TIMEWAIT: to solve the external-old-duplicates problem. address lost final-ack problem TIMEWAIT requires that between closing and reopening a connection, a long enough interval must pass that any packets from the first instance will disappear. After the expiration of the TIMEWAIT interval, an old duplicate cannot arrive. If host A sends its final ACK to host B and this is lost, then B will eventually retransmit its final packet, which will be its FIN. As long as A remains in state TIMEWAIT, it can appropriately reply to a retransmitted FIN from B with a duplicate final ACK TIMEWAIT only blocks re-connections for which both sides reuse the same port they used before. If A connects to B and closes the connection, A is free to connect again to B using a different port at A\u2019s end. the host must thus maintain for each of its ports a list of all the remote sockets currently in TIMEWAIT for that port. If a host is connecting as a client, this list likely will amount to a list of recently used ports; no port is likely to have been used twice within the TIMEWAIT interval. If a host is a server, however, accepting connections on a standardized port, and happens to be the side that initiates the active close and thus later goes into TIMEWAIT, then its TIMEWAIT list for that port can grow quite long. Generally, busy servers prefer to be free from these bookkeeping requirements of TIMEWAIT, so many protocols are designed so that it is the client that initiates the active close . In an environment in which many short-lived connections are made from host A to the same port on server B, port exhaustion \u2013 having all ports tied up in TIMEWAIT \u2013 is a theoretical possibility. early Berkeley-Unix TCP implementations often made only about 4,000 ports available to clients; with a 120-second TIMEWAIT interval, port exhaustion would occur with only 33 connections per second.","title":"12.9 TIME_WAIT"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1210-three-way-handshake-revisited","text":"the original TCP specification, for the ISN to be determined by a special clock, incremented by 1 every 4 microseconds. ISN is used to detect duplicated packets. if A sends a SYN, restarts, and sends the SYN again as part of a reopening the same connection, the arrival of a second SYN with a new ISN means that the original connection cannot proceed, because that ISN is now wrong. The receiver of the duplicate SYN should drop any connection state it has recorded so far, and restart processing the second SYN from scratch . The clock-driven ISN also originally added a second layer of protection against external old duplicates. Suppose that A opens a connection to B, and chooses a clock-based ISN N1. A then transfers M bytes of data, closed the connection, and reopens it with ISN N2. If N1 + M < N2, then the old-duplicates problem cannot occur: all of the absolute sequence numbers used in the first instance of the connection are less than or equal to N1 + M, and all of the absolute sequence numbers used in the second instance will be greater than N2. In fact, early Berkeley-Unix implementations of the socket library often allowed a second connection meeting this ISN requirement to be reopened before TIMEWAIT would have expired;","title":"12.10 three-way handshake revisited"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#12101-isn-and-spoofing","text":"The clock-based ISN proved to have a significant weakness: it often allowed an attacker to guess the ISN a remote host might use. early version of Berkeley Unix, instead of incrementing the ISN 250,000 times a second, incremented it once a second, by 250,000 (plus something for each connection). By guessing the ISN a remote host would choose, an attacker might be able to mimic a local, trusted host, and thus gain privileged access. The IP-spoofing technique was used in the 1994 Christmas Day attack against UCSD, launched from Loyola\u2019s own apollo.it.luc.edu; the attack was associated with Kevin Mitnick though apparently not actually carried out by him. Mitnick was arrested a few months later. in May 1996, introduced a technique for introducing a degree of randomization in ISN selection, that insures: the same ISN won't be used twice in a row for the same connection. ISN = C(t) + hash(local_addr, local_port, remote_addr, remote_port, key) where C(t) is 4-microseconds clock; key is a random value chosen by the host. RFC 5925 addresses spoofing and related attacks by introducing an optional TCP authentication mechanism: the TCP header includes an option containing a secure hash of the rest of the TCP header; and a shared secret key .","title":"12.10.1 ISN and spoofing"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1211-anomalous-tcp-scenarios","text":"TCP addresses the Duplicate Connection Request (Duplicate SYN) issue by noting whether the ISN has changed. This is handled at the kernel level by TCP, versus TFTP\u2019s application-level (and rather desultory) approach to handing Duplicate RRQs. TCP addresses Loss of Final ACK through TIMEWAIT: as long as the TIMEWAIT period has not expired, if the final ACK is lost and the other side re-sends its final FIN, TCP will still be able to reissue that final ACK. TIMEWAIT in this sense serves a similar function to TFTP\u2019s DALLY state. External Old Duplicates, arriving as part of a previous instance of the connection, are prevented by TIME- WAIT, and may also be prevented by the use of a clock-driven ISN. Internal Old Duplicates, from the same instance of the connection, that is, sequence number wraparound, is only an issue for bandwidths exceeding 500 Mbps: only at bandwidths above that can 4 GB be sent in one 60-second MSL; solution is PAWS: Protection Against Wrapped Segments ; PAWS adds a 32-bit \u201ctimestamp option\u201d to the TCP header. The granularity of the timestamp clock is left unspecified; ne tick must be small enough that sequence numbers cannot wrap in that interval (eg less than 3 seconds for 10,000 Mbps), and large enough that the timestamps cannot wrap in time MSL. The PAWS mechanism also requires ACK packets to echo back the sender\u2019s timestamp, in addition to including their own. This allows senders to accurately measure round-trip times. Reboots are a potential problem as the host presumably has no record of what aborted connections need to remain in TIMEWAIT. TCP addresses this on paper by requiring hosts to implement Quiet Time on Startup: no new connections are to be accepted for 1*MSL.","title":"12.11 Anomalous TCP scenarios"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1212-tcp-faster-opening","text":"There have been periodic calls to allow TCP clients to include data with the first SYN packet and have it be delivered immediately upon arrival \u2013 this is known as accelerated open. If there will be a series of requests and replies, the simplest fix is to pipeline all the requests and replies over one persistent connection; the one-RTT delay then applies only to the first request. If the pipeline connection is idle for a long-enough interval, it may be closed, and then reopened later if necessary. T/TCP, or TCP for Transactions, : introduced a connection count TCP option, called CC; each participant would include a 32-bit CC value in its SYN; each participant\u2019s own CC values were to be monotonically increasing. Accelerated open was allowed if the server side had the client\u2019s previous CC in a cache, and the new CC value was strictly greater than this cached value. This ensured that the new SYN was not a duplicate of an older SYN. The recent TCP Fast Open proposal , described in RFC 7413, involves a secure \u201ccookie\u201d sent by the client as a TCP option; if a SYN+Data packet has a valid cookie, then the client has proven its identity and the data may be released immediately to the receiving application. Cookies are cryptographically secure, and are requested ahead of time from the server. Because cookies have an expiration date and must be requested ahead of time, TCP Fast Open is not fundamentally faster from the connection-pipeline option, except that holding a TCP connection open uses more resources than simply storing a cookie. The likely application for TCP Fast Open is in accessing web servers. Web clients and servers already keep a persistent connection open for a while, but often \u201ca while\u201d here amounts only to several seconds; TCP Fast Open cookies could remain active for much longer.","title":"12.12 TCP faster opening"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1213-path-mtu-discovery","text":"CP connections are more efficient if they can keep large packets flowing between the endpoints. Once upon a time, TCP endpoints included just 512 bytes of data in each packet that was not destined for local delivery, to avoid fragmentation. TCP endpoints now typically engage in Path MTU Discovery which almost always allows them to send larger packets; backbone ISPs are now usually able to carry 1500-byte packets . The Path MTU is the largest packet size that can be sent along a path without fragmentation . The IPv4 strategy is to send an initial data packet with the IPv4 DONT_FRAG bit set. If the ICMP message Frag_Required/DONT_FRAG_Set comes back, or if the packet times out, the sender tries a smaller size. If the sender receives a TCP ACK for the packet, t might try a larger size. Usually, the size range of 512-1500 bytes is covered by less than a dozen discrete values; the point is not to find the exact Path MTU but to determine a reasonable approximation rapidly. IPv6 has no DONT_FRAG bit. Path MTU Discovery over IPv6 involves the periodic sending of larger packets; if the ICMPv6 message Packet Too Big is received,a smaller packet size must be used. RFC 1981 has details.","title":"12.13 Path MTU discovery"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1214-tcp-sliding-windows","text":"Window sizes are measured in terms of bytes rather than packets; this leaves TCP free to packetize the data in whatever segment size it elects. In the initial three-way handshake, each side specifies the maximum window size it is willing to accept, in the Window Size field of the TCP header. This 16-bit field can only go to 64 kB, and a 1 Gbps * 100 ms bandwidth * delay product is 12 MB ; as a result, there is a TCP Window Scale option that can also be negotiated in the opening handshake. The scale option specifies a power of 2 that is to be multiplied by the actual Window Size value. TCP may either transmit a bulk stream of data, using sliding windows fully, or it may send slowly generated interactive data; in the latter case, TCP may never have even one full segment outstanding. the window size included in the TCP header is known as the Advertised Window Size . On startup, TCP does not send a full window all at once; it uses a mechanism called \u201cslow start\u201d .","title":"12.14 TCP sliding windows"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1215-tcp-delayed-acks","text":"TCP receivers are allowed briefly to delay their ACK responses to new data. This offers perhaps the most benefit for interactive applications that exchange small packets, such as ssh and telnet. If A sends a data packet to B and expects an immediate response, delaying B\u2019s ACK allows the receiving application on B time to wake up and generate that application-level response, which can then be sent together with B\u2019s ACK. Without delayed ACKs, the kernel layer on B may send its ACK before the receiving application on B has even been scheduled to run. If response packets are small, that doubles the total traffic. The maximum ACK delay is 500 ms, according to RFC 1122 and RFC 2581. acknowledging too many data packets with one ACK can interfere with the self-clocking aspect of sliding windows; the arrival of that ACK will then trigger a burst of additional data packets, which would otherwise have been transmitted at regular intervals. ACK be sent, at a minimum, for every other data packet .","title":"12.15 TCP delayed ACKs"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1216-nagle-algorithm","text":"attempts to improve the behavior of interactive small-packet applications. It specifies that a TCP endpoint generating small data segments should queue them until either it accumulates a full segment\u2019s worth or receives an ACK for the previous batch of small segments. If the full-segment threshold is not reached, this means that only one (consolidated) segment will be sent per RTT. As an example, suppose A wishes to send to B packets containing consecutive letters, starting with \u201ca\u201d. The application on A generates these every 100 ms, but the RTT is 501 ms. At T=0, A transmits \u201ca\u201d. The application on A continues to generate \u201cb\u201d, \u201cc\u201d, \u201cd\u201d, \u201ce\u201d and \u201cf\u201d at times 100 ms through 500 ms, but A does not send them immediately. At T=501 ms, ACK(\u201ca\u201d) arrives; at this point A transmits its backlogged \u201cbcdef\u201d. The ACK for this arrives at T=1002, by which point A has queued \u201cghijk\u201d. The end result is that A sends a fifth as many packets as it would without the Nagle algorithm. If these letters are generated by a user typing them with telnet, and the ACKs also include the echoed responses, then if the user pauses the echoed responses will very soon catch up.","title":"12.16 Nagle Algorithm"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1217-tcp-flow-control","text":"It is possible for a TCP sender to send data faster than the receiver can process it. When this happens, a TCP receiver may reduce the advertised Window Size value of an open connection, thus informing the sender to switch to a smaller window size. This provides support for flow control. The window-size reduction appears in the ACKs sent back by the receiver. A given ACK is not supposed to reduce the window size by so much that the upper end of the window gets smaller. A window might shrink from the byte range [20,000..28,000] to [22,000..28,000] but never to [20,000..26,000]. If a TCP receiver uses this technique to shrink the advertised window size to 0, this means that the sender may not send data. The receiver has thus informed the sender that, yes, the data was received, but that, no, more may not yet be sent. This corresponds to the ACK WAIT Eventually, when the receiver is ready to receive data, it will send an ACK increasing the advertised window size again.","title":"12.17 TCP flow control"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1218-silly-window-syndrome","text":"TCP transfers only small amounts of data at a time. Because TCP/IP packets have a minimum fixed header size of 40 bytes, sending small packets uses the network inefficiently. The silly-window syndrome can occur when either by the receiving application consuming data slowly or when the sending application generating data slowly. As an example involving a slow-consuming receiver, suppose a TCP connection has a window size of 1000 bytes, but the receiving application consumes data only 10 bytes at a time, at intervals about equal to the RTT. The following can then happen: The sender sends bytes 1-1000. The receiving application consumes 10 bytes, numbered 1-10. The receiving TCP buffers the remaining 990 bytes and sends an ACK reducing the window size to 10. Upon receipt of the ACK, the sender sends 10 bytes numbered 1001-1010, the most it is permitted. In the meantime, the receiving application has consumed bytes 11-20. The window size therefore remains at 10 in the next ACK. the sender sends bytes 1011-1020 while the application consumes bytes 21-30. The window size remains at 10. The sender may end up sending 10 bytes at a time indefinitely. This is of no benefit to either side; the sender might as well send larger packets less often. The standard fix, set forth in RFC 1122, is for the receiver to use its ACKs to keep the window at 0 until it has consumed one full packet\u2019s worth (or half the window, for small window sizes). At that point the sender is invited \u2013 by an appropriate window-size advertisement in the next ACK \u2013 to send another full packet of data. The silly-window syndrome can also occur if the sender is generating data slowly, say 10 bytes at a time. The Nagle algorithm, above, can be used to prevent this, though for interactive applications sending small amounts of data in separate but closely spaced packets may actually be useful.","title":"12.18 silly window syndrome"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1219-tcp-timeouts-and-retransmits","text":"When TCP sends a packet containing user data (this excludes ACK-only packets), it sets a timeout. If that timeout expires before the packet data is acknowledged, it is retransmitted. Acknowledgments are sent for every arriving data packet (unless Delayed ACKs are implemented). this amounts to receiver-side retransmit-on-duplicate. Because ACKs are cumulative, and so a later ACK can replace an earlier one, lost ACKs are seldom a problem. For TCP to work well for both intra-server-room and trans-global connections, with RTTs ranging from well under 1 ms to close to 1 second, the length of the timeout interval must adapt. TCP manages this by maintaining a running estimate of the RTT, EstRTT. In the original version, TCP then set TimeOut = 2\u02c6EstRTT (in the literature, the TCP TimeOut value is often known as RTO, for Retransmission TimeOut). EstRTT itself was a running average of periodically measured SampleRTT values, according to EstRTT = \ud835\udefc\u02c6EstRTT + (1-\ud835\udefc)\u02c6SampleRTT","title":"12.19 TCP timeouts and retransmits"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1220-keepalive","text":"There is no reason that a TCP connection should not be idle for a long period of time; TCP supports an optional KeepAlive mechanism: each side \u201cpolls\u201d the other with a data-less packet. KeepAlive timeout was 2 hours, but this could be reduced to 15 minutes. If a connection failed the KeepAlive test, it would be closed.","title":"12.20 KeepAlive"},{"location":"knowledge-base/cs2204-netwroks1/unit7/#1221-tcp-timers","text":"TimeOut : a per-segment timer; TimeOut values vary widely 2 *MSL TIMEWAIT : a per-connection timer Persist : the timer used to poll the receiving end when winsize = 0 KeepAlive;","title":"12.21 TCP timers"},{"location":"knowledge-base/cs2204-netwroks1/unit7/da7/","text":"cs2204: unit 7: Discussion Assignment \u00b6 Simultaneous TCP connection initiations are rare, but simultaneous connection termination is relatively common. How do two TCP nodes negotiate the simultaneous sending of FIN packets to one another? Draw the ladder diagram, and label the states on each side. Which node goes into TIMEWAIT state? let's start by explaining the normal circumstances; in which the host who wishes to close the connection sends its FIN packet and enters the active close path; while the other end will enter the passive close path. the active close path, starts from ESTABLISHED state; then this host sends its FIN packet, call it FIN1, and enters FIN_WAIT_1 ; upon receiving the other end's ACK of FIN1, this host enters the FIN_WAIT_2 state where it waits for any data that is left to be sent from the other end; once the other end finishes sending data it sends its own FIN, call it FIN2 , then this host enters the TIMEWAIT state; after that, it will send the ACK of FIN2 and enters the CLOSED state. the other host starts the closing process by receiving the FIN1 signal where its state changes to CLOSE_WAIT ; then it sends all the data to the other end; the sends its own FIN, we called it FIN2, and enters the LAST_ACK state where it waits for the ACK of FIN2; and then enters the CLOSED state. so in the normal circumstances, the host who wishes to end the connection enters the TIMEWAIT state; but the newer versions of TCP, requires the client to always initialize the closing routine, since this will save the server from having to save the connection connection data, to make sure the same client can not use the same port very soon. References \u00b6 Dordal, P. (2019). An introduction to computer networks. https://eng.libretexts.org/Bookshelves/Computer_Science/Networks/Book%3A_An_Introduction_to_Computer_Networks_(Dordal)/07%3A_IP_version_4","title":"cs2204: unit 7: Discussion Assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit7/da7/#cs2204-unit-7-discussion-assignment","text":"Simultaneous TCP connection initiations are rare, but simultaneous connection termination is relatively common. How do two TCP nodes negotiate the simultaneous sending of FIN packets to one another? Draw the ladder diagram, and label the states on each side. Which node goes into TIMEWAIT state? let's start by explaining the normal circumstances; in which the host who wishes to close the connection sends its FIN packet and enters the active close path; while the other end will enter the passive close path. the active close path, starts from ESTABLISHED state; then this host sends its FIN packet, call it FIN1, and enters FIN_WAIT_1 ; upon receiving the other end's ACK of FIN1, this host enters the FIN_WAIT_2 state where it waits for any data that is left to be sent from the other end; once the other end finishes sending data it sends its own FIN, call it FIN2 , then this host enters the TIMEWAIT state; after that, it will send the ACK of FIN2 and enters the CLOSED state. the other host starts the closing process by receiving the FIN1 signal where its state changes to CLOSE_WAIT ; then it sends all the data to the other end; the sends its own FIN, we called it FIN2, and enters the LAST_ACK state where it waits for the ACK of FIN2; and then enters the CLOSED state. so in the normal circumstances, the host who wishes to end the connection enters the TIMEWAIT state; but the newer versions of TCP, requires the client to always initialize the closing routine, since this will save the server from having to save the connection connection data, to make sure the same client can not use the same port very soon.","title":"cs2204: unit 7: Discussion Assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit7/da7/#references","text":"Dordal, P. (2019). An introduction to computer networks. https://eng.libretexts.org/Bookshelves/Computer_Science/Networks/Book%3A_An_Introduction_to_Computer_Networks_(Dordal)/07%3A_IP_version_4","title":"References"},{"location":"knowledge-base/cs2204-netwroks1/unit7/ja7/","text":"CS2204: unit 7: Journal \u00b6 What TCP message will be generated for an incoming SYN request for which there is no matching LISTENING port? What error will be given to the application that requested the connection? In the normal circumstances, when a client sends a SYN request, if the server is ready it will reply with an ACK, then the client acknowledges the server's response by another ACK and the three-way handshake is complete and the connection is established. if no port is available -for any reason- the server won't acknowledge the SYN successfully, instead it will send a packet with RST bit set. This packet is being translated on the requester as Connection Refused . The client may do another request with the correct port if necessary.","title":"CS2204: unit 7: Journal"},{"location":"knowledge-base/cs2204-netwroks1/unit7/ja7/#cs2204-unit-7-journal","text":"What TCP message will be generated for an incoming SYN request for which there is no matching LISTENING port? What error will be given to the application that requested the connection? In the normal circumstances, when a client sends a SYN request, if the server is ready it will reply with an ACK, then the client acknowledges the server's response by another ACK and the three-way handshake is complete and the connection is established. if no port is available -for any reason- the server won't acknowledge the SYN successfully, instead it will send a packet with RST bit set. This packet is being translated on the requester as Connection Refused . The client may do another request with the correct port if necessary.","title":"CS2204: unit 7: Journal"},{"location":"knowledge-base/cs2204-netwroks1/unit7/wa7/","text":"CS2204: unit 7: Written Assignment \u00b6 Q1 \u00b6 Why does the maximum packet lifetime have to be large enough to ensure that not only the packet but also its acknowledgments have disappeared? this will help solving the old duplicates problem. maximum packet lifetime is the maximum time a packet spend on the network before it gets discarded, if not arrived to its destination. if the maximum packet lifetime is short, then a packet (or its ACK) may get lost before it has arrived to its destination, causing the other end to retransmit which may increase the possibility of duplicate packets. references: Dordal, P. (2019). An introduction to computer networks. https://eng.libretexts.org/Bookshelves/Computer_Science/Networks/Book%3A_An_Introduction_to_Computer_Networks_(Dordal)/07%3A_IP_version_4 Q2 \u00b6 Give one potential disadvantage when Nagle's algorithm is used on a badly congested network. Nagle's algorithm buffers small packets until it reaches an agreed upon size then it sends them into one TCP/IP package. consider an open ssh connection on TCP/IP protocol, without the Nagle's algorithm, the ssh will send a lot of small packets (eg. for every character you type in the terminal) which indicates ineffective use of the network. instead, using the Nagle algorithm, will buffer these small packets, till you reach few characters (for example) and send them in one packet. The problem arises if the Nagle's algorithm buffers its packet to a size that is larger than the network can handle, worsening the congestion on the network. in this case, the packet may take longer time to arrive to the server causing the client to retransmit which increases the possibility of duplicate packets. another problem might arise with the last packet, if another full packet has already been sent, this packet may never reach a full packet size, causing the client waiting for the ACK of the previous packet before sending this partial packet to the server, this may be a minor problem since the it will only adds few milliseconds of wait on the last packet. references: Network Cyclopedia. (n.d.). What is Nagle\u2019s algorithm? https://networkencyclopedia.com/nagles-algorithm/ Stuart Cheshire (2005) TCP Performance problems caused by interaction between Nagle\u2019s Algorithm and Delayed ACK Q3 \u00b6 Give two examples of cases where TCP sends data-less packets on an established connection (which is not being torn down). A TCP has its window size reduced to 0, then this sender will keep sending data-less packets at regular intervals; each of those polling packets acknowledges the receiver's current ACK, till it receives the receiver's ACK containing window-enlargement announcement. packets that its only purpose is to acknowledge a previous packet from the other end, might be also data-less. references: Dordal, P. (2019). An introduction to computer networks. chapter 12.17 TCP flow control. Q4 \u00b6 Exercise 5.0 from section 12.24 of the textbook: 1. Suppose you see multiple TCP connections on your workstation in state FIN_WAIT_1. What is likely going on? Whose fault is it? the host who is wishing to close the connection enters FIN_WAIT_1, waiting for the other end's acknowledgment of its FIN. this state should not last long, unless the acknowledgment of the other end has been lost. in this case, either the other end is not communicating, as if it is busy or stuck or even unexpectedly disconnected or shutdown; or the acknowledgment from the other end have not being arrived due to congestion on the network; or that our device is disconnected. since we have multiple hosts with state, then the problem is probably on our side, mostly that our very close network is congested, or our device is disconnected. 2.What might be going on if you see connections languishing in state FIN_WAIT_2? FIN_WAIT_2 happens after receiving the acknowledgements of our FIN, waiting for the other end's FIN; before the other end can send its FIN, it must send us all of the remaining data from the connection, before the connection can be closed safely. finding multiple connections on this state, means either these connections have large amount of data left, that need to be transmitted before the FIN; or we are not able to receive those FINs; or even our very close network is congested and all the FINs may arrive at once shortly. finding multiple connections with this state indicates that the problem is on our side of the network.","title":"CS2204: unit 7: Written Assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit7/wa7/#cs2204-unit-7-written-assignment","text":"","title":"CS2204: unit 7: Written Assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit7/wa7/#q1","text":"Why does the maximum packet lifetime have to be large enough to ensure that not only the packet but also its acknowledgments have disappeared? this will help solving the old duplicates problem. maximum packet lifetime is the maximum time a packet spend on the network before it gets discarded, if not arrived to its destination. if the maximum packet lifetime is short, then a packet (or its ACK) may get lost before it has arrived to its destination, causing the other end to retransmit which may increase the possibility of duplicate packets. references: Dordal, P. (2019). An introduction to computer networks. https://eng.libretexts.org/Bookshelves/Computer_Science/Networks/Book%3A_An_Introduction_to_Computer_Networks_(Dordal)/07%3A_IP_version_4","title":"Q1"},{"location":"knowledge-base/cs2204-netwroks1/unit7/wa7/#q2","text":"Give one potential disadvantage when Nagle's algorithm is used on a badly congested network. Nagle's algorithm buffers small packets until it reaches an agreed upon size then it sends them into one TCP/IP package. consider an open ssh connection on TCP/IP protocol, without the Nagle's algorithm, the ssh will send a lot of small packets (eg. for every character you type in the terminal) which indicates ineffective use of the network. instead, using the Nagle algorithm, will buffer these small packets, till you reach few characters (for example) and send them in one packet. The problem arises if the Nagle's algorithm buffers its packet to a size that is larger than the network can handle, worsening the congestion on the network. in this case, the packet may take longer time to arrive to the server causing the client to retransmit which increases the possibility of duplicate packets. another problem might arise with the last packet, if another full packet has already been sent, this packet may never reach a full packet size, causing the client waiting for the ACK of the previous packet before sending this partial packet to the server, this may be a minor problem since the it will only adds few milliseconds of wait on the last packet. references: Network Cyclopedia. (n.d.). What is Nagle\u2019s algorithm? https://networkencyclopedia.com/nagles-algorithm/ Stuart Cheshire (2005) TCP Performance problems caused by interaction between Nagle\u2019s Algorithm and Delayed ACK","title":"Q2"},{"location":"knowledge-base/cs2204-netwroks1/unit7/wa7/#q3","text":"Give two examples of cases where TCP sends data-less packets on an established connection (which is not being torn down). A TCP has its window size reduced to 0, then this sender will keep sending data-less packets at regular intervals; each of those polling packets acknowledges the receiver's current ACK, till it receives the receiver's ACK containing window-enlargement announcement. packets that its only purpose is to acknowledge a previous packet from the other end, might be also data-less. references: Dordal, P. (2019). An introduction to computer networks. chapter 12.17 TCP flow control.","title":"Q3"},{"location":"knowledge-base/cs2204-netwroks1/unit7/wa7/#q4","text":"Exercise 5.0 from section 12.24 of the textbook: 1. Suppose you see multiple TCP connections on your workstation in state FIN_WAIT_1. What is likely going on? Whose fault is it? the host who is wishing to close the connection enters FIN_WAIT_1, waiting for the other end's acknowledgment of its FIN. this state should not last long, unless the acknowledgment of the other end has been lost. in this case, either the other end is not communicating, as if it is busy or stuck or even unexpectedly disconnected or shutdown; or the acknowledgment from the other end have not being arrived due to congestion on the network; or that our device is disconnected. since we have multiple hosts with state, then the problem is probably on our side, mostly that our very close network is congested, or our device is disconnected. 2.What might be going on if you see connections languishing in state FIN_WAIT_2? FIN_WAIT_2 happens after receiving the acknowledgements of our FIN, waiting for the other end's FIN; before the other end can send its FIN, it must send us all of the remaining data from the connection, before the connection can be closed safely. finding multiple connections on this state, means either these connections have large amount of data left, that need to be transmitted before the FIN; or we are not able to receive those FINs; or even our very close network is congested and all the FINs may arrive at once shortly. finding multiple connections with this state indicates that the problem is on our side of the network.","title":"Q4"},{"location":"knowledge-base/cs2204-netwroks1/unit8/","text":"CS2204: unit 8: Application Layer and Security \u00b6 [b] Application Layer \u00b6 Application layer is the layer were users interact with. 2 principles: client-side principle, peer-to-peer principle. 3.1 Client-side principle \u00b6 the oldest model. This model comes naturally from the mainframes and minicomputers that were the only networked computers used until the 1980s. model protocol: the set of rules that define the format of messages exchanged and their ordering: syntax and organization of information . characters encoded in ASCII , where each character is defined by 7 bits then transferred as 8 bits where the first left (most significant bit) is set to 0 . Backus-Naur Form (BNF) are set of rules that generate all valid characters strings. big-endian: sends the most significant bit followed by the least significant bit . little-endian: sends the least significant bit followed by the most significant bit . a host is either a server or a client, the servers serves large number of clients. 3.1.1 the peer-to-peer model \u00b6 emerged in the last ten years. all hosts work as both servers and clients at the same time. used in: Internet telephony, file sharing, and Internet-wide file systems. 3.1.2 the transport services \u00b6 2 types of transport services: connectionless or diagram services connection-oriented or byte-stream services connectionless: allow applications to easily exchange messages or service data units uses UDP unreliable, but able to detect transmission errors. several networked applications may be running at the same time on a single host. each networked application can be identified by: the host in which the application is running the port number in which the application is listening on the Internet, the host is the network address, the port number is an integer. IPv4 addresses are 32 bits, represented by a dotted decimal where each decimal represents one byte of the address, 4 parts each represent a byte. IPv6 addresses are 128 bits, uses hexadecimal numbers (16 bits) separated by colons, 8 parts each represent a 16-bit (2 bytes). used when short queries and responses are exchanged. connection-oriented creates a reliable byte stream between the 2 applications. applications are identified by their hos and their port numbers. use TCP. reliable and bidirectional. used when longer responses are expected. 3.2.1 The Domaine name system \u00b6 names are used as an aliases for IP addresses. change your ISP will change the IP address of the host or application. started with one file: hosts.txt : contains the mapping between the name of each Internet host and its associated IP address. It was maintained by SRI International that coordinated the Network Information Center (NIC). when a new host comes available, a system admin will register the host name and IP address in this file. operating systems have this file stored somewhere in the filesystem, and update it regularly form the RSI server. today hosts are saved in a tree structure, where the top level is the country code or generic suffix like, gov, com .. hosts registration are managed by Internet Corporation for Assigned Names and Numbers (ICANN). This grammar specifies that a host name is an ordered list of labels separated by the dot (.) character. Each label can contain letters, numbers and the hyphen character (-) 4. Fully qualified domain names are read from left to right. The first label is a hostname or a domain name followed by the hierarchy of domains and ending with the root implicitly at the right. The top-level domain name must be one of the registered TLDs 5. For example, www.whitehouse.gov corresponds to a host named www inside the whitehouse domain that belongs to the gov top-level domain . info.ucl.ac.be corresponds to the info domain inside the ucl domain that is included in the ac sub-domain of the be top-level domain. DNS: distributed database that contains mappings between fully qualified domain names and IP addresses. uses client server model: clients are host need to retrieve the IP addresses of other hosts stored in the DNS database. each nameserver stores part of the DNS database and answer queries by clients. root nameserver: Each root nameserver maintains the list of all the nameservers that are responsible for each of the top-level domain names and their IP addresses. All root nameservers are synchronised and provide the same answers. By querying any of the root nameservers, a DNS client can obtain the nameserver that is responsible for any top-level-domain name. From this nameserver, it is possible to resolve any domain name. DNS resolvers: resolver is a server that provides the resolution service for a set of clients. each host sends all its DNS queries to the local resolver These queries are called recursive queries as the resolver must recurse through the hierarchy of nameservers to obtain the answer. advantages: regular internet hosts dont need to keep a list of up-to-date host services. hosts dont need to send their DNS queries over the Internet, since the local resolver caches large number of hosts so it can return quickly and saves bandwidth. DNS protocol: runs above both datagram and byte-stream services. DNS messages are in 5 parts: Header: (mandatory), information about the type of the message, information about other parts of the message: 12 bytes. Question: (mandatory), information about the question that is been sent to the resolver. Answer: (mandatory), information about the answer to the question above, when a client sends a query, this section is empty. Answer: (optional), information about the servers that can provide authoritative answer if required. additional information: (optional), any additional information needs to be sent to the resolver. DNS Header (12 bytes): ID: 16-bit random number chosen by the client, the server (resolver) need to return this id to the client so client can know to which question this answer belongs. QR flag: 0 in the query(question), 1 in the answer. Opcode: specify the type of query, eq. standard query. AA bit: set if the server (resolver) has an authority for the domain name in question. The authoritative servers are managed by the system administrators responsible for a given domain, and the always store the most up-to-date information. RD: recursion desired: set bu client with the request, the resolver will recurse through the DNS hierarchy to retrieve the answer on behalf of the client. RA: one-bit indicates if the server supports recursion. RCODE: distinguish between different types of errors. The last four fields indicate the size of the Question, Answer, Authority and Additional sections of the DNS message. last 4 sections of the DNS message (Question, Answer, Authority and Additional Information) contain Resource Record (RR) Resource Record (RR): Name: the name of the node this resource associated with. Type (2 bytes = 16 bits): Class: used to support the utilization of the DNS in environments other than the internet. TTL: lifetime of RR in seconds, set by the server, indicates how long the RR answer can set in the cache. long TTL = stable RR. RDLength: length of RData field that contains information about the type specified in the type field. RR Types: type A: encode IPv4 address. type AAAA: encode IPv6 address. type ANS: the name of the DNS server. type CNAME: (canonical name) used to define aliases,eg. www.example.com could be a CNAME for pc12.example.com that is the actual name of the server on which the web server for www.example.com runs. PTR (pointer) RR : obtain the name that corresponds to an IP address. IP address will be sent in the question and the answer will contain the domain name. so that RR Name is the IP address, while RData contains the domain name. suffix .in-addr.arpa needs to be added to the IP address. 3.2.2 Electronic mail \u00b6 appeared in the 70s. email system has 4 components: message format: defines how the valid email messages are encoded. protocols: that allows hosts and servers to exchange email messages. client software: allow users to create and read email messages. software: allow servers to effectively exchange messages. email messages: contains header and body. lines of ASCII characters, each line contains 998 characters terminated by CR and LF. empty line marks the end of the header. email header: several lines, each line starts with keyword then colon then the value. as: key:value\\n 2 lines are mandatory, From line From: (name: optional)\\< sender email address > , and the Date line. Other header lines (optional): TO: email address of the receiver. cc: email addresses to receive a copy, separated by commas. bcc: email addresses to receive blind carbon copy . subject. message id. in-reply-to: holds the id of the message that this message is replying to. Received: used when the message is processed by several email servers before reaching its destination. MIME-Version: MIME-Version specification used to encode this email. eg. '1.0', if no MIME-Version then the email encoded with ASCII. Content-Type: indicates how the message is structured: multipart/mixed : email contains several independent parts, eg. plain text and binary data. multipart/alternative : email contains several representations of the same information, eg. bot plain text and HTML version of the same text. Content-Transfer-Encoding: specify how the message has been encoded, default is 7-bit ASCII , popular: quoted-printable, base64 ASCII characters was limited to other languages than english and to other encodings like binary files. to solve ASCII problems, the IETF developed the Multipurpose Internet Mail Extensions (MIME) : allow email to carry non-ASCII characters and binary files without breaking the email servers. to support 2 types of mime messages, the receiver must be able to extract the different parts of the message, separation uses special lines to define the boundaries between parts of a MIME message, eg -LAST_LINE- , to solve this, the content type may contain another parameter to define the boundary that has been used eg. Content-Type: multipart/mixed; boundary=\"simple boundary\" , so that an empty line contain the text '--simple boundary' indicates the separation. Content-Type header also used inside a MIME part specifying the type of this part , popular Content-Type headers: text: which has several sub-types eg. text/plain for ASCII, text/html for html, text/enriched. can contain second parameter defining the character set used for encoding the text. eg. charset=utf-8, charset=us-ascii, charset=iso-8859-1 . image audio video application: contains binary information produced by a particular application listed as the subtype. the client needs to lunch the application listed in the subtype to extract the binary information. email clients use the Simple Mail Transfer Protocol (SMTP) . Mail eXchange (MX) records of the DNS maps the SMTP servers addresses that contain a specific mailbox, where set of MX records can be associated with each domain. Each MX record contains numerical preference and fully qualified domain name of SMTP server that is able to send mail to all valid email addresses of this domain. The DNS can return several MX records for a given domain. In this case, the server with the lowest preference is used first. If this server is not reachable, the second most preferred server is used etc. the receiver SMTP server will store the email message, and client can retrieve it using web mail interface or protocols such as Post Office Protocol (POP) or the Internet Message Access Protocol (IMAP) . The Simple Mail Transfer Protocol (SMTP) \u00b6 client-server protocol. 5 types of processes involved in the delivery of email message. Mail User Agent (MUA): email client or web mail, sends the message to MSA. Mail Submission Agent (MSA): processes the email message and forwards it to MTA. Mail Transmission Agent (MTA): transmits the message directly -or via intermediate MTAs- to MTA of the destination domain. destination MTA forewords the MDA. Mail Delivery Agent (MDA): will store the message and make it available to the receiving MUA. receiver MUA. SMTP is used for the interactions between MUA-MSA, MSA-MTA, and MTA-MTA. SMTP is text-based protocol, relies on byte-stream services . SMTP servers listen on port 25. SMTP clients send commands that each is ASCII text terminated by CR+FL , SMTP servers respond with 3 digit number indicate success/error and optional comments . SMTP request BNF has 5 main commands: EHLO, MAIL FROM:, RCPT TO:, DATA and QUIT . Postmaster is the alias of the system administrator who is responsible for a given domain or SMTP server. All domains must have a Postmaster alias. SMTP servers use structured reply codes containing three digits and an optional comment. The first digit indicates success/error, 2xy = command accepted. 3xy = command accepted, but additional information from the client is expected. 4xy = negative reply, try again later. 5xy = permanent failure or error, don't try again. 250 is the standard success response. The transfer of an email message is performed in three phases: client opens transport connection with server. client and server exchange greetings messages (EHLO command), if no proper greetings received, server would close the connection. email transfer phase: the client transfers one or more email messages by indicating the email address of the sender (MAIL FROM: command), the email address of the recipient (RCPT TO: command) followed by the headers and the body of the email message (DATA command). Once the client has finished sending all its queued email messages to the SMTP server, it terminates the SMTP association (QUIT command). The Post Office Protocol (POP) \u00b6 Internet Message Access Protocol (IMAP): allow client applications to efficiently access in real-time to messages stored in various folders on servers, and provides the functions that are necessary to search, download, delete or filter messages. POP allows a client to download all the messages destined to a given user from his/her email server. POP is another example of a simple line-based protocol. POP listens on port 110. POP session is composed of 3 parts: authorization phase: server verifies client credentials transaction phase: client downloads messages. update phase: concludes the session. POP client sends commands and the server replies are prefixed by +OK to indicate a successful command or by -ERR to indicate errors. POP commands: USER, PASS: exchange username and password STAT: retrieve information about the status of the server, server replies with OK + number of messages in the mailbox + size of the mailbox in bytes RETR: retrieve number of messages (n) supplied in the command. DELE: delete a message with the position (n) supplied in the command. QUIT. HTTP protocol \u00b6 document sharing system such as the world wide web is composed of 3 parts: standard addressing schema. standard document format: HTML. standard protocol: facilities efficient retrieve of documents stored ona server. BNF of URI URI components: scheme: identifies the application-layer protocol that must be used by the client to retrieve the document, as [scheme]:// eg. http, https, ws ... authority: identifies the DNS name or the IP address of the server. document will be retrieved from the server based on the scheme. path: ([A-Z0-9]\\/)+ query: ?p1=any : optional The first version of HTML was derived from the Standard Generalized Markup Language (SGML) that was standardised in 1986 by ISO A markup language is a structured way of adding annotations about the formatting of the document within the document itself. Example markup languages include troff, which is used to write the Unix man pages or Latex. HTTP is a text-based protocol, in which the client sends a request and the server returns a response. HTTP listens on port 80, runs above byte-stream services . HTTP request contains 3 parts: method header optional MIME document HTTP response contains: status line header MIME document HTTP methods: GET: client must open tcp connection on port 80 with the server to receive the response document. HEAD: similar to GET, but retrieves only the header of the response POST: send a document to the server, sent document attached to the request as MIME document. HTTP Response headers: Content-Length: length of the MIME document in bytes Content-Type: type of the MIME document attached, html documents = text/html. Content-Encoding: encoding of the MIME document, eg. 'x-gzip' Server: version of the web server generated the response, may include server software, optional modules they used Date Last-Modified: indicates the last modification of the attached document. HTTP request headers: User-Agent: information about the client generated the request. If-Modified-Since: enables clients to cache documents on their memory, so that the client will check the existence of this file in its cache before sending the request over the network. Referrer: contains URI of the document that the client visited before requesting this document, Host: contains fully qualified domain name of the URI being requested. HTTP response status code: 2xy: OK 3xy: requested document is no longer available on the server, 301 = moved permanently, Location header with the new URI is sent with the response. 304 = not modified. 4xy: bad request 5xy: server error. using TCP with http has multiple performance problems: open TCP connection for each URI, requires the client and server to exchange multiple open/close connection packets that are not necessary. increases the delay, where large number of client connections may be a bottleneck for the server. the solution was to introduce support for persistent TCP connections : where a multiple http requests can be sent over a single TCP connection. several new headers introduced to support persistent TCP connections : Connection: used with keep-alive argument by the client to indicate that the TCP connection needs to stay persistent. Keep-Alive: contains 2 parameters: maximum number of requests that server agrees to serve through this TCP connection. timeout (in seconds) after which the server will close an idle the connection . references \u00b6 [b] Bonaventure, O. (2011). Computer networking: Principles, protocols and practice. The Saylor Foundation. This book is licensed under Creative Commons Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0).","title":"CS2204: unit 8: Application Layer and Security"},{"location":"knowledge-base/cs2204-netwroks1/unit8/#cs2204-unit-8-application-layer-and-security","text":"","title":"CS2204: unit 8: Application Layer and Security"},{"location":"knowledge-base/cs2204-netwroks1/unit8/#b-application-layer","text":"Application layer is the layer were users interact with. 2 principles: client-side principle, peer-to-peer principle.","title":"[b] Application Layer"},{"location":"knowledge-base/cs2204-netwroks1/unit8/#31-client-side-principle","text":"the oldest model. This model comes naturally from the mainframes and minicomputers that were the only networked computers used until the 1980s. model protocol: the set of rules that define the format of messages exchanged and their ordering: syntax and organization of information . characters encoded in ASCII , where each character is defined by 7 bits then transferred as 8 bits where the first left (most significant bit) is set to 0 . Backus-Naur Form (BNF) are set of rules that generate all valid characters strings. big-endian: sends the most significant bit followed by the least significant bit . little-endian: sends the least significant bit followed by the most significant bit . a host is either a server or a client, the servers serves large number of clients.","title":"3.1 Client-side principle"},{"location":"knowledge-base/cs2204-netwroks1/unit8/#311-the-peer-to-peer-model","text":"emerged in the last ten years. all hosts work as both servers and clients at the same time. used in: Internet telephony, file sharing, and Internet-wide file systems.","title":"3.1.1 the peer-to-peer model"},{"location":"knowledge-base/cs2204-netwroks1/unit8/#312-the-transport-services","text":"2 types of transport services: connectionless or diagram services connection-oriented or byte-stream services connectionless: allow applications to easily exchange messages or service data units uses UDP unreliable, but able to detect transmission errors. several networked applications may be running at the same time on a single host. each networked application can be identified by: the host in which the application is running the port number in which the application is listening on the Internet, the host is the network address, the port number is an integer. IPv4 addresses are 32 bits, represented by a dotted decimal where each decimal represents one byte of the address, 4 parts each represent a byte. IPv6 addresses are 128 bits, uses hexadecimal numbers (16 bits) separated by colons, 8 parts each represent a 16-bit (2 bytes). used when short queries and responses are exchanged. connection-oriented creates a reliable byte stream between the 2 applications. applications are identified by their hos and their port numbers. use TCP. reliable and bidirectional. used when longer responses are expected.","title":"3.1.2 the transport services"},{"location":"knowledge-base/cs2204-netwroks1/unit8/#321-the-domaine-name-system","text":"names are used as an aliases for IP addresses. change your ISP will change the IP address of the host or application. started with one file: hosts.txt : contains the mapping between the name of each Internet host and its associated IP address. It was maintained by SRI International that coordinated the Network Information Center (NIC). when a new host comes available, a system admin will register the host name and IP address in this file. operating systems have this file stored somewhere in the filesystem, and update it regularly form the RSI server. today hosts are saved in a tree structure, where the top level is the country code or generic suffix like, gov, com .. hosts registration are managed by Internet Corporation for Assigned Names and Numbers (ICANN). This grammar specifies that a host name is an ordered list of labels separated by the dot (.) character. Each label can contain letters, numbers and the hyphen character (-) 4. Fully qualified domain names are read from left to right. The first label is a hostname or a domain name followed by the hierarchy of domains and ending with the root implicitly at the right. The top-level domain name must be one of the registered TLDs 5. For example, www.whitehouse.gov corresponds to a host named www inside the whitehouse domain that belongs to the gov top-level domain . info.ucl.ac.be corresponds to the info domain inside the ucl domain that is included in the ac sub-domain of the be top-level domain. DNS: distributed database that contains mappings between fully qualified domain names and IP addresses. uses client server model: clients are host need to retrieve the IP addresses of other hosts stored in the DNS database. each nameserver stores part of the DNS database and answer queries by clients. root nameserver: Each root nameserver maintains the list of all the nameservers that are responsible for each of the top-level domain names and their IP addresses. All root nameservers are synchronised and provide the same answers. By querying any of the root nameservers, a DNS client can obtain the nameserver that is responsible for any top-level-domain name. From this nameserver, it is possible to resolve any domain name. DNS resolvers: resolver is a server that provides the resolution service for a set of clients. each host sends all its DNS queries to the local resolver These queries are called recursive queries as the resolver must recurse through the hierarchy of nameservers to obtain the answer. advantages: regular internet hosts dont need to keep a list of up-to-date host services. hosts dont need to send their DNS queries over the Internet, since the local resolver caches large number of hosts so it can return quickly and saves bandwidth. DNS protocol: runs above both datagram and byte-stream services. DNS messages are in 5 parts: Header: (mandatory), information about the type of the message, information about other parts of the message: 12 bytes. Question: (mandatory), information about the question that is been sent to the resolver. Answer: (mandatory), information about the answer to the question above, when a client sends a query, this section is empty. Answer: (optional), information about the servers that can provide authoritative answer if required. additional information: (optional), any additional information needs to be sent to the resolver. DNS Header (12 bytes): ID: 16-bit random number chosen by the client, the server (resolver) need to return this id to the client so client can know to which question this answer belongs. QR flag: 0 in the query(question), 1 in the answer. Opcode: specify the type of query, eq. standard query. AA bit: set if the server (resolver) has an authority for the domain name in question. The authoritative servers are managed by the system administrators responsible for a given domain, and the always store the most up-to-date information. RD: recursion desired: set bu client with the request, the resolver will recurse through the DNS hierarchy to retrieve the answer on behalf of the client. RA: one-bit indicates if the server supports recursion. RCODE: distinguish between different types of errors. The last four fields indicate the size of the Question, Answer, Authority and Additional sections of the DNS message. last 4 sections of the DNS message (Question, Answer, Authority and Additional Information) contain Resource Record (RR) Resource Record (RR): Name: the name of the node this resource associated with. Type (2 bytes = 16 bits): Class: used to support the utilization of the DNS in environments other than the internet. TTL: lifetime of RR in seconds, set by the server, indicates how long the RR answer can set in the cache. long TTL = stable RR. RDLength: length of RData field that contains information about the type specified in the type field. RR Types: type A: encode IPv4 address. type AAAA: encode IPv6 address. type ANS: the name of the DNS server. type CNAME: (canonical name) used to define aliases,eg. www.example.com could be a CNAME for pc12.example.com that is the actual name of the server on which the web server for www.example.com runs. PTR (pointer) RR : obtain the name that corresponds to an IP address. IP address will be sent in the question and the answer will contain the domain name. so that RR Name is the IP address, while RData contains the domain name. suffix .in-addr.arpa needs to be added to the IP address.","title":"3.2.1 The Domaine name system"},{"location":"knowledge-base/cs2204-netwroks1/unit8/#322-electronic-mail","text":"appeared in the 70s. email system has 4 components: message format: defines how the valid email messages are encoded. protocols: that allows hosts and servers to exchange email messages. client software: allow users to create and read email messages. software: allow servers to effectively exchange messages. email messages: contains header and body. lines of ASCII characters, each line contains 998 characters terminated by CR and LF. empty line marks the end of the header. email header: several lines, each line starts with keyword then colon then the value. as: key:value\\n 2 lines are mandatory, From line From: (name: optional)\\< sender email address > , and the Date line. Other header lines (optional): TO: email address of the receiver. cc: email addresses to receive a copy, separated by commas. bcc: email addresses to receive blind carbon copy . subject. message id. in-reply-to: holds the id of the message that this message is replying to. Received: used when the message is processed by several email servers before reaching its destination. MIME-Version: MIME-Version specification used to encode this email. eg. '1.0', if no MIME-Version then the email encoded with ASCII. Content-Type: indicates how the message is structured: multipart/mixed : email contains several independent parts, eg. plain text and binary data. multipart/alternative : email contains several representations of the same information, eg. bot plain text and HTML version of the same text. Content-Transfer-Encoding: specify how the message has been encoded, default is 7-bit ASCII , popular: quoted-printable, base64 ASCII characters was limited to other languages than english and to other encodings like binary files. to solve ASCII problems, the IETF developed the Multipurpose Internet Mail Extensions (MIME) : allow email to carry non-ASCII characters and binary files without breaking the email servers. to support 2 types of mime messages, the receiver must be able to extract the different parts of the message, separation uses special lines to define the boundaries between parts of a MIME message, eg -LAST_LINE- , to solve this, the content type may contain another parameter to define the boundary that has been used eg. Content-Type: multipart/mixed; boundary=\"simple boundary\" , so that an empty line contain the text '--simple boundary' indicates the separation. Content-Type header also used inside a MIME part specifying the type of this part , popular Content-Type headers: text: which has several sub-types eg. text/plain for ASCII, text/html for html, text/enriched. can contain second parameter defining the character set used for encoding the text. eg. charset=utf-8, charset=us-ascii, charset=iso-8859-1 . image audio video application: contains binary information produced by a particular application listed as the subtype. the client needs to lunch the application listed in the subtype to extract the binary information. email clients use the Simple Mail Transfer Protocol (SMTP) . Mail eXchange (MX) records of the DNS maps the SMTP servers addresses that contain a specific mailbox, where set of MX records can be associated with each domain. Each MX record contains numerical preference and fully qualified domain name of SMTP server that is able to send mail to all valid email addresses of this domain. The DNS can return several MX records for a given domain. In this case, the server with the lowest preference is used first. If this server is not reachable, the second most preferred server is used etc. the receiver SMTP server will store the email message, and client can retrieve it using web mail interface or protocols such as Post Office Protocol (POP) or the Internet Message Access Protocol (IMAP) .","title":"3.2.2 Electronic mail"},{"location":"knowledge-base/cs2204-netwroks1/unit8/#the-simple-mail-transfer-protocol-smtp","text":"client-server protocol. 5 types of processes involved in the delivery of email message. Mail User Agent (MUA): email client or web mail, sends the message to MSA. Mail Submission Agent (MSA): processes the email message and forwards it to MTA. Mail Transmission Agent (MTA): transmits the message directly -or via intermediate MTAs- to MTA of the destination domain. destination MTA forewords the MDA. Mail Delivery Agent (MDA): will store the message and make it available to the receiving MUA. receiver MUA. SMTP is used for the interactions between MUA-MSA, MSA-MTA, and MTA-MTA. SMTP is text-based protocol, relies on byte-stream services . SMTP servers listen on port 25. SMTP clients send commands that each is ASCII text terminated by CR+FL , SMTP servers respond with 3 digit number indicate success/error and optional comments . SMTP request BNF has 5 main commands: EHLO, MAIL FROM:, RCPT TO:, DATA and QUIT . Postmaster is the alias of the system administrator who is responsible for a given domain or SMTP server. All domains must have a Postmaster alias. SMTP servers use structured reply codes containing three digits and an optional comment. The first digit indicates success/error, 2xy = command accepted. 3xy = command accepted, but additional information from the client is expected. 4xy = negative reply, try again later. 5xy = permanent failure or error, don't try again. 250 is the standard success response. The transfer of an email message is performed in three phases: client opens transport connection with server. client and server exchange greetings messages (EHLO command), if no proper greetings received, server would close the connection. email transfer phase: the client transfers one or more email messages by indicating the email address of the sender (MAIL FROM: command), the email address of the recipient (RCPT TO: command) followed by the headers and the body of the email message (DATA command). Once the client has finished sending all its queued email messages to the SMTP server, it terminates the SMTP association (QUIT command).","title":"The Simple Mail Transfer Protocol (SMTP)"},{"location":"knowledge-base/cs2204-netwroks1/unit8/#the-post-office-protocol-pop","text":"Internet Message Access Protocol (IMAP): allow client applications to efficiently access in real-time to messages stored in various folders on servers, and provides the functions that are necessary to search, download, delete or filter messages. POP allows a client to download all the messages destined to a given user from his/her email server. POP is another example of a simple line-based protocol. POP listens on port 110. POP session is composed of 3 parts: authorization phase: server verifies client credentials transaction phase: client downloads messages. update phase: concludes the session. POP client sends commands and the server replies are prefixed by +OK to indicate a successful command or by -ERR to indicate errors. POP commands: USER, PASS: exchange username and password STAT: retrieve information about the status of the server, server replies with OK + number of messages in the mailbox + size of the mailbox in bytes RETR: retrieve number of messages (n) supplied in the command. DELE: delete a message with the position (n) supplied in the command. QUIT.","title":"The Post Office Protocol (POP)"},{"location":"knowledge-base/cs2204-netwroks1/unit8/#http-protocol","text":"document sharing system such as the world wide web is composed of 3 parts: standard addressing schema. standard document format: HTML. standard protocol: facilities efficient retrieve of documents stored ona server. BNF of URI URI components: scheme: identifies the application-layer protocol that must be used by the client to retrieve the document, as [scheme]:// eg. http, https, ws ... authority: identifies the DNS name or the IP address of the server. document will be retrieved from the server based on the scheme. path: ([A-Z0-9]\\/)+ query: ?p1=any : optional The first version of HTML was derived from the Standard Generalized Markup Language (SGML) that was standardised in 1986 by ISO A markup language is a structured way of adding annotations about the formatting of the document within the document itself. Example markup languages include troff, which is used to write the Unix man pages or Latex. HTTP is a text-based protocol, in which the client sends a request and the server returns a response. HTTP listens on port 80, runs above byte-stream services . HTTP request contains 3 parts: method header optional MIME document HTTP response contains: status line header MIME document HTTP methods: GET: client must open tcp connection on port 80 with the server to receive the response document. HEAD: similar to GET, but retrieves only the header of the response POST: send a document to the server, sent document attached to the request as MIME document. HTTP Response headers: Content-Length: length of the MIME document in bytes Content-Type: type of the MIME document attached, html documents = text/html. Content-Encoding: encoding of the MIME document, eg. 'x-gzip' Server: version of the web server generated the response, may include server software, optional modules they used Date Last-Modified: indicates the last modification of the attached document. HTTP request headers: User-Agent: information about the client generated the request. If-Modified-Since: enables clients to cache documents on their memory, so that the client will check the existence of this file in its cache before sending the request over the network. Referrer: contains URI of the document that the client visited before requesting this document, Host: contains fully qualified domain name of the URI being requested. HTTP response status code: 2xy: OK 3xy: requested document is no longer available on the server, 301 = moved permanently, Location header with the new URI is sent with the response. 304 = not modified. 4xy: bad request 5xy: server error. using TCP with http has multiple performance problems: open TCP connection for each URI, requires the client and server to exchange multiple open/close connection packets that are not necessary. increases the delay, where large number of client connections may be a bottleneck for the server. the solution was to introduce support for persistent TCP connections : where a multiple http requests can be sent over a single TCP connection. several new headers introduced to support persistent TCP connections : Connection: used with keep-alive argument by the client to indicate that the TCP connection needs to stay persistent. Keep-Alive: contains 2 parameters: maximum number of requests that server agrees to serve through this TCP connection. timeout (in seconds) after which the server will close an idle the connection .","title":"HTTP protocol"},{"location":"knowledge-base/cs2204-netwroks1/unit8/#references","text":"[b] Bonaventure, O. (2011). Computer networking: Principles, protocols and practice. The Saylor Foundation. This book is licensed under Creative Commons Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0).","title":"references"},{"location":"knowledge-base/cs2204-netwroks1/unit8/da8/","text":"CS2204: unit 8: Discussion Assignment \u00b6 Choose ONE of the below security topics from chapter 22 of the Dordal (2019) course text. Research the topic, then provide a summary of the technology and how it works . 1. Network Intrusion Detection 2. Secure Shell (SSH) 3. Transport Layer Security (TLS) 4. Public Key Encryption TLS is a security protocol that allows client/server applications to communicate in a way that is designed to prevent eavesdropping, tampering, or message forgery. TLS is used to encrypt communications between a server and a client such as browser, email client, VoIP. TLS evolved from SSL. HTTPS is an implementation of HTTP + TLS. TLS can help with: encryption, authentication, integrity where TLS can check if data has been changed or manipulated in its way through the network. The server needs to have SSL certificate. TLS starts with a handshake between client and server, where the TLS version, cipher suite, and session keys is being set. during the handshake, the server is being authenticated and identified according to its TLS certificate. this handshake sets up a ciphered communication session, according to the encryption keys that both client and server agreed upon. for both client and server, before they send message -either request or response- the packet is being encrypted according to the encryption keys agreed upon, then the TLS on the other side of the communication will authenticate the sender and decipher packet data, then deliver it to the recipient. references: CloudFlare (n.d.) What is TLS (Transport Layer Security) https://www.cloudflare.com/en-gb/learning/ssl/transport-layer-security-tls/","title":"CS2204: unit 8: Discussion Assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit8/da8/#cs2204-unit-8-discussion-assignment","text":"Choose ONE of the below security topics from chapter 22 of the Dordal (2019) course text. Research the topic, then provide a summary of the technology and how it works . 1. Network Intrusion Detection 2. Secure Shell (SSH) 3. Transport Layer Security (TLS) 4. Public Key Encryption TLS is a security protocol that allows client/server applications to communicate in a way that is designed to prevent eavesdropping, tampering, or message forgery. TLS is used to encrypt communications between a server and a client such as browser, email client, VoIP. TLS evolved from SSL. HTTPS is an implementation of HTTP + TLS. TLS can help with: encryption, authentication, integrity where TLS can check if data has been changed or manipulated in its way through the network. The server needs to have SSL certificate. TLS starts with a handshake between client and server, where the TLS version, cipher suite, and session keys is being set. during the handshake, the server is being authenticated and identified according to its TLS certificate. this handshake sets up a ciphered communication session, according to the encryption keys that both client and server agreed upon. for both client and server, before they send message -either request or response- the packet is being encrypted according to the encryption keys agreed upon, then the TLS on the other side of the communication will authenticate the sender and decipher packet data, then deliver it to the recipient. references: CloudFlare (n.d.) What is TLS (Transport Layer Security) https://www.cloudflare.com/en-gb/learning/ssl/transport-layer-security-tls/","title":"CS2204: unit 8: Discussion Assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit8/ja8/","text":"CS2204: unit 8: Journal Assignment \u00b6 1. DNS uses UDP instead of TCP. If a DNS packet is lost, there is no automatic recovery. Does this cause a problem, and if so, how is it solved? The DNS header contains 16-bit number as its ID. this number is chosen by the client, if a client sent a DNS packet with a specific ID, it will expect the server to send a DNS packet as a response with the same ID. If no response is received with that ID after a specific period of time, the client will retransmit its packet again. 2. Suppose that someone sets up a vacation reply and sends a message before logging out. Unfortunately, the recipient has also set up a vacation reply message. What will happen in this case? Will the canned replies keep on going back and forth until someone returns? In the early days of email systems, this may cause email loop which causes both clients to respond to each other infinitely; however, today, email clients with auto-responders set a header indicating that this an auto response message, which will break this loop after one message has been sent to the same recipient. So, maybe the actual email will be sent from A to B, then B responds that the recipient is in vacation to A; then also A responds that the recipient is in vacation; then the loop will break; so a sum of 3 emails may be sent, the actual email that is been sent before logging out, and a single auto-response from each client. 3. When web pages are sent out, they are prefixed with MIME headers. why? Life was simpler when only text needed to be exchanged; however, there are several different MIME types available for HTTP responses right now, like: text/plain, text/html, application/xhtml+xml, application/json. The headers needs to identify the MIME type of the document that is delivering so that browsers -or any consumers- can recognize it, and parse it correctly, then can be properly used. 4. Does VoIP have same the problems with the firewall that streaming audio has? VoIP runs over UDP, while streaming audio runs over TCP; usually firewalls allow session based communication so TCP based connections are fine and won't face problems. While for session-less communications like UDP are not allowed by default in firewalls due to security concerns, they can be allowed by opening UDP ports on the firewall;","title":"CS2204: unit 8: Journal Assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit8/ja8/#cs2204-unit-8-journal-assignment","text":"1. DNS uses UDP instead of TCP. If a DNS packet is lost, there is no automatic recovery. Does this cause a problem, and if so, how is it solved? The DNS header contains 16-bit number as its ID. this number is chosen by the client, if a client sent a DNS packet with a specific ID, it will expect the server to send a DNS packet as a response with the same ID. If no response is received with that ID after a specific period of time, the client will retransmit its packet again. 2. Suppose that someone sets up a vacation reply and sends a message before logging out. Unfortunately, the recipient has also set up a vacation reply message. What will happen in this case? Will the canned replies keep on going back and forth until someone returns? In the early days of email systems, this may cause email loop which causes both clients to respond to each other infinitely; however, today, email clients with auto-responders set a header indicating that this an auto response message, which will break this loop after one message has been sent to the same recipient. So, maybe the actual email will be sent from A to B, then B responds that the recipient is in vacation to A; then also A responds that the recipient is in vacation; then the loop will break; so a sum of 3 emails may be sent, the actual email that is been sent before logging out, and a single auto-response from each client. 3. When web pages are sent out, they are prefixed with MIME headers. why? Life was simpler when only text needed to be exchanged; however, there are several different MIME types available for HTTP responses right now, like: text/plain, text/html, application/xhtml+xml, application/json. The headers needs to identify the MIME type of the document that is delivering so that browsers -or any consumers- can recognize it, and parse it correctly, then can be properly used. 4. Does VoIP have same the problems with the firewall that streaming audio has? VoIP runs over UDP, while streaming audio runs over TCP; usually firewalls allow session based communication so TCP based connections are fine and won't face problems. While for session-less communications like UDP are not allowed by default in firewalls due to security concerns, they can be allowed by opening UDP ports on the firewall;","title":"CS2204: unit 8: Journal Assignment"},{"location":"knowledge-base/cs2204-netwroks1/unit9/","text":"CS2204: unit 9: Revision \u00b6 Private IP address range: Class A: 10.0.0.0 \u2014 10.255.255.255 Class B: 172.16.0.0 \u2014 172.31.255.255 Class C: 192.168.0.0 \u2014 192.168.255.255 private IPv4 address are the IP address of the device in the local network; they exposed to the outer network behind the router's IP address. private IPv6 addresses can be reused for every network. the most common default private IP address format is 192.168.x.y; private IP can be found in the device settings and it is assigned to the device by the router. public IPv4 addresses (router IP) must be unique; and assigned to your router by the ISP; attenuation : The degeneration of a signal over distance on a network cable. encoding: 100MB Fast Ethernet => 4B/5B Gigabit Ethernet 1000Base-T => PAM-5 Routing algorithms: Routing Information Protocol (RIP) => distance vector ISP => link-state routing","title":"CS2204: unit 9: Revision"},{"location":"knowledge-base/cs2204-netwroks1/unit9/#cs2204-unit-9-revision","text":"Private IP address range: Class A: 10.0.0.0 \u2014 10.255.255.255 Class B: 172.16.0.0 \u2014 172.31.255.255 Class C: 192.168.0.0 \u2014 192.168.255.255 private IPv4 address are the IP address of the device in the local network; they exposed to the outer network behind the router's IP address. private IPv6 addresses can be reused for every network. the most common default private IP address format is 192.168.x.y; private IP can be found in the device settings and it is assigned to the device by the router. public IPv4 addresses (router IP) must be unique; and assigned to your router by the ISP; attenuation : The degeneration of a signal over distance on a network cable. encoding: 100MB Fast Ethernet => 4B/5B Gigabit Ethernet 1000Base-T => PAM-5 Routing algorithms: Routing Information Protocol (RIP) => distance vector ISP => link-state routing","title":"CS2204: unit 9: Revision"},{"location":"knowledge-base/general/","text":"Table of contents \u00b6 Advanced Design Patterns OOP Algorithms algorithms algorithms on graphs freecodeCamp algorithms MIT 6006 quick find quick union union find APIs API list Packages DataBases MongoDB Mongoose Redis Sequalize Data Structures binary trees dataStructures dis-joint sets hash tables heap linked lists priority queue queue stack trees Design CSS FrameWorks Bootstrap React Redux Interviews Javascript General Javascript OOP references Javascript Arrays Date Time Functions JS General Node cli Numbers Objects Strings Node js Node Argv stdout Programming Languages go python Software Development agile requirments terminoloy Think thinking logically games Tools bash docker git grunt linux shell scripting vim windows Web dom fonts meta pwa seo urls","title":"Table of contents"},{"location":"knowledge-base/general/#table-of-contents","text":"Advanced Design Patterns OOP Algorithms algorithms algorithms on graphs freecodeCamp algorithms MIT 6006 quick find quick union union find APIs API list Packages DataBases MongoDB Mongoose Redis Sequalize Data Structures binary trees dataStructures dis-joint sets hash tables heap linked lists priority queue queue stack trees Design CSS FrameWorks Bootstrap React Redux Interviews Javascript General Javascript OOP references Javascript Arrays Date Time Functions JS General Node cli Numbers Objects Strings Node js Node Argv stdout Programming Languages go python Software Development agile requirments terminoloy Think thinking logically games Tools bash docker git grunt linux shell scripting vim windows Web dom fonts meta pwa seo urls","title":"Table of contents"},{"location":"knowledge-base/general/continous-integration-vs-branching/","text":"Continuous Integration Vs Branching \u00b6 ci means working with our changes always visible to everyone in the team. branching is the opposite of ci. branching means hiding changes from the main part of the code. we should minimize the amount of changes in every PR. we should commit constantly throughout the day, at least once. we should be comfortable with committing partially complete features if we are doing continuous delivery CD , we should be comfortable with our partially complete features going into production . separate deployment from release , deploying if different from releasing a feature for people to use. partially complete features can be deployed, but they are not ready to be released yet. references \u00b6 https://www.youtube.com/watch?v=v4Ijkq6Myfc DevOps Handbook.","title":"Continuous Integration Vs Branching"},{"location":"knowledge-base/general/continous-integration-vs-branching/#continuous-integration-vs-branching","text":"ci means working with our changes always visible to everyone in the team. branching is the opposite of ci. branching means hiding changes from the main part of the code. we should minimize the amount of changes in every PR. we should commit constantly throughout the day, at least once. we should be comfortable with committing partially complete features if we are doing continuous delivery CD , we should be comfortable with our partially complete features going into production . separate deployment from release , deploying if different from releasing a feature for people to use. partially complete features can be deployed, but they are not ready to be released yet.","title":"Continuous Integration Vs Branching"},{"location":"knowledge-base/general/continous-integration-vs-branching/#references","text":"https://www.youtube.com/watch?v=v4Ijkq6Myfc DevOps Handbook.","title":"references"},{"location":"knowledge-base/general/advanced/","text":"","title":"Index"},{"location":"knowledge-base/general/advanced/design_patterns/","text":"Design Patterns \u00b6 intro \u00b6 useful for learn frameworks quickly. ease communications between teams. SRP: single responsiplity priciple, in oop, every class should have only one single responsabilty. OCP: open close priciple, our classes should be open for extention, closed for modification. Essentials \u00b6 package: the name where the main class is actually defined, entry point. class: object that holds properties and methods and can be instanciated. public: accessable every where. static: we can call the function directly without creating an instance of the classs. void: no return. main function takes one single argument as an array of strings holds everything you are passing from the command line. // Main.java package com.activityLog public class Main { public static void main ( String [] args ) { User me = new User ( \"Ahmad\" ); // calling the constructor of User. me . sayHello (); // hello, Ahmad } } // User.java package com.activityLog public class User { // property public String name ; // constructor public User ( String name ) { this . name = name ; } // Method public void sayHello (){ system . out . printLn ( \"Hello, \" + this . name ); } } class coupling \u00b6 when a class is depending on another class. eg. main class is coupling with (depending on) User class. if we change User class, Main class may broke, then you need to change it or recompile and redeploy. interface \u00b6 a contract that specifies the cabapilities that a class should provide. it is a class, holds essential functions of the class that they need to be redefined in the classes. you can't use the interface itself, but you can create a class from it then use this class. package activityLog // define interface, TaxCalculator.java public interface TaxCalculator { float calculateTax (); } // implement class from interface, TaxCalculator2020.java public class TaxCalculator2020 implements TaxCalculator { @override public float calculateTax () { return 1.0 ; } } // implement another class from interface, TaxCalculator2021.java public class TaxCalculator2021 implements TaxCalculator { @override public float calculateTax () { return 2.0 ; } } //main public class Main { public static void main ( String [] args ) { TaxCalculator calc = getTaxCalculator (); clac . calculateTax (); // what ever you change under the hood, this will stay working. } public static TaxCalculator getTaxCalculator () { if ( date . year () == 2020 ){ return new TaxCalculator2020 (); } else { return new TaxCalculator2021 (); } } } encapsulation \u00b6 use access modifiers to encapsulate class properties (class state). so, no direct changes to class properties except through functions (setters and getters) bundlling the data and the methods working on this data in one single class. and hide the state of the object inside the class. so, we protect object state from unwanted changes. then saving our program from going into invalid state. Abstraction \u00b6 hiding un-necessary details deep inside the class. this will reduce complexity, functions with less params. we can have few public main method in our class that fires several private functions that we don't see. eg. MyClass.sendEmail() and the send emain will be like this function sendEmail(){ getSender(); getRecever(); copyItToDb(); getSendingStatus() } so instead of calling those 4 methods manually, we fire sendEmail() will trigger them without us knowing that these functions actually existed. Inheretance \u00b6 a way of reusing code, so it eliminate redandant code. polymorphism \u00b6 many forms. the object can behaves in many forms depending on the context. create an abstract class with an abstract mthod then define those differently in each inhereted class. the extended class can behave as the super class (if it is not abstract or interface) or as his own class. UML \u00b6 unified moduling language draw charts to represent code. meaning of the three types of arrows in the diagrams: Memento pattern \u00b6 3 classes: originator, memento, caretaker. undo problem (mechanism). create our class eg. Editor, have 2 funcs: createState() , restoreState() . ----> originator . create another class eg. EditorState to save the state of our class. ------> memento push our states to a third class called history which stores list of our class states. have funcs: pushToHistory() , popFromHistory() . ------> caretaker state patteren \u00b6 naiive \u00b6 the app behavior changes with a state variable. this state variable should be private on its class and you need setters and getters to deal with it. you can pass this state by define it on top of your application then pass it to every other class in the app. at every function you should check for this state app and define different behavior depending on this state. pattern \u00b6 polymorphism is essential. 3 classes: context, state, concreteStateA, concreteStateB .. canvas problem you define a canvas class has the mouse listeners functions . ---> context. define a new class called tool wich also have the same functions. -----> state. the second class should be abstract and take the defentions of its functions from other classes like selection or bruch, those classes have the dedfinitons of those functions. ----> concreteStates. the second class (state) chooses the functions defentions depending on the app state.","title":"Design Patterns"},{"location":"knowledge-base/general/advanced/design_patterns/#design-patterns","text":"","title":"Design Patterns"},{"location":"knowledge-base/general/advanced/design_patterns/#intro","text":"useful for learn frameworks quickly. ease communications between teams. SRP: single responsiplity priciple, in oop, every class should have only one single responsabilty. OCP: open close priciple, our classes should be open for extention, closed for modification.","title":"intro"},{"location":"knowledge-base/general/advanced/design_patterns/#essentials","text":"package: the name where the main class is actually defined, entry point. class: object that holds properties and methods and can be instanciated. public: accessable every where. static: we can call the function directly without creating an instance of the classs. void: no return. main function takes one single argument as an array of strings holds everything you are passing from the command line. // Main.java package com.activityLog public class Main { public static void main ( String [] args ) { User me = new User ( \"Ahmad\" ); // calling the constructor of User. me . sayHello (); // hello, Ahmad } } // User.java package com.activityLog public class User { // property public String name ; // constructor public User ( String name ) { this . name = name ; } // Method public void sayHello (){ system . out . printLn ( \"Hello, \" + this . name ); } }","title":"Essentials"},{"location":"knowledge-base/general/advanced/design_patterns/#class-coupling","text":"when a class is depending on another class. eg. main class is coupling with (depending on) User class. if we change User class, Main class may broke, then you need to change it or recompile and redeploy.","title":"class coupling"},{"location":"knowledge-base/general/advanced/design_patterns/#interface","text":"a contract that specifies the cabapilities that a class should provide. it is a class, holds essential functions of the class that they need to be redefined in the classes. you can't use the interface itself, but you can create a class from it then use this class. package activityLog // define interface, TaxCalculator.java public interface TaxCalculator { float calculateTax (); } // implement class from interface, TaxCalculator2020.java public class TaxCalculator2020 implements TaxCalculator { @override public float calculateTax () { return 1.0 ; } } // implement another class from interface, TaxCalculator2021.java public class TaxCalculator2021 implements TaxCalculator { @override public float calculateTax () { return 2.0 ; } } //main public class Main { public static void main ( String [] args ) { TaxCalculator calc = getTaxCalculator (); clac . calculateTax (); // what ever you change under the hood, this will stay working. } public static TaxCalculator getTaxCalculator () { if ( date . year () == 2020 ){ return new TaxCalculator2020 (); } else { return new TaxCalculator2021 (); } } }","title":"interface"},{"location":"knowledge-base/general/advanced/design_patterns/#encapsulation","text":"use access modifiers to encapsulate class properties (class state). so, no direct changes to class properties except through functions (setters and getters) bundlling the data and the methods working on this data in one single class. and hide the state of the object inside the class. so, we protect object state from unwanted changes. then saving our program from going into invalid state.","title":"encapsulation"},{"location":"knowledge-base/general/advanced/design_patterns/#abstraction","text":"hiding un-necessary details deep inside the class. this will reduce complexity, functions with less params. we can have few public main method in our class that fires several private functions that we don't see. eg. MyClass.sendEmail() and the send emain will be like this function sendEmail(){ getSender(); getRecever(); copyItToDb(); getSendingStatus() } so instead of calling those 4 methods manually, we fire sendEmail() will trigger them without us knowing that these functions actually existed.","title":"Abstraction"},{"location":"knowledge-base/general/advanced/design_patterns/#inheretance","text":"a way of reusing code, so it eliminate redandant code.","title":"Inheretance"},{"location":"knowledge-base/general/advanced/design_patterns/#polymorphism","text":"many forms. the object can behaves in many forms depending on the context. create an abstract class with an abstract mthod then define those differently in each inhereted class. the extended class can behave as the super class (if it is not abstract or interface) or as his own class.","title":"polymorphism"},{"location":"knowledge-base/general/advanced/design_patterns/#uml","text":"unified moduling language draw charts to represent code. meaning of the three types of arrows in the diagrams:","title":"UML"},{"location":"knowledge-base/general/advanced/design_patterns/#memento-pattern","text":"3 classes: originator, memento, caretaker. undo problem (mechanism). create our class eg. Editor, have 2 funcs: createState() , restoreState() . ----> originator . create another class eg. EditorState to save the state of our class. ------> memento push our states to a third class called history which stores list of our class states. have funcs: pushToHistory() , popFromHistory() . ------> caretaker","title":"Memento pattern"},{"location":"knowledge-base/general/advanced/design_patterns/#state-patteren","text":"","title":"state patteren"},{"location":"knowledge-base/general/advanced/design_patterns/#naiive","text":"the app behavior changes with a state variable. this state variable should be private on its class and you need setters and getters to deal with it. you can pass this state by define it on top of your application then pass it to every other class in the app. at every function you should check for this state app and define different behavior depending on this state.","title":"naiive"},{"location":"knowledge-base/general/advanced/design_patterns/#pattern","text":"polymorphism is essential. 3 classes: context, state, concreteStateA, concreteStateB .. canvas problem you define a canvas class has the mouse listeners functions . ---> context. define a new class called tool wich also have the same functions. -----> state. the second class should be abstract and take the defentions of its functions from other classes like selection or bruch, those classes have the dedfinitons of those functions. ----> concreteStates. the second class (state) chooses the functions defentions depending on the app state.","title":"pattern"},{"location":"knowledge-base/general/advanced/oop/","text":"Object Oriented Programming \u00b6 some types of oop to look at \u00b6 object oriented modular programming incremental programming notes \u00b6 applying encapsulation will lead to Abstraction [2]. in c++: class is a structure that all its members are private by default [2]. summary about entity access control types in classes: Entity type Inhereted ? accessed by class methods ? accessed outside the class ? eg. setters and getters eg. directly as className.entity private - - - protected + + - public + + + intro \u00b6 structures (classes) is a collection of variables of possibly different types. an object is in instance of the structure, we can call it a structure itself. structures can be memberes of another structures you can't the same structure type as member of the same type, because we will get infinite loop of memory reservations. // c++ struct MyStruct { int x ; MyStruct y ; // this is wrong } Structures and pointers \u00b6 structures in memory: ![pointers in em] myStruct* is pointer to MyStruct type. Example: // c++ struct MyStruct { int x ; int y ; } MyStrcut p1 ; // defining a new object MyStruct * ptr // define a variable of type pointer to mystruct ptr = & p1 // assign memory addres of p1 to ptr * ptr = { 1 , 2 } // assign the data in memory addres &p1 to {1,2} // to access the pointer of y of p1 ptrY = & p1 + 4 // &p1 points to the first member of p1 whic is x, x takes 4 bytes, after 4 byets we reach y. // OR ptr -> y = 2 // assign data in y pointer to 2 // (&p1)->y = 2 pointers in structures: in the photo above, istead of reserving extra 54 bytes for the driver in t1 we are pointing to d1 which cost only 4 bytes. since we can not use a structure as member of the same structure type, we can use its pointer in our code as this: in this photo, t2.next points to t1 , although t1 and t2 are from the same type, we can't use t1 as member of t2, but we can point to it. writting to the heap starts with the word new , as int* ptr = new int // c++ struct MyStruct { int x ; int y ; } MyStruct * p1 = new MyStruct // we are writting to the heap // (dynamically allocating memory for the new struct) // now can dynamically handle p1 contents concepts \u00b6 entites: all members of class. abstraction: hide the un-necesary details of the class. entites can be [1]: fixed (like methods) don't change while the object interacts. state (like vars) change while the object interacts. member functions \u00b6 member functions are the methods of a class. member functions are fixed entries [1]. calling member functions. className . classFnc ( funcArg ) // className is the reciever // claassFunc is the member function // funcArg arguments that are passed to the method interfaces: simple layer hiding everything (eg. member function with no arguments) Access control of members in structures \u00b6 Crucial for data hiding or encapsulation [2]. public \u00b6 Member can be accessed from anywhere in program can be inhereted. private \u00b6 Member can be accessed only from member functions of same structure (class) . reading and writting can be done only using the class methods setters and getters . make sure that these getters and setters are public so they can be accessesd by other parts of the program. can not be inhereted. protected \u00b6 moderate private. can be accessed only in the defintion of the derived classes. Mutator functions \u00b6 Member functions that update values of data members that other functions are allowed to update [3]. Constructor \u00b6 Invoked automatically when an object of the class is allocated [3]. Convenient way to initialize data members . Just like any other member function Accepts optional input parameters Can be used to perform tasks other than initialization too [3]. class can have multiple constructors as long as each one has a distinct list of parameter types. When allocating an object of the class, the types of parameters passed to the constructor determine which constructor is invoked. constructors must be public [3]. calss V3 { double x , y , z ; // first constructor: normal. V3 ( double a , double b , double c ){ x = a ; y = b ; z = c ; return ; } // second constructor: initialazation, no parameters. // default constructor V3 (){ x = y = z = 0.0 ; return ; } // third constructor : with default values, some params are optional V3 ( double a = 0.0 , double b = 1.0 , double c = 2.0 ){ x = a ; y = b ; z = c ; return ; } // destructor ~ V3 () { if ( length () == 0.0 ) { cout << \u201c Zero vector !!! \u201d ; return ; } } V3 myObj1 ; // invoke second constructor V3 * myObj2 = new V3 ( 1.0 , 2.0 . 3.0 ); // invokes first constuctor V3 * myObj2 = new V3 ( 1.0 , 2.0 ); // invokes third constuctor Default constructor \u00b6 a constructor method without any params. when you define an array of type MyClass : the default constructor, the one without parameters, will be invoked when you intialize a new class without specyfying a constructr [3]. if no default constructor the compiler will provide one for you, but it might not be as you want [3]. If a non-default constructor is defined, but not a default constructor, C++ compiler will NOT provide a bare-bones default constructor, and the array will not defined, get an Error . Best practice: Always Define default constructors. copy constructor \u00b6 constructor method that take a class as parameter, and return a new class from the same type. the parameter class should be passed by reference , so the old class values will be copied to the new class. class V3 { int x , y , z ; V3 copyConstrutor ( const V3 & objFromSameClass ){ V3 v ; v . x = objFromSameClass . x v . y = objFromSameClass . y v . z = objFromSameClass . z return v ; } // another Example here: https://i.imgur.com/UfiJgY2.png } Destructor \u00b6 Invoked automatically when an object of the class is de-allocated. Convenient way to do book-keeping/cleaning-up before deallocating object [3]. Accepts no parameters. Can be used to perform other tasks before de-allocating object [3]. must be public. operator overloading \u00b6 create a special functions to be invoked after some operetors (eg: + - * /) [4]. class V3 { private : double x , y , z ; public : // operator + overloading V3 operator + ( const V3 & b ) { return V3 ( x + b . x , y + b . y , z + b . z ); } // operator * overloading V3 operator * ( const double factor ) { return V3 ( x * factor , y * factor , z * factor ); } }; // in main v1 = new V3 ( 1.0 , 2.0 , 3.0 ) v2 = new V3 ( 4.0 , 5.0 , 6.0 ) v4 = v1 * v2 // this will execute the function with the operator loading. Assignment Overloading \u00b6 We can re-define the assignment operator for a class/struct by defining the member function operator= [4]. Friend classes and functions \u00b6 A \u201cfriend\u201d declaration allows a class to explicitly allow specific non-member functions to access its private members. a function can be friend to several classess. a class can be friends with several functions. in the class defenetion I declare: ```c++ // c++ class V3 { // code friend ReturnType FuncName( ...Params ); // this will give the FuncName Access to private properties of the class V3 // OR friend class ClassName; // all functions of className will be friends with V3. } static data members \u00b6 members of class that will share its value with all objects of the class [4]. if this static data changed in one object, it will change with all other objects of this class [4]. Inheritance \u00b6 when there are some common features between mutliple classes, we can use a base class contains the common properties. then we extend each class with its own properties. Compositional Way \u00b6 the inhereted class cotains one property of the type base class . we need to access our new class, then the base class to get access to its properties. Inheritance Way \u00b6 the new class extends the previous one, so we can access the propieties of the base class directly as if they were a properties to the extended class. copmaring composional way to inhertance way. example of Bank accouts heirachy, where you need to use inheretance: https://i.imgur.com/XiXsPcR.png [4]. Access Control in Derived Classes \u00b6 the base class is the class that contains the common properties. the derived class is the new class that extends the base one. the propereties types in base class can be: 1.public 2.private 3.protected. the type of derivation (inhertenace) can also be: 1.public 2.private 3.protected. the comination of propert type and inhertenace type can control the accessebilty of the base property in the derived class. From both types, the one with more privacy and less accessbility will be dominated as : private > protected > public . summary of the inheretance depending on both types [4]: Methods Inheretance \u00b6 methods can be redefined in the derived class. to call the method of the base (super) class after you redefine you can do: c++ class savings : public base { public: int age; long int ATM; void printInfo() { base::printInfo() // when call savings.printInfo() will execute the both methods from base and savings. cout << \"\\nPrinting in savings: \\n\"; cout << age << \", \" << ATM << endl; } }; instantianting a new derived class will call both constructors of the default constructor of the base then the constructor of the derived class. if the base class doesn't have default construcor (eg. its constructor has arguments) you need to explicitly call the base constructor when constructing the derived class. polymorphism \u00b6 a derived class can be used as its shape or as base class because it has all data in the base class. so, we have 2 shapes or forms of the derived class, that's polymorphism. inhereted methods that are redefined in the derived class can be used as its original defention or its second defenition, this is also polymorphism. example here: https://i.imgur.com/d6Wz17A.png virtual functions \u00b6 implements polymorphism in the function calls. functions defined in the base class, but they need to be redefined in the dervied class in order to work. example here: https://i.imgur.com/DPbIAqe.png Abstract class \u00b6 A class that cannot be instantiated directly. instantiation of this class will give compiler error. Implemented as a class that has one or more pure virtual functions As. virtual void FuncName() = 0; Which should be overridden by member function definitions of derived class. used when using the base class has no meaningfull meaning. Example (Bank account): A person does not have just a bank account. It is either a savings bank account or a current bank account Instantiating class \u2018base\u2019 by itself has no meaningful purpose References \u00b6 [1] IITBombayX: CS101.2x, edx 1 [2] IITBombayX: CS101.2x, edx 2 [3] IITBombayX: CS101.2x, edx 3 [4] IITBombayX: CS101.2x, edx 4","title":"Object Oriented Programming"},{"location":"knowledge-base/general/advanced/oop/#object-oriented-programming","text":"","title":"Object Oriented Programming"},{"location":"knowledge-base/general/advanced/oop/#some-types-of-oop-to-look-at","text":"object oriented modular programming incremental programming","title":"some types of oop to look at"},{"location":"knowledge-base/general/advanced/oop/#notes","text":"applying encapsulation will lead to Abstraction [2]. in c++: class is a structure that all its members are private by default [2]. summary about entity access control types in classes: Entity type Inhereted ? accessed by class methods ? accessed outside the class ? eg. setters and getters eg. directly as className.entity private - - - protected + + - public + + +","title":"notes"},{"location":"knowledge-base/general/advanced/oop/#intro","text":"structures (classes) is a collection of variables of possibly different types. an object is in instance of the structure, we can call it a structure itself. structures can be memberes of another structures you can't the same structure type as member of the same type, because we will get infinite loop of memory reservations. // c++ struct MyStruct { int x ; MyStruct y ; // this is wrong }","title":"intro"},{"location":"knowledge-base/general/advanced/oop/#structures-and-pointers","text":"structures in memory: ![pointers in em] myStruct* is pointer to MyStruct type. Example: // c++ struct MyStruct { int x ; int y ; } MyStrcut p1 ; // defining a new object MyStruct * ptr // define a variable of type pointer to mystruct ptr = & p1 // assign memory addres of p1 to ptr * ptr = { 1 , 2 } // assign the data in memory addres &p1 to {1,2} // to access the pointer of y of p1 ptrY = & p1 + 4 // &p1 points to the first member of p1 whic is x, x takes 4 bytes, after 4 byets we reach y. // OR ptr -> y = 2 // assign data in y pointer to 2 // (&p1)->y = 2 pointers in structures: in the photo above, istead of reserving extra 54 bytes for the driver in t1 we are pointing to d1 which cost only 4 bytes. since we can not use a structure as member of the same structure type, we can use its pointer in our code as this: in this photo, t2.next points to t1 , although t1 and t2 are from the same type, we can't use t1 as member of t2, but we can point to it. writting to the heap starts with the word new , as int* ptr = new int // c++ struct MyStruct { int x ; int y ; } MyStruct * p1 = new MyStruct // we are writting to the heap // (dynamically allocating memory for the new struct) // now can dynamically handle p1 contents","title":"Structures and pointers"},{"location":"knowledge-base/general/advanced/oop/#concepts","text":"entites: all members of class. abstraction: hide the un-necesary details of the class. entites can be [1]: fixed (like methods) don't change while the object interacts. state (like vars) change while the object interacts.","title":"concepts"},{"location":"knowledge-base/general/advanced/oop/#member-functions","text":"member functions are the methods of a class. member functions are fixed entries [1]. calling member functions. className . classFnc ( funcArg ) // className is the reciever // claassFunc is the member function // funcArg arguments that are passed to the method interfaces: simple layer hiding everything (eg. member function with no arguments)","title":"member functions"},{"location":"knowledge-base/general/advanced/oop/#access-control-of-members-in-structures","text":"Crucial for data hiding or encapsulation [2].","title":"Access control of members in structures"},{"location":"knowledge-base/general/advanced/oop/#public","text":"Member can be accessed from anywhere in program can be inhereted.","title":"public"},{"location":"knowledge-base/general/advanced/oop/#private","text":"Member can be accessed only from member functions of same structure (class) . reading and writting can be done only using the class methods setters and getters . make sure that these getters and setters are public so they can be accessesd by other parts of the program. can not be inhereted.","title":"private"},{"location":"knowledge-base/general/advanced/oop/#protected","text":"moderate private. can be accessed only in the defintion of the derived classes.","title":"protected"},{"location":"knowledge-base/general/advanced/oop/#mutator-functions","text":"Member functions that update values of data members that other functions are allowed to update [3].","title":"Mutator functions"},{"location":"knowledge-base/general/advanced/oop/#constructor","text":"Invoked automatically when an object of the class is allocated [3]. Convenient way to initialize data members . Just like any other member function Accepts optional input parameters Can be used to perform tasks other than initialization too [3]. class can have multiple constructors as long as each one has a distinct list of parameter types. When allocating an object of the class, the types of parameters passed to the constructor determine which constructor is invoked. constructors must be public [3]. calss V3 { double x , y , z ; // first constructor: normal. V3 ( double a , double b , double c ){ x = a ; y = b ; z = c ; return ; } // second constructor: initialazation, no parameters. // default constructor V3 (){ x = y = z = 0.0 ; return ; } // third constructor : with default values, some params are optional V3 ( double a = 0.0 , double b = 1.0 , double c = 2.0 ){ x = a ; y = b ; z = c ; return ; } // destructor ~ V3 () { if ( length () == 0.0 ) { cout << \u201c Zero vector !!! \u201d ; return ; } } V3 myObj1 ; // invoke second constructor V3 * myObj2 = new V3 ( 1.0 , 2.0 . 3.0 ); // invokes first constuctor V3 * myObj2 = new V3 ( 1.0 , 2.0 ); // invokes third constuctor","title":"Constructor"},{"location":"knowledge-base/general/advanced/oop/#default-constructor","text":"a constructor method without any params. when you define an array of type MyClass : the default constructor, the one without parameters, will be invoked when you intialize a new class without specyfying a constructr [3]. if no default constructor the compiler will provide one for you, but it might not be as you want [3]. If a non-default constructor is defined, but not a default constructor, C++ compiler will NOT provide a bare-bones default constructor, and the array will not defined, get an Error . Best practice: Always Define default constructors.","title":"Default constructor"},{"location":"knowledge-base/general/advanced/oop/#copy-constructor","text":"constructor method that take a class as parameter, and return a new class from the same type. the parameter class should be passed by reference , so the old class values will be copied to the new class. class V3 { int x , y , z ; V3 copyConstrutor ( const V3 & objFromSameClass ){ V3 v ; v . x = objFromSameClass . x v . y = objFromSameClass . y v . z = objFromSameClass . z return v ; } // another Example here: https://i.imgur.com/UfiJgY2.png }","title":"copy constructor"},{"location":"knowledge-base/general/advanced/oop/#destructor","text":"Invoked automatically when an object of the class is de-allocated. Convenient way to do book-keeping/cleaning-up before deallocating object [3]. Accepts no parameters. Can be used to perform other tasks before de-allocating object [3]. must be public.","title":"Destructor"},{"location":"knowledge-base/general/advanced/oop/#operator-overloading","text":"create a special functions to be invoked after some operetors (eg: + - * /) [4]. class V3 { private : double x , y , z ; public : // operator + overloading V3 operator + ( const V3 & b ) { return V3 ( x + b . x , y + b . y , z + b . z ); } // operator * overloading V3 operator * ( const double factor ) { return V3 ( x * factor , y * factor , z * factor ); } }; // in main v1 = new V3 ( 1.0 , 2.0 , 3.0 ) v2 = new V3 ( 4.0 , 5.0 , 6.0 ) v4 = v1 * v2 // this will execute the function with the operator loading.","title":"operator overloading"},{"location":"knowledge-base/general/advanced/oop/#assignment-overloading","text":"We can re-define the assignment operator for a class/struct by defining the member function operator= [4].","title":"Assignment Overloading"},{"location":"knowledge-base/general/advanced/oop/#friend-classes-and-functions","text":"A \u201cfriend\u201d declaration allows a class to explicitly allow specific non-member functions to access its private members. a function can be friend to several classess. a class can be friends with several functions. in the class defenetion I declare: ```c++ // c++ class V3 { // code friend ReturnType FuncName( ...Params ); // this will give the FuncName Access to private properties of the class V3 // OR friend class ClassName; // all functions of className will be friends with V3. }","title":"Friend classes and functions"},{"location":"knowledge-base/general/advanced/oop/#static-data-members","text":"members of class that will share its value with all objects of the class [4]. if this static data changed in one object, it will change with all other objects of this class [4].","title":"static data members"},{"location":"knowledge-base/general/advanced/oop/#inheritance","text":"when there are some common features between mutliple classes, we can use a base class contains the common properties. then we extend each class with its own properties.","title":"Inheritance"},{"location":"knowledge-base/general/advanced/oop/#compositional-way","text":"the inhereted class cotains one property of the type base class . we need to access our new class, then the base class to get access to its properties.","title":"Compositional Way"},{"location":"knowledge-base/general/advanced/oop/#inheritance-way","text":"the new class extends the previous one, so we can access the propieties of the base class directly as if they were a properties to the extended class. copmaring composional way to inhertance way. example of Bank accouts heirachy, where you need to use inheretance: https://i.imgur.com/XiXsPcR.png [4].","title":"Inheritance Way"},{"location":"knowledge-base/general/advanced/oop/#access-control-in-derived-classes","text":"the base class is the class that contains the common properties. the derived class is the new class that extends the base one. the propereties types in base class can be: 1.public 2.private 3.protected. the type of derivation (inhertenace) can also be: 1.public 2.private 3.protected. the comination of propert type and inhertenace type can control the accessebilty of the base property in the derived class. From both types, the one with more privacy and less accessbility will be dominated as : private > protected > public . summary of the inheretance depending on both types [4]:","title":"Access Control in Derived Classes"},{"location":"knowledge-base/general/advanced/oop/#methods-inheretance","text":"methods can be redefined in the derived class. to call the method of the base (super) class after you redefine you can do: c++ class savings : public base { public: int age; long int ATM; void printInfo() { base::printInfo() // when call savings.printInfo() will execute the both methods from base and savings. cout << \"\\nPrinting in savings: \\n\"; cout << age << \", \" << ATM << endl; } }; instantianting a new derived class will call both constructors of the default constructor of the base then the constructor of the derived class. if the base class doesn't have default construcor (eg. its constructor has arguments) you need to explicitly call the base constructor when constructing the derived class.","title":"Methods Inheretance"},{"location":"knowledge-base/general/advanced/oop/#polymorphism","text":"a derived class can be used as its shape or as base class because it has all data in the base class. so, we have 2 shapes or forms of the derived class, that's polymorphism. inhereted methods that are redefined in the derived class can be used as its original defention or its second defenition, this is also polymorphism. example here: https://i.imgur.com/d6Wz17A.png","title":"polymorphism"},{"location":"knowledge-base/general/advanced/oop/#virtual-functions","text":"implements polymorphism in the function calls. functions defined in the base class, but they need to be redefined in the dervied class in order to work. example here: https://i.imgur.com/DPbIAqe.png","title":"virtual functions"},{"location":"knowledge-base/general/advanced/oop/#abstract-class","text":"A class that cannot be instantiated directly. instantiation of this class will give compiler error. Implemented as a class that has one or more pure virtual functions As. virtual void FuncName() = 0; Which should be overridden by member function definitions of derived class. used when using the base class has no meaningfull meaning. Example (Bank account): A person does not have just a bank account. It is either a savings bank account or a current bank account Instantiating class \u2018base\u2019 by itself has no meaningful purpose","title":"Abstract class"},{"location":"knowledge-base/general/advanced/oop/#references","text":"[1] IITBombayX: CS101.2x, edx 1 [2] IITBombayX: CS101.2x, edx 2 [3] IITBombayX: CS101.2x, edx 3 [4] IITBombayX: CS101.2x, edx 4","title":"References"},{"location":"knowledge-base/general/algorithms/","text":"Data Structures and Algorithms Notes \u00b6 resources \u00b6 Data Structures and Algorithms speialization coursera Algorithmic Toolbox : My Notes -- coursera FreeCodeCamp Algorithms section My Notes -- FreecodeCamp MIT 6.006, Introduction to Algorithms: MY NOTES -- MIT 6.006 Algorithms1 MY NOETS -- cousera Medium Atricle: https://medium.com/siliconwat/algorithms-in-javascript-b0bed68f4038 Khan Academy: https://www.khanacademy.org/computing/computer-science/algorithms Visualize \u00b6 David Galles Visualization here: https://www.cs.usfca.edu/~galles/visualization/about.html toptal visualization here: https://www.toptal.com/developers/sorting-algorithms Big o Cheatsheet here: https://www.bigocheatsheet.com/","title":"Data Structures and Algorithms Notes"},{"location":"knowledge-base/general/algorithms/#data-structures-and-algorithms-notes","text":"","title":"Data Structures and Algorithms Notes"},{"location":"knowledge-base/general/algorithms/#resources","text":"Data Structures and Algorithms speialization coursera Algorithmic Toolbox : My Notes -- coursera FreeCodeCamp Algorithms section My Notes -- FreecodeCamp MIT 6.006, Introduction to Algorithms: MY NOTES -- MIT 6.006 Algorithms1 MY NOETS -- cousera Medium Atricle: https://medium.com/siliconwat/algorithms-in-javascript-b0bed68f4038 Khan Academy: https://www.khanacademy.org/computing/computer-science/algorithms","title":"resources"},{"location":"knowledge-base/general/algorithms/#visualize","text":"David Galles Visualization here: https://www.cs.usfca.edu/~galles/visualization/about.html toptal visualization here: https://www.toptal.com/developers/sorting-algorithms Big o Cheatsheet here: https://www.bigocheatsheet.com/","title":"Visualize"},{"location":"knowledge-base/general/algorithms/algorithms-on-graphs/","text":"Algorithms on Graphs \u00b6 graphs are used in: represent Internet pages and the connections between them. maps social networks. An \\(undirected\\) Graph is a collection V of vertices, and a collection E of edges each of which connects a pair of vertices. Drawing Graphs: Vertices: Points. Edges: Lines. Loops connect a vertex to itself. Multiple edges between same vertices. Represinting Graphs: List of all edges: Edges: (A, B), (A, C), (A,D), (C,D) Adjacency Matrix: Matrix. Entries 1 if there is an edge, 0 if there is not. Adjacency List : For each vertex, a list of adjacent vertices. time cost for each of the previous Graph representaions: Week1: Exploring Undirected Graphs \u00b6 A path in a graph G is a sequence of vertices v0, v1, . . . , vn so that for all i, \\(vi , vi+1\\) is an edge of G. operations on graphs: Reachability: outputs The collection of vertices v of G so that there is a path from s to v. connectivity: outputs The connected components of G. Reachability \\(All components\\) \u00b6 All Component ( s : ): DiscoveredNodes = [] while there is an edge e leaving DiscoveredNodes that has not been explored : add vertex at other end of e to DiscoveredNodes return DiscoveredNodes Visit Markers : To keep track of vertices found we Give each vertex boolean visited \\(v\\) . Unprocessed Vertices: Keep a list of vertices with edges left to check. Depth First Ordering \u00b6 We will explore new edges in Depth First order. We will follow a long path forward, only backtracking when we hit a dead end . Explore ( v : vertix ): visited ( v ) = true for ( v , w ) \u2208 E : # E contains all niebourghs w of v if not visited ( w ): Explore ( w ) Connectivity \u00b6 The vertices of a graph G can be partitioned into Connected Components so that v is reachable from w if and only if they are in the same connected component . Depth - First - Search ( G : graph ): for all v \u2208 V mark v unvisited : #initialize all vertices as un visted using counter cc cc = 1 for v \u2208 V : # V contains all vertices v of the single connected component V if not visited ( v ): ` Explore ( v ) cc = cc + 1 # increase the counter so it's giving a different mark to every vertices # in a single component Explore: Explore ( v : vertix ): visited ( v ) = true # mark that this vertix has visted CCnum ( v ) = cc # mark all vertices in the same component with the same counter for ( v , w ) \u2208 E : # check if there are still unvisted vertices in v niebourghs if not visited ( w ): Explore ( w ) Each new explore() finds new single connected component . Runtime O(|V: vertices | + |E: edges|) . Previsit and Postvisit Functions \u00b6 you might want to track more data while you going over the vertices, these data can be tracked using preVistit() and postVisit() functions. adding pre and post visits functions to DSF() in the Explore() : Explore ( v : vertix ): visited ( v ) = true previsit ( v ) # execute pre visit for ( v , w ) \u2208 E : # adjacents if not visited ( w ): explore ( w ) postvisit ( v ) ####### def pre visit and posy vists ######## clock = 1 previsit ( v ): pre ( v ) = clock clock = clock + 1 postvisit ( v ): post ( v ) = clock clock = clock + 1 week2: Directed Graphs \u00b6 Directed Graph : a graph where each edge has a start vertex and an end vertex. Directed graphs might be used to represent: Streets with one-way roads. Links between webpages. Followers on social network. Dependencies between tasks Directed DFS : Only follow directed edges. explore(v) finds all vertices reachable from v. Can still compute pre- and postorderings. cycle : A cycle in a graph G is a sequence of vertices v1, v2, . . . , vn so that (v1, v2),(v2, v3), . . . ,(vn\u22121, vn),(vn, v1) are all edges. If G contains a cycle, it cannot be linearly ordered . DAGs : A directed graph G is a Directed Acyclic Graph \\(or DAG\\) if it has no cycles. Example: only A is a DAG Any DAG can be linearly ordered. Topological Sort \u00b6 Last Vertex : a vertex that cannot have any edges pointing out of it. source : a vertex with no incoming edges. sink : a vertex with no outgoing edges, simply it's a last vertix . Example: red vertices are sinks topological sort idea: 1. Find sink. 2. Put that sink at end of order. 3. Remove the sink from graph. 4. Repeat. finding a sink: to find a sink we need to follow the path pointing to this sink until we : Cannot extend => we found a sink. Repeat a previous vertex => we found a cycle. Topological sort algorithm: LinearOrder ( G ): while G non - empty : Follow a path until cannot extend Find sink v Put v at end of order Remove v from G LinearOrder ( G ) # G is now less by 1 vertix. ## Runtime O(|V|^2) weaknesses in the previous algorithm: Retrace same path every time. every time we start from the begining, sowe can: Instead only back up as far as necessary . Optimized topological sort \u00b6 TopologicalSort ( G ): DFS ( G ) # run depth first search with pre and post order functions sort vertices by reverse post - order # greater post-order value comes first in the output. If G is a DAG, with an edge u -> v, so: post(u) > post(v) .","title":"Algorithms on Graphs"},{"location":"knowledge-base/general/algorithms/algorithms-on-graphs/#algorithms-on-graphs","text":"graphs are used in: represent Internet pages and the connections between them. maps social networks. An \\(undirected\\) Graph is a collection V of vertices, and a collection E of edges each of which connects a pair of vertices. Drawing Graphs: Vertices: Points. Edges: Lines. Loops connect a vertex to itself. Multiple edges between same vertices. Represinting Graphs: List of all edges: Edges: (A, B), (A, C), (A,D), (C,D) Adjacency Matrix: Matrix. Entries 1 if there is an edge, 0 if there is not. Adjacency List : For each vertex, a list of adjacent vertices. time cost for each of the previous Graph representaions:","title":"Algorithms on Graphs"},{"location":"knowledge-base/general/algorithms/algorithms-on-graphs/#week1-exploring-undirected-graphs","text":"A path in a graph G is a sequence of vertices v0, v1, . . . , vn so that for all i, \\(vi , vi+1\\) is an edge of G. operations on graphs: Reachability: outputs The collection of vertices v of G so that there is a path from s to v. connectivity: outputs The connected components of G.","title":"Week1: Exploring Undirected Graphs"},{"location":"knowledge-base/general/algorithms/algorithms-on-graphs/#reachability-all-components","text":"All Component ( s : ): DiscoveredNodes = [] while there is an edge e leaving DiscoveredNodes that has not been explored : add vertex at other end of e to DiscoveredNodes return DiscoveredNodes Visit Markers : To keep track of vertices found we Give each vertex boolean visited \\(v\\) . Unprocessed Vertices: Keep a list of vertices with edges left to check.","title":"Reachability \\(All components\\)"},{"location":"knowledge-base/general/algorithms/algorithms-on-graphs/#depth-first-ordering","text":"We will explore new edges in Depth First order. We will follow a long path forward, only backtracking when we hit a dead end . Explore ( v : vertix ): visited ( v ) = true for ( v , w ) \u2208 E : # E contains all niebourghs w of v if not visited ( w ): Explore ( w )","title":"Depth First Ordering"},{"location":"knowledge-base/general/algorithms/algorithms-on-graphs/#connectivity","text":"The vertices of a graph G can be partitioned into Connected Components so that v is reachable from w if and only if they are in the same connected component . Depth - First - Search ( G : graph ): for all v \u2208 V mark v unvisited : #initialize all vertices as un visted using counter cc cc = 1 for v \u2208 V : # V contains all vertices v of the single connected component V if not visited ( v ): ` Explore ( v ) cc = cc + 1 # increase the counter so it's giving a different mark to every vertices # in a single component Explore: Explore ( v : vertix ): visited ( v ) = true # mark that this vertix has visted CCnum ( v ) = cc # mark all vertices in the same component with the same counter for ( v , w ) \u2208 E : # check if there are still unvisted vertices in v niebourghs if not visited ( w ): Explore ( w ) Each new explore() finds new single connected component . Runtime O(|V: vertices | + |E: edges|) .","title":"Connectivity"},{"location":"knowledge-base/general/algorithms/algorithms-on-graphs/#previsit-and-postvisit-functions","text":"you might want to track more data while you going over the vertices, these data can be tracked using preVistit() and postVisit() functions. adding pre and post visits functions to DSF() in the Explore() : Explore ( v : vertix ): visited ( v ) = true previsit ( v ) # execute pre visit for ( v , w ) \u2208 E : # adjacents if not visited ( w ): explore ( w ) postvisit ( v ) ####### def pre visit and posy vists ######## clock = 1 previsit ( v ): pre ( v ) = clock clock = clock + 1 postvisit ( v ): post ( v ) = clock clock = clock + 1","title":"Previsit and Postvisit Functions"},{"location":"knowledge-base/general/algorithms/algorithms-on-graphs/#week2-directed-graphs","text":"Directed Graph : a graph where each edge has a start vertex and an end vertex. Directed graphs might be used to represent: Streets with one-way roads. Links between webpages. Followers on social network. Dependencies between tasks Directed DFS : Only follow directed edges. explore(v) finds all vertices reachable from v. Can still compute pre- and postorderings. cycle : A cycle in a graph G is a sequence of vertices v1, v2, . . . , vn so that (v1, v2),(v2, v3), . . . ,(vn\u22121, vn),(vn, v1) are all edges. If G contains a cycle, it cannot be linearly ordered . DAGs : A directed graph G is a Directed Acyclic Graph \\(or DAG\\) if it has no cycles. Example: only A is a DAG Any DAG can be linearly ordered.","title":"week2: Directed Graphs"},{"location":"knowledge-base/general/algorithms/algorithms-on-graphs/#topological-sort","text":"Last Vertex : a vertex that cannot have any edges pointing out of it. source : a vertex with no incoming edges. sink : a vertex with no outgoing edges, simply it's a last vertix . Example: red vertices are sinks topological sort idea: 1. Find sink. 2. Put that sink at end of order. 3. Remove the sink from graph. 4. Repeat. finding a sink: to find a sink we need to follow the path pointing to this sink until we : Cannot extend => we found a sink. Repeat a previous vertex => we found a cycle. Topological sort algorithm: LinearOrder ( G ): while G non - empty : Follow a path until cannot extend Find sink v Put v at end of order Remove v from G LinearOrder ( G ) # G is now less by 1 vertix. ## Runtime O(|V|^2) weaknesses in the previous algorithm: Retrace same path every time. every time we start from the begining, sowe can: Instead only back up as far as necessary .","title":"Topological Sort"},{"location":"knowledge-base/general/algorithms/algorithms-on-graphs/#optimized-topological-sort","text":"TopologicalSort ( G ): DFS ( G ) # run depth first search with pre and post order functions sort vertices by reverse post - order # greater post-order value comes first in the output. If G is a DAG, with an edge u -> v, so: post(u) > post(v) .","title":"Optimized topological sort"},{"location":"knowledge-base/general/algorithms/algorithms/","text":"Algorithms \u00b6 Alogrithms 1&2 - university of Princeton : couresera page , My notes are in this file. data structures and algorithms : coursera page My notes Repo intro Best resource: https://algs4.cs.princeton.edu/home/ Resource: https://introcs.cs.princeton.edu/java/home/ \u201c An algorithm must be seen to be believed. \u201d \u2014 Donald Knuth \u201c Algorithms + Data Structures = Programs. \u201d \u2014 Niklaus Wirth \u201c Algorithms: a common language for nature, human, and computer. \u201d \u2014 Avi Wigderson General notes \u00b6 cost : N^2 > N > lg N > 1 . cost: quadratic > linear > logaritmic > const cost for N=1000 : N^2 = 1000000 - quadratic. N = 1000 - lenear lg N = 10 - logaritmic lg* N = 0-5 - almost const 1 = fixed number - const in practice: you start using quick sort algorithm, if you find it a bit slow: you stop and chamge to heap sort studying algorithms: algorithms stuff knapsack problem here: https://github.com/ahmad-ali14/data-structures-and-algorithms/blob/master/algorithm-toolbox/week3/maximum_loot.js algorithm for making a good algorithm: always start with a naiive algorithm and make it works, normally slow. next find a standard toolbox to help you: greedy algorithm. divide and conqur. dynamic programming. optimize your algorithm. more on algorithms here: https://github.com/aa947/data-structures-and-algorithms/tree/master/algorithm-toolbox greedy algorithm : 1 - make the first move . 2 - test if it ' s a safe move or start from the begining 3 - test if that move is optimized or optimize it the most 4 - you get a sub - problem handle it with the same approach .","title":"Algorithms"},{"location":"knowledge-base/general/algorithms/algorithms/#algorithms","text":"Alogrithms 1&2 - university of Princeton : couresera page , My notes are in this file. data structures and algorithms : coursera page My notes Repo intro Best resource: https://algs4.cs.princeton.edu/home/ Resource: https://introcs.cs.princeton.edu/java/home/ \u201c An algorithm must be seen to be believed. \u201d \u2014 Donald Knuth \u201c Algorithms + Data Structures = Programs. \u201d \u2014 Niklaus Wirth \u201c Algorithms: a common language for nature, human, and computer. \u201d \u2014 Avi Wigderson","title":"Algorithms"},{"location":"knowledge-base/general/algorithms/algorithms/#general-notes","text":"cost : N^2 > N > lg N > 1 . cost: quadratic > linear > logaritmic > const cost for N=1000 : N^2 = 1000000 - quadratic. N = 1000 - lenear lg N = 10 - logaritmic lg* N = 0-5 - almost const 1 = fixed number - const in practice: you start using quick sort algorithm, if you find it a bit slow: you stop and chamge to heap sort studying algorithms: algorithms stuff knapsack problem here: https://github.com/ahmad-ali14/data-structures-and-algorithms/blob/master/algorithm-toolbox/week3/maximum_loot.js algorithm for making a good algorithm: always start with a naiive algorithm and make it works, normally slow. next find a standard toolbox to help you: greedy algorithm. divide and conqur. dynamic programming. optimize your algorithm. more on algorithms here: https://github.com/aa947/data-structures-and-algorithms/tree/master/algorithm-toolbox greedy algorithm : 1 - make the first move . 2 - test if it ' s a safe move or start from the begining 3 - test if that move is optimized or optimize it the most 4 - you get a sub - problem handle it with the same approach .","title":"General notes"},{"location":"knowledge-base/general/algorithms/freecodecamp/","text":"Free Code Camp Algorithms section \u00b6 symetric differnce: https://www.freecodecamp.org/learn/coding-interview-prep/algorithms/find-the-symmetric-difference inventory update : https://www.freecodecamp.org/learn/coding-interview-prep/algorithms/inventory-update","title":"Free Code Camp Algorithms section"},{"location":"knowledge-base/general/algorithms/freecodecamp/#free-code-camp-algorithms-section","text":"symetric differnce: https://www.freecodecamp.org/learn/coding-interview-prep/algorithms/find-the-symmetric-difference inventory update : https://www.freecodecamp.org/learn/coding-interview-prep/algorithms/inventory-update","title":"Free Code Camp Algorithms section"},{"location":"knowledge-base/general/algorithms/mit6006/","text":"Algorithms Notes \u00b6 This is a summary of the course on MIT open source here Lecture 2: Models of Computation pdf \u00b6 for x in L costs linear time o(n) A1 + A2 adding 2 arrays, creates an empty array then add every elemnt to it, costs 1 + o(A1) + o(A2) Arr.length costs constant o(1) Arr.sort() costs n * log n Document Distance Problem \u2014 compute d \\(D1, D2\\) \u00b6 split each document into words count word frequencies \\(document vectors\\) compute dot product \\(& divide\\) lecture 3: Insertion Sort, Merge Sort pdf \u00b6 sorting make things easier, like binary srearch and find the median Finding the median in an array \u00b6 simply sort the array, and look to the elemnt at n/2 . costs contatnt time o(1) if you start from sorted array. Insertion sort \u00b6 insert key A[j] into the \\(already sorted\\) sub-array A[1 .. j-1]. by pairwise key-swaps down to its right position. costs o(n^2) cause, o(n^2) for compares, o(n^2) for the swaps. => o(n) + o(n) = o(n^2) Binary Insertion sort \u00b6 insert key A[j] into the \\(already sorted\\) sub-array A[1 ..j-1]. Use binary search to find the right position. costs \\(Complexity\\) : \u0398(n log n) for comparisons, and \u0398(n^2) for swaps. Merge Sort \u00b6 recurrsion. split => sort splits => merge sorted splits. need to copy the array first, so it tskes more space than insert sort. costs o(n) extra aux space. costs o(n log n) In-place sorting \u00b6 do sorting without copying the arrays, costs o(1) auxiliary space. used in insertion sort. Heap \u00b6 priority queue","title":"Algorithms Notes"},{"location":"knowledge-base/general/algorithms/mit6006/#algorithms-notes","text":"This is a summary of the course on MIT open source here","title":"Algorithms Notes"},{"location":"knowledge-base/general/algorithms/mit6006/#lecture-2-models-of-computation-pdf","text":"for x in L costs linear time o(n) A1 + A2 adding 2 arrays, creates an empty array then add every elemnt to it, costs 1 + o(A1) + o(A2) Arr.length costs constant o(1) Arr.sort() costs n * log n","title":"Lecture 2: Models of Computation pdf"},{"location":"knowledge-base/general/algorithms/mit6006/#document-distance-problem-compute-dd1-d2","text":"split each document into words count word frequencies \\(document vectors\\) compute dot product \\(& divide\\)","title":"Document Distance Problem \u2014 compute d\\(D1, D2\\)"},{"location":"knowledge-base/general/algorithms/mit6006/#lecture-3-insertion-sort-merge-sort-pdf","text":"sorting make things easier, like binary srearch and find the median","title":"lecture 3: Insertion Sort, Merge Sort pdf"},{"location":"knowledge-base/general/algorithms/mit6006/#finding-the-median-in-an-array","text":"simply sort the array, and look to the elemnt at n/2 . costs contatnt time o(1) if you start from sorted array.","title":"Finding the median in an array"},{"location":"knowledge-base/general/algorithms/mit6006/#insertion-sort","text":"insert key A[j] into the \\(already sorted\\) sub-array A[1 .. j-1]. by pairwise key-swaps down to its right position. costs o(n^2) cause, o(n^2) for compares, o(n^2) for the swaps. => o(n) + o(n) = o(n^2)","title":"Insertion sort"},{"location":"knowledge-base/general/algorithms/mit6006/#binary-insertion-sort","text":"insert key A[j] into the \\(already sorted\\) sub-array A[1 ..j-1]. Use binary search to find the right position. costs \\(Complexity\\) : \u0398(n log n) for comparisons, and \u0398(n^2) for swaps.","title":"Binary Insertion sort"},{"location":"knowledge-base/general/algorithms/mit6006/#merge-sort","text":"recurrsion. split => sort splits => merge sorted splits. need to copy the array first, so it tskes more space than insert sort. costs o(n) extra aux space. costs o(n log n)","title":"Merge Sort"},{"location":"knowledge-base/general/algorithms/mit6006/#in-place-sorting","text":"do sorting without copying the arrays, costs o(1) auxiliary space. used in insertion sort.","title":"In-place sorting"},{"location":"knowledge-base/general/algorithms/mit6006/#heap","text":"priority queue","title":"Heap"},{"location":"knowledge-base/general/algorithms/quick_find/","text":"quick-find \u00b6 eager approach we follow the ids, until we find same id. 2 elements are connected if they have the same id. find easy, union is slow. cost N^2 resource: https://www.coursera.org/learn/algorithms-part1/lecture/EcF3P/quick-find Example: public class QuickFindUF { private int [] id ; public QuickFindUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } public boolean connected ( int p , int q ) { return id [ p ] == id [ q ] ; } public void union ( int p , int q ) { int pid = id [ p ] ; int qid = id [ q ] ; for ( int i = 0 ; i < id . length ; i ++ ) if ( id [ i ] == pid ) id [ i ] = qid ; } }","title":"quick-find"},{"location":"knowledge-base/general/algorithms/quick_find/#quick-find","text":"eager approach we follow the ids, until we find same id. 2 elements are connected if they have the same id. find easy, union is slow. cost N^2 resource: https://www.coursera.org/learn/algorithms-part1/lecture/EcF3P/quick-find Example: public class QuickFindUF { private int [] id ; public QuickFindUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } public boolean connected ( int p , int q ) { return id [ p ] == id [ q ] ; } public void union ( int p , int q ) { int pid = id [ p ] ; int qid = id [ q ] ; for ( int i = 0 ; i < id . length ; i ++ ) if ( id [ i ] == pid ) id [ i ] = qid ; } }","title":"quick-find"},{"location":"knowledge-base/general/algorithms/quick_union/","text":"quick-union \u00b6 lazy approach: avoid doing work until we have to. we follow the parent roots until we find a parent points to itself, this will be the parent of all elements in the tree. 2 elements are connectd if they have the same parent root. union is easy, find is slow. cost N resource: https://www.coursera.org/learn/algorithms-part1/lecture/ZgecU/quick-union Example: public class QuickFindUF { private int [] id ; public QuickFindUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } public boolean connected ( int p , int q ) { return id [ p ] == id [ q ] ; } public void union ( int p , int q ) { int pid = id [ p ] ; int qid = id [ q ] ; for ( int i = 0 ; i < id . length ; i ++ ) if ( id [ i ] == pid ) id [ i ] = qid ; } } quick-union Improvments \u00b6 1. weighting \u00b6 while implementing, avoid long trees. track the number of objects in each tree, then put the short \\(small\\) tree under the long tree. cost lg N = log(2) N Ex: N=1000 => lg N = 10 , N=1000000 => lg N = 20 , N=1000000000 => lg N = 30 . -Example: Modify quick-union union function: public void union ( int p , int q ) { int i = root ( p ); int j = root ( q ); if ( i == j ) return ; if ( sz [ i ] < sz [ j ] ) { id [ i ] = j ; sz [ j ] += sz [ i ] ; } else { id [ j ] = i ; sz [ i ] += sz [ j ] ; } } 2. path-compression \u00b6 while searching, change the pointers of each sub-tree to point directly to the parent root. flatten the tree. Example: Modify quick-union-weighting root function: private int root ( int i ) { while ( i != id [ i ] ) { id [ i ] = id [ id [ i ]] ; i = id [ i ] ; } return i ; }","title":"quick-union"},{"location":"knowledge-base/general/algorithms/quick_union/#quick-union","text":"lazy approach: avoid doing work until we have to. we follow the parent roots until we find a parent points to itself, this will be the parent of all elements in the tree. 2 elements are connectd if they have the same parent root. union is easy, find is slow. cost N resource: https://www.coursera.org/learn/algorithms-part1/lecture/ZgecU/quick-union Example: public class QuickFindUF { private int [] id ; public QuickFindUF ( int N ) { id = new int [ N ] ; for ( int i = 0 ; i < N ; i ++ ) id [ i ] = i ; } public boolean connected ( int p , int q ) { return id [ p ] == id [ q ] ; } public void union ( int p , int q ) { int pid = id [ p ] ; int qid = id [ q ] ; for ( int i = 0 ; i < id . length ; i ++ ) if ( id [ i ] == pid ) id [ i ] = qid ; } }","title":"quick-union"},{"location":"knowledge-base/general/algorithms/quick_union/#quick-union-improvments","text":"","title":"quick-union Improvments"},{"location":"knowledge-base/general/algorithms/quick_union/#1-weighting","text":"while implementing, avoid long trees. track the number of objects in each tree, then put the short \\(small\\) tree under the long tree. cost lg N = log(2) N Ex: N=1000 => lg N = 10 , N=1000000 => lg N = 20 , N=1000000000 => lg N = 30 . -Example: Modify quick-union union function: public void union ( int p , int q ) { int i = root ( p ); int j = root ( q ); if ( i == j ) return ; if ( sz [ i ] < sz [ j ] ) { id [ i ] = j ; sz [ j ] += sz [ i ] ; } else { id [ j ] = i ; sz [ i ] += sz [ j ] ; } }","title":"1. weighting"},{"location":"knowledge-base/general/algorithms/quick_union/#2-path-compression","text":"while searching, change the pointers of each sub-tree to point directly to the parent root. flatten the tree. Example: Modify quick-union-weighting root function: private int root ( int i ) { while ( i != id [ i ] ) { id [ i ] = id [ id [ i ]] ; i = id [ i ] ; } return i ; }","title":"2.  path-compression"},{"location":"knowledge-base/general/algorithms/union_find/","text":"union-find \u00b6 Read in number of objects N from standard input. -\u30fbRepeat: \u2013 read in pair of integers from standard input \u2013 if they are not yet connected, connect them and print out pair resource: https://d3c33hcgiwev3.cloudfront.net/_b65e7611894ba175de27bd14793f894a_15UnionFind.pdf?Expires=1587945600&Signature=acCR6aDGXnA7luzAqFrIeDW6riwnUjsEuigduiwEkwFUkL9tvIH5k39Lo1bSy00AP0nC1QBlXexq6fqaGslGLbIbWBOc0jO99mi09BCFV6InmXXlPIIgk90DDf9Vk67O4tEs-jSRma7NSCCohSE6~WCb36UWOGVLQOIq7Amm5DY_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A EX: Monte Carlo Simulation ` general example public static void main ( String [] args ) { int N = StdIn . readInt (); UF uf = new UF ( N ); while ( ! StdIn . isEmpty ()) { int p = StdIn . readInt (); int q = StdIn . readInt (); if ( ! uf . connected ( p , q )){ uf . union ( p , q ); StdOut . println ( p + \" \" + q ); } } }","title":"union-find"},{"location":"knowledge-base/general/algorithms/union_find/#union-find","text":"Read in number of objects N from standard input. -\u30fbRepeat: \u2013 read in pair of integers from standard input \u2013 if they are not yet connected, connect them and print out pair resource: https://d3c33hcgiwev3.cloudfront.net/_b65e7611894ba175de27bd14793f894a_15UnionFind.pdf?Expires=1587945600&Signature=acCR6aDGXnA7luzAqFrIeDW6riwnUjsEuigduiwEkwFUkL9tvIH5k39Lo1bSy00AP0nC1QBlXexq6fqaGslGLbIbWBOc0jO99mi09BCFV6InmXXlPIIgk90DDf9Vk67O4tEs-jSRma7NSCCohSE6~WCb36UWOGVLQOIq7Amm5DY_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A EX: Monte Carlo Simulation ` general example public static void main ( String [] args ) { int N = StdIn . readInt (); UF uf = new UF ( N ); while ( ! StdIn . isEmpty ()) { int p = StdIn . readInt (); int q = StdIn . readInt (); if ( ! uf . connected ( p , q )){ uf . union ( p , q ); StdOut . println ( p + \" \" + q ); } } }","title":"union-find"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/","text":"algoritm toolbox \u00b6 the course link is here week1: Programming Challenges \u00b6 week2: Algorithmic Warm-up \u00b6 week3: Greedy Algorithms \u00b6 week4: Divide-and-Conquer \u00b6 braek the problem down into a set of non-overlaping sub-problems. sub-problems have the same type as the original we start solving sub problems in order. after solving all sub-problems we combine the results. since all sub-problems are of the same type, when we solve a sub-problem and iterate the solution recurresively to the rest of the problems. Definitions \u00b6 key, K: the elemet we are looking for. Array, A: the array we searching in linear search \u00b6 loop through each elemnt of the array, determine if we find the element that we are looking for or not. after each search we face a sub-array \\(the main array execluded the previous element\\) with the same type of problem. we define the time that alogrithm takes as recurrence relation witch is an equation T . worst case : element not found, defined by: T(n) = T(n-1) + c => cost = o(n) . best \\(base\\) case: empty array, T(0) = c => cost = o(1) . used in serch for the free elemnts stored in a linked-list . Binary search \u00b6 dividing problems into halves, also uses recurresion. starts by sorting the array. we calculate the middle index mid of the sorted array, search for key at that index. if A[mid] == key were done. else if: key > A[mid] we do another binary search on the top half of the sorted array. else if key < A[mid] we do binary search on the first half. if the key is not found. we will return the best index to put our key in this sorted-array . worst case: element not found, T(n) = T(n/2) + c => cost = Log2 (n) base case: empty array, T(0) = c => cost = o(1) . Augmented Array \u00b6 a sorted array that keep tracks element indexes in the original array before sorting. polynomial multiplication \u00b6 usage: error-correcting code. large integer multiplication. generating functions. convolution in signal processing. input: n=3, A=(3,2,5), B=(5,1,2) => output: C=(15,13,33,9,10) . input: n = 2, A = (3, 4) B = (1, 2) => output: C=(3, 10, 8) corrseponding to: (3x+4) * (1x + 2) = 3x^2 + 10x + 8 . Naiive algorithm for solving polynomial multiplication problem \u00b6 function MultPoly ( A , B , n ) { /** * A is polynomial as [3, 2, 5]; * B is another polynomial * n is the hiegher degree of both polynomial (hieghest power of X) * */ let product = new Array ( 2 * n - 1 ); for ( let i = 0 ; i < 2 * n - 1 ; i ++ ) { product [ i ] = 0 ; } for ( let i = 0 ; i < n ; i ++ ) { for ( let j = 0 ; j < n ; j ++ ) { product [ i + j ] = A [ i ] + B [ j ] + product [ i + j ]; } } return product ; } cost \\(runtime\\) o(n^2) ; Naiive divide-and-conquer algorithm for solving polynomial multiplication \u00b6 function MultPoly2Wrapper ( A , B , n ){ // n needs to be divided by 2, if not add another degree to the polynomial, // were its values is 0. function MultPoly2 ( A , B , n , a1 , b1 ){ /** * A is polynomial as [3, 2, 5]; * B is another polynomial * n is the hiegher degree of both polynomial (hieghest power of X) * a1 specify the lower bound of the beginning of the sub-polynomials that are being multiplied * b1 specify the lower bound of the beginning of the sub-polynomials that are being multiplied * Mathematics Info in this photo: https://i.imgur.com/QZLIBQK.png * programming info in this photo: https://i.imgur.com/UWKFjWB.png */ let R = new Array ( 2 * n - 1 ); for ( let i = 0 ; i < 2 * n - 1 ; i ++ ){ R [ i ] = 0 ; } if ( n == 1 ){ R [ 0 ] = A [ a1 ] * B [ b1 ]; return R ; } //doing first half for ( let i = 0 ; i < n - 1 ; i ++ ){ R [ i ] = MultPoly2 ( A , B , n / 2 , a1 , b1 ); } //we kept R[n-1] empty //doing second half for ( let j = n ; j < 2 * n - 1 ; j ++ ){ R [ j ] = MultPoly2 ( A , B , n / 2 , a1 + n / 2 , b1 + n / 2 ); } let D0E1 = MultPoly2 ( A , B , n / 2 , a1 , b1 + n / 2 ); let D1E0 = MultPoly2 ( A , B , n / 2 , a1 + n / 2 , b1 ); // ??????? let D1E1 = MultPoly2 ( A , B , n / 2 , a1 + n / 2 , b1 + n / 2 ); let D0E0 = MultPoly2 ( A , B , n / 2 , a1 , b1 ); //filling the empty elemnt R[n-1] let R [ n - 1 ] = D1E0 + D0E1 ; return R ; } } cost T(n) = 4 * T(n/2) + n*c => 4 * n * log(n) + n => n2. Faster divide-and-conquer algorithm for solving polynomial multiplication \u00b6 /** * Maths Explanation here: https://i.imgur.com/qa0to77.png */ function MultPoly3wrapper ( A , B , n ){ // n needs to be divided by 2, if not add another degree to the polynomial, // were its values is 0. let D1E1 = //calcualted in the previous function; let D0E0 = //calculated before let ( D1 + D0 ) + ( E1 + E0 ) = //calculated as the previous function. } karatsuba approach , do 3 multiplications instead of 4. cost T(n) = 3 * T(n/2) + n*c => 3 * n * log(n) + c * n => nlog3 => n1.58. Master Theorem \u00b6 for calculating cost in divide-and-concouer cost, binary search, problem divided into 2, each cost constant time c : T(n) = T(n/2) +c => log n divide-and-conquer 1, problem divided into 4, each cost linear time n : T(n) = 4 * T(n/2) + n * c => nlog 4 => n2 divide-and-conquer 2, problem divided into 3, each cost linear time n : T(n) = 3 * T(n/2) + n * c => nlog 3 => n1.58 divide-and-conquer 3, problem divided into 2, each cost linear time n : T(n) = 2 * T(n/2) + n * c => nlog 2 => nlog \\(n\\) => o(n log n) more info [here] \\([https://en.wikipedia.org/wiki/Master\\_theorem\\_\\(analysis\\_of\\_algorithms\\) ](https://en.wikipedia.org/wiki/Master_theorem_%28analysis_of_algorithms%29)) then Theorom: let T(n) = a * T(n/b) + o(n^d) if d > logb ^a => T(n) = o(n^d) if d = logb ^a => T(n) = o(n^d log n) if d < logb ^a => T(n) = o(n^logb ^a) selection sort \u00b6 find the minimum element => swap with first => reprat this process. /** * psudeo code here: https://i.imgur.com/IPq72GD.png */ function selectionSort ( A ) { let n = A . length ; for ( let i = 0 ; i < A . length ; i ++ ) { let minIndex = i ; for ( let j = i + 1 ; j < A . length ; j ++ ) { if ( A [ j ] < A [ minIndex ]) { minIndex = j ; } if ( minIndex !== i ) { let c = A [ minIndex ]; swap ( A , A [ i ], c ); // A[0,i] is now sorted. } } } return A ; } function swap ( A , B , C ) { let c = A . indexOf ( C ); let b = A . indexOf ( B ); let temp = B ; A [ b ] = C ; A [ c ] = temp ; } cost o(n^2) Merge sort \u00b6 split => sort splits => merge splits. merging has its own rules, evaluate the first elemnt from each arrays to be merged, we choose their minimum and put it in the result => repeat. /** * psudo code: https://i.imgur.com/rGmTGr4.png */ function mergeSort ( A ) { if ( A . length < 2 ) { return A ; } let m = Math . floor ( A . length / 2 ); let BB = mergeSort ( A . slice ( 0 , m )); let CC = mergeSort ( A . slice ( m , A . length )); let AA = merge ( BB , CC ); return AA ; } /** * psuedo code: https://i.imgur.com/xlKWjXX.png */ function merge ( B , C ) { let D = []; while ( B . length && C . length ) { if ( B [ 0 ] <= C [ 0 ]) { D . push ( B [ 0 ]); B . splice ( 0 , 1 ); } else { D . push ( C . shift ()); } } while ( B . length ) { D . push ( B . shift ()); } while ( C . length ) { D . push ( C . shift ()); } return D ; } cost n * log n counting sort \u00b6 the selection sort and merge sort use object comparision to complete the sort. if the array is consesting from repitive small ints we can apply counting sort count occurences of each element => store counts to each elemnt \\(n\\) => fill up the result array by each elemnt reptitve with its corresponding count. /** * psuedo code: https://i.imgur.com/Nqb0cCg.png * Now, works only for postivie ints. */ function countSort ( A ) { let counts = {}; //let s = Math.min(...A); let m = Math . max (... A ); // console.log('m', m); for ( let i = 0 ; i <= m ; i ++ ) { counts [ i ] = 0 ; } //console.log(counts) for ( let i = 0 ; i < A . length ; i ++ ) { // if(Object.keys(counts).includes(A[i].toString())){ // if(A.includes(Number(counts[i.toString()]))){ counts [ A [ i ]] ++ ; // console.log('obj inside', counts); //} } let countsArray = Object . keys ( counts ); //.every(e => e.toString()); // console.log('counts Array', countsArray); // console.log('count obj', counts) let result = []; for ( let i = 0 ; i < countsArray . length ; i ++ ) { if ( counts [ i ] > 0 ) { let n = counts [ i ]; //console.log('n', n) for ( let j = 0 ; j < n ; j ++ ) { result . push ( Number ( countsArray [ i ])); } } } return result ; } costs o( Array.length + counts.length ) => o(n) quick sort \u00b6 take first elemnt A[0] => rearrange the array so A[0] will be in the middle, all elements less or equal to A[0] will be on the left, all ements greater than A[0] will be on the right. A[0] is in its final positon => we need to sort [left Array] and [right Array] => repeat. partion or pivot we choose it, either first or last element or in the middle or any element, however, it's important to skip this element in the for loop . /** * psuedo code: https://i.imgur.com/eczLG6T.png */ const quickSort = ( array ) => { if ( array . length < 2 ) return array ; const pivot = array [ array . length - 1 ]; const left = [], right = []; for ( let i = 0 ; i < array . length - 1 ; i ++ ) { if ( array [ i ] < pivot ) left . push ( array [ i ]); else right . push ( array [ i ]); } return [... quickSort ( left ), pivot , ... quickSort ( right )]; }; best animation: https://www.youtube.com/watch?v=cnzIChso3cc costs o(n^2) at worst: right or lift is empty, o(n log n) at average: right and left are nearly equal. select our pivot randomly will give us more balanced left, right arrays => costs less, we should skipt it from the loop. quick sort is not so fast on Arrays with few uniqe elements : when you have few elemnets that are repeated. costs o(n^2) quick3 \u00b6 quick sort on array with few unique elements. we partion to get 3 sub-arrays insted of 2: left, middle, right. left: All elements less than pivot middle: all elemnts equal to pivot lift: all elements greateer than pivot /** * psuedo code: https://i.imgur.com/PaMqD6E.png */ const quickSort3 = ( array ) => { if ( array . length < 2 ) return array ; const pivot = array [ array . length - 1 ]; const left = [], right = []; middle = []; for ( let i = 0 ; i < array . length - 1 ; i ++ ) { if ( array [ i ] == pivot ) middle . push ( array [ i ]); if ( array [ i ] < pivot ) left . push ( array [ i ]); else right . push ( array [ i ]); } return [... quickSort3 ( left ), ... middle , ... quickSort3 ( right )]; }; notes about quick sort \u00b6 In-place algorithm : does not use extra auxilary space on the memory. elemeiate tail recursion or Tail recursion call optimization : GeekForGeek article psuedo code even if random pivot is faster, it makes our program behaves differently on the same dataset, so it's not welcomed. intro sort : when choosing the pivot, we select first, last and middle elemnts of the Array, then we compare these pivots => choose the median pivot as owr pivot. Intro quickSort \u00b6 const IntroquickSort = ( array ) => { if ( array . length < 2 ) return array ; //comparing pivots const pivot1 = array [ 0 ]; const pivot2 = array [ Math . floor (( array . length - 1 ) / 2 )]; const pivot3 = array [ array . length - 1 ]; //chosen pivot let pivot = Math . min ( pivot1 , pivot2 , pivot3 ); let pivotIndex = array . indexOf ( pivot ); const left = [], right = []; for ( let i = 0 ; i < array . length ; i ++ ) { if ( i === pivotIndex ) continue ; else if ( array [ i ] < pivot ) left . push ( array [ i ]); else right . push ( array [ i ]); } return [... IntroquickSort ( left ), pivot , ... IntroquickSort ( right )]; }; costs o(n log n) at worst. Array sorting algorithms cost summary \u00b6 week5: Dynamic programming 1 \u00b6 Greedy change \u00b6 psuedo code: https://i.imgur.com/3KGLxLJ.png recursive change \u00b6 psuedo code: https://i.imgur.com/DW0MLfG.png dynamic prgramming change \\(dp change\\) : \u00b6 psuedo code: https://i.imgur.com/DL7Cpeo.png Edit distance \u00b6 psuedo code: https://i.imgur.com/iyCgqR1.png resource \u00b6 https://www.dropbox.com/s/qxzh146jd72188d/dynprog.pdf?dl=0 week 6: dynamic programming 2 \u00b6 knapsack with repetions \u00b6 psudeo code: https://i.imgur.com/4hQNath.png pdf: https://www.cc.gatech.edu/~rpeng/CS3510_F17/Notes/Oct2MoreDP.pdf video: https://www.youtube.com/watch?v=wFP5VHGHFdk&t=866s def unboundedKnapsack ( W , n , val , wt ): # dp[i] is going to store maximum # value with knapsack capacity i. dp = [ 0 for i in range ( W + 1 )] ans = 0 # Fill dp[] using above recursive formula for i in range ( W + 1 ): for j in range ( n ): if ( wt [ j ] <= i ): dp [ i ] = max ( dp [ i ], dp [ i - wt [ j ]] + val [ j ]) return dp [ W ] # Driver program W = 100 val = [ 10 , 30 , 20 ] wt = [ 5 , 10 , 15 ] n = len ( val ) print ( unboundedKnapsack ( W , n , val , wt )) knapsack with repetition \u00b6 psuedo code: https://i.imgur.com/RDVqYi6.png def knapSack ( W , wt , val , n ): # Base Case if n == 0 or W == 0 : return 0 # If weight of the nth item is more than Knapsack of capacity # W, then this item cannot be included in the optimal solution if ( wt [ n - 1 ] > W ): return knapSack ( W , wt , val , n - 1 ) # return the maximum of two cases: # (1) nth item included # (2) not included else : return max ( val [ n - 1 ] + knapSack ( W - wt [ n - 1 ] , wt , val , n - 1 ), knapSack ( W , wt , val , n - 1 )) # To test above function val = [ 60 , 100 , 120 ] wt = [ 10 , 20 , 30 ] W = 50 n = len ( val ) print knapSack ( W , wt , val , n ) memoization \u00b6 psuedo code: https://i.imgur.com/k2tkdnd.png iterative algorithm : starts from the smaller problem into the larger ones recursive algirithm : starts from the largest problem into the smaller ones. resursive is slower. placing parentheses \u00b6 you take an arithmetic operation and choose where to put parentheses so it will maximize its output","title":"algoritm toolbox"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#algoritm-toolbox","text":"the course link is here","title":"algoritm toolbox"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#week1-programming-challenges","text":"","title":"week1: Programming Challenges"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#week2-algorithmic-warm-up","text":"","title":"week2: Algorithmic Warm-up"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#week3-greedy-algorithms","text":"","title":"week3: Greedy Algorithms"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#week4-divide-and-conquer","text":"braek the problem down into a set of non-overlaping sub-problems. sub-problems have the same type as the original we start solving sub problems in order. after solving all sub-problems we combine the results. since all sub-problems are of the same type, when we solve a sub-problem and iterate the solution recurresively to the rest of the problems.","title":"week4: Divide-and-Conquer"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#definitions","text":"key, K: the elemet we are looking for. Array, A: the array we searching in","title":"Definitions"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#linear-search","text":"loop through each elemnt of the array, determine if we find the element that we are looking for or not. after each search we face a sub-array \\(the main array execluded the previous element\\) with the same type of problem. we define the time that alogrithm takes as recurrence relation witch is an equation T . worst case : element not found, defined by: T(n) = T(n-1) + c => cost = o(n) . best \\(base\\) case: empty array, T(0) = c => cost = o(1) . used in serch for the free elemnts stored in a linked-list .","title":"linear search"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#binary-search","text":"dividing problems into halves, also uses recurresion. starts by sorting the array. we calculate the middle index mid of the sorted array, search for key at that index. if A[mid] == key were done. else if: key > A[mid] we do another binary search on the top half of the sorted array. else if key < A[mid] we do binary search on the first half. if the key is not found. we will return the best index to put our key in this sorted-array . worst case: element not found, T(n) = T(n/2) + c => cost = Log2 (n) base case: empty array, T(0) = c => cost = o(1) .","title":"Binary search"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#augmented-array","text":"a sorted array that keep tracks element indexes in the original array before sorting.","title":"Augmented Array"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#polynomial-multiplication","text":"usage: error-correcting code. large integer multiplication. generating functions. convolution in signal processing. input: n=3, A=(3,2,5), B=(5,1,2) => output: C=(15,13,33,9,10) . input: n = 2, A = (3, 4) B = (1, 2) => output: C=(3, 10, 8) corrseponding to: (3x+4) * (1x + 2) = 3x^2 + 10x + 8 .","title":"polynomial multiplication"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#naiive-algorithm-for-solving-polynomial-multiplication-problem","text":"function MultPoly ( A , B , n ) { /** * A is polynomial as [3, 2, 5]; * B is another polynomial * n is the hiegher degree of both polynomial (hieghest power of X) * */ let product = new Array ( 2 * n - 1 ); for ( let i = 0 ; i < 2 * n - 1 ; i ++ ) { product [ i ] = 0 ; } for ( let i = 0 ; i < n ; i ++ ) { for ( let j = 0 ; j < n ; j ++ ) { product [ i + j ] = A [ i ] + B [ j ] + product [ i + j ]; } } return product ; } cost \\(runtime\\) o(n^2) ;","title":"Naiive algorithm for solving polynomial multiplication problem"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#naiive-divide-and-conquer-algorithm-for-solving-polynomial-multiplication","text":"function MultPoly2Wrapper ( A , B , n ){ // n needs to be divided by 2, if not add another degree to the polynomial, // were its values is 0. function MultPoly2 ( A , B , n , a1 , b1 ){ /** * A is polynomial as [3, 2, 5]; * B is another polynomial * n is the hiegher degree of both polynomial (hieghest power of X) * a1 specify the lower bound of the beginning of the sub-polynomials that are being multiplied * b1 specify the lower bound of the beginning of the sub-polynomials that are being multiplied * Mathematics Info in this photo: https://i.imgur.com/QZLIBQK.png * programming info in this photo: https://i.imgur.com/UWKFjWB.png */ let R = new Array ( 2 * n - 1 ); for ( let i = 0 ; i < 2 * n - 1 ; i ++ ){ R [ i ] = 0 ; } if ( n == 1 ){ R [ 0 ] = A [ a1 ] * B [ b1 ]; return R ; } //doing first half for ( let i = 0 ; i < n - 1 ; i ++ ){ R [ i ] = MultPoly2 ( A , B , n / 2 , a1 , b1 ); } //we kept R[n-1] empty //doing second half for ( let j = n ; j < 2 * n - 1 ; j ++ ){ R [ j ] = MultPoly2 ( A , B , n / 2 , a1 + n / 2 , b1 + n / 2 ); } let D0E1 = MultPoly2 ( A , B , n / 2 , a1 , b1 + n / 2 ); let D1E0 = MultPoly2 ( A , B , n / 2 , a1 + n / 2 , b1 ); // ??????? let D1E1 = MultPoly2 ( A , B , n / 2 , a1 + n / 2 , b1 + n / 2 ); let D0E0 = MultPoly2 ( A , B , n / 2 , a1 , b1 ); //filling the empty elemnt R[n-1] let R [ n - 1 ] = D1E0 + D0E1 ; return R ; } } cost T(n) = 4 * T(n/2) + n*c => 4 * n * log(n) + n => n2.","title":"Naiive divide-and-conquer algorithm for solving polynomial multiplication"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#faster-divide-and-conquer-algorithm-for-solving-polynomial-multiplication","text":"/** * Maths Explanation here: https://i.imgur.com/qa0to77.png */ function MultPoly3wrapper ( A , B , n ){ // n needs to be divided by 2, if not add another degree to the polynomial, // were its values is 0. let D1E1 = //calcualted in the previous function; let D0E0 = //calculated before let ( D1 + D0 ) + ( E1 + E0 ) = //calculated as the previous function. } karatsuba approach , do 3 multiplications instead of 4. cost T(n) = 3 * T(n/2) + n*c => 3 * n * log(n) + c * n => nlog3 => n1.58.","title":"Faster divide-and-conquer algorithm for solving polynomial multiplication"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#master-theorem","text":"for calculating cost in divide-and-concouer cost, binary search, problem divided into 2, each cost constant time c : T(n) = T(n/2) +c => log n divide-and-conquer 1, problem divided into 4, each cost linear time n : T(n) = 4 * T(n/2) + n * c => nlog 4 => n2 divide-and-conquer 2, problem divided into 3, each cost linear time n : T(n) = 3 * T(n/2) + n * c => nlog 3 => n1.58 divide-and-conquer 3, problem divided into 2, each cost linear time n : T(n) = 2 * T(n/2) + n * c => nlog 2 => nlog \\(n\\) => o(n log n) more info [here] \\([https://en.wikipedia.org/wiki/Master\\_theorem\\_\\(analysis\\_of\\_algorithms\\) ](https://en.wikipedia.org/wiki/Master_theorem_%28analysis_of_algorithms%29)) then Theorom: let T(n) = a * T(n/b) + o(n^d) if d > logb ^a => T(n) = o(n^d) if d = logb ^a => T(n) = o(n^d log n) if d < logb ^a => T(n) = o(n^logb ^a)","title":"Master Theorem"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#selection-sort","text":"find the minimum element => swap with first => reprat this process. /** * psudeo code here: https://i.imgur.com/IPq72GD.png */ function selectionSort ( A ) { let n = A . length ; for ( let i = 0 ; i < A . length ; i ++ ) { let minIndex = i ; for ( let j = i + 1 ; j < A . length ; j ++ ) { if ( A [ j ] < A [ minIndex ]) { minIndex = j ; } if ( minIndex !== i ) { let c = A [ minIndex ]; swap ( A , A [ i ], c ); // A[0,i] is now sorted. } } } return A ; } function swap ( A , B , C ) { let c = A . indexOf ( C ); let b = A . indexOf ( B ); let temp = B ; A [ b ] = C ; A [ c ] = temp ; } cost o(n^2)","title":"selection sort"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#merge-sort","text":"split => sort splits => merge splits. merging has its own rules, evaluate the first elemnt from each arrays to be merged, we choose their minimum and put it in the result => repeat. /** * psudo code: https://i.imgur.com/rGmTGr4.png */ function mergeSort ( A ) { if ( A . length < 2 ) { return A ; } let m = Math . floor ( A . length / 2 ); let BB = mergeSort ( A . slice ( 0 , m )); let CC = mergeSort ( A . slice ( m , A . length )); let AA = merge ( BB , CC ); return AA ; } /** * psuedo code: https://i.imgur.com/xlKWjXX.png */ function merge ( B , C ) { let D = []; while ( B . length && C . length ) { if ( B [ 0 ] <= C [ 0 ]) { D . push ( B [ 0 ]); B . splice ( 0 , 1 ); } else { D . push ( C . shift ()); } } while ( B . length ) { D . push ( B . shift ()); } while ( C . length ) { D . push ( C . shift ()); } return D ; } cost n * log n","title":"Merge sort"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#counting-sort","text":"the selection sort and merge sort use object comparision to complete the sort. if the array is consesting from repitive small ints we can apply counting sort count occurences of each element => store counts to each elemnt \\(n\\) => fill up the result array by each elemnt reptitve with its corresponding count. /** * psuedo code: https://i.imgur.com/Nqb0cCg.png * Now, works only for postivie ints. */ function countSort ( A ) { let counts = {}; //let s = Math.min(...A); let m = Math . max (... A ); // console.log('m', m); for ( let i = 0 ; i <= m ; i ++ ) { counts [ i ] = 0 ; } //console.log(counts) for ( let i = 0 ; i < A . length ; i ++ ) { // if(Object.keys(counts).includes(A[i].toString())){ // if(A.includes(Number(counts[i.toString()]))){ counts [ A [ i ]] ++ ; // console.log('obj inside', counts); //} } let countsArray = Object . keys ( counts ); //.every(e => e.toString()); // console.log('counts Array', countsArray); // console.log('count obj', counts) let result = []; for ( let i = 0 ; i < countsArray . length ; i ++ ) { if ( counts [ i ] > 0 ) { let n = counts [ i ]; //console.log('n', n) for ( let j = 0 ; j < n ; j ++ ) { result . push ( Number ( countsArray [ i ])); } } } return result ; } costs o( Array.length + counts.length ) => o(n)","title":"counting sort"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#quick-sort","text":"take first elemnt A[0] => rearrange the array so A[0] will be in the middle, all elements less or equal to A[0] will be on the left, all ements greater than A[0] will be on the right. A[0] is in its final positon => we need to sort [left Array] and [right Array] => repeat. partion or pivot we choose it, either first or last element or in the middle or any element, however, it's important to skip this element in the for loop . /** * psuedo code: https://i.imgur.com/eczLG6T.png */ const quickSort = ( array ) => { if ( array . length < 2 ) return array ; const pivot = array [ array . length - 1 ]; const left = [], right = []; for ( let i = 0 ; i < array . length - 1 ; i ++ ) { if ( array [ i ] < pivot ) left . push ( array [ i ]); else right . push ( array [ i ]); } return [... quickSort ( left ), pivot , ... quickSort ( right )]; }; best animation: https://www.youtube.com/watch?v=cnzIChso3cc costs o(n^2) at worst: right or lift is empty, o(n log n) at average: right and left are nearly equal. select our pivot randomly will give us more balanced left, right arrays => costs less, we should skipt it from the loop. quick sort is not so fast on Arrays with few uniqe elements : when you have few elemnets that are repeated. costs o(n^2)","title":"quick sort"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#quick3","text":"quick sort on array with few unique elements. we partion to get 3 sub-arrays insted of 2: left, middle, right. left: All elements less than pivot middle: all elemnts equal to pivot lift: all elements greateer than pivot /** * psuedo code: https://i.imgur.com/PaMqD6E.png */ const quickSort3 = ( array ) => { if ( array . length < 2 ) return array ; const pivot = array [ array . length - 1 ]; const left = [], right = []; middle = []; for ( let i = 0 ; i < array . length - 1 ; i ++ ) { if ( array [ i ] == pivot ) middle . push ( array [ i ]); if ( array [ i ] < pivot ) left . push ( array [ i ]); else right . push ( array [ i ]); } return [... quickSort3 ( left ), ... middle , ... quickSort3 ( right )]; };","title":"quick3"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#notes-about-quick-sort","text":"In-place algorithm : does not use extra auxilary space on the memory. elemeiate tail recursion or Tail recursion call optimization : GeekForGeek article psuedo code even if random pivot is faster, it makes our program behaves differently on the same dataset, so it's not welcomed. intro sort : when choosing the pivot, we select first, last and middle elemnts of the Array, then we compare these pivots => choose the median pivot as owr pivot.","title":"notes about quick sort"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#intro-quicksort","text":"const IntroquickSort = ( array ) => { if ( array . length < 2 ) return array ; //comparing pivots const pivot1 = array [ 0 ]; const pivot2 = array [ Math . floor (( array . length - 1 ) / 2 )]; const pivot3 = array [ array . length - 1 ]; //chosen pivot let pivot = Math . min ( pivot1 , pivot2 , pivot3 ); let pivotIndex = array . indexOf ( pivot ); const left = [], right = []; for ( let i = 0 ; i < array . length ; i ++ ) { if ( i === pivotIndex ) continue ; else if ( array [ i ] < pivot ) left . push ( array [ i ]); else right . push ( array [ i ]); } return [... IntroquickSort ( left ), pivot , ... IntroquickSort ( right )]; }; costs o(n log n) at worst.","title":"Intro quickSort"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#array-sorting-algorithms-cost-summary","text":"","title":"Array sorting algorithms cost summary"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#week5-dynamic-programming-1","text":"","title":"week5: Dynamic programming 1"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#greedy-change","text":"psuedo code: https://i.imgur.com/3KGLxLJ.png","title":"Greedy change"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#recursive-change","text":"psuedo code: https://i.imgur.com/DW0MLfG.png","title":"recursive change"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#dynamic-prgramming-change-dp-change","text":"psuedo code: https://i.imgur.com/DL7Cpeo.png","title":"dynamic prgramming change \\(dp change\\):"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#edit-distance","text":"psuedo code: https://i.imgur.com/iyCgqR1.png","title":"Edit distance"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#resource","text":"https://www.dropbox.com/s/qxzh146jd72188d/dynprog.pdf?dl=0","title":"resource"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#week-6-dynamic-programming-2","text":"","title":"week 6: dynamic programming 2"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#knapsack-with-repetions","text":"psudeo code: https://i.imgur.com/4hQNath.png pdf: https://www.cc.gatech.edu/~rpeng/CS3510_F17/Notes/Oct2MoreDP.pdf video: https://www.youtube.com/watch?v=wFP5VHGHFdk&t=866s def unboundedKnapsack ( W , n , val , wt ): # dp[i] is going to store maximum # value with knapsack capacity i. dp = [ 0 for i in range ( W + 1 )] ans = 0 # Fill dp[] using above recursive formula for i in range ( W + 1 ): for j in range ( n ): if ( wt [ j ] <= i ): dp [ i ] = max ( dp [ i ], dp [ i - wt [ j ]] + val [ j ]) return dp [ W ] # Driver program W = 100 val = [ 10 , 30 , 20 ] wt = [ 5 , 10 , 15 ] n = len ( val ) print ( unboundedKnapsack ( W , n , val , wt ))","title":"knapsack with repetions"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#knapsack-with-repetition","text":"psuedo code: https://i.imgur.com/RDVqYi6.png def knapSack ( W , wt , val , n ): # Base Case if n == 0 or W == 0 : return 0 # If weight of the nth item is more than Knapsack of capacity # W, then this item cannot be included in the optimal solution if ( wt [ n - 1 ] > W ): return knapSack ( W , wt , val , n - 1 ) # return the maximum of two cases: # (1) nth item included # (2) not included else : return max ( val [ n - 1 ] + knapSack ( W - wt [ n - 1 ] , wt , val , n - 1 ), knapSack ( W , wt , val , n - 1 )) # To test above function val = [ 60 , 100 , 120 ] wt = [ 10 , 20 , 30 ] W = 50 n = len ( val ) print knapSack ( W , wt , val , n )","title":"knapsack with repetition"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#memoization","text":"psuedo code: https://i.imgur.com/k2tkdnd.png iterative algorithm : starts from the smaller problem into the larger ones recursive algirithm : starts from the largest problem into the smaller ones. resursive is slower.","title":"memoization"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/#placing-parentheses","text":"you take an arithmetic operation and choose where to put parentheses so it will maximize its output","title":"placing parentheses"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/algorithms_notes/","text":"Algorithms Notes \u00b6 This is a summary of the course on MIT open source here Lecture 2: Models of Computation pdf \u00b6 for x in L costs linear time o(n) A1 + A2 adding 2 arrays, creates an empty array then add every elemnt to it, costs 1 + o(A1) + o(A2) Arr.length costs constant o(1) Arr.sort() costs n * log n Document Distance Problem \u2014 compute d \\(D1, D2\\) \u00b6 split each document into words count word frequencies \\(document vectors\\) compute dot product \\(& divide\\) lecture 3: Insertion Sort, Merge Sort pdf \u00b6 sorting make things easier, like binary srearch and find the median Finding the median in an array \u00b6 simply sort the array, and look to the elemnt at n/2 . costs contatnt time o(1) if you start from sorted array. Insertion sort \u00b6 insert key A[j] into the \\(already sorted\\) sub-array A[1 .. j-1]. by pairwise key-swaps down to its right position. costs o(n^2) cause, o(n^2) for compares, o(n^2) for the swaps. => o(n) + o(n) = o(n^2) Binary Insertion sort \u00b6 insert key A[j] into the \\(already sorted\\) sub-array A[1 ..j-1]. Use binary search to find the right position. costs \\(Complexity\\) : \u0398(n log n) for comparisons, and \u0398(n^2) for swaps. Merge Sort \u00b6 recurrsion. split => sort splits => merge sorted splits. need to copy the array first, so it tskes more space than insert sort. costs o(n) extra aux space. costs o(n log n) In-place sorting \u00b6 do sorting without copying the arrays, costs o(1) auxiliary space. used in insertion sort. Heap \u00b6 priority queue","title":"Algorithms Notes"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/algorithms_notes/#algorithms-notes","text":"This is a summary of the course on MIT open source here","title":"Algorithms Notes"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/algorithms_notes/#lecture-2-models-of-computation-pdf","text":"for x in L costs linear time o(n) A1 + A2 adding 2 arrays, creates an empty array then add every elemnt to it, costs 1 + o(A1) + o(A2) Arr.length costs constant o(1) Arr.sort() costs n * log n","title":"Lecture 2: Models of Computation pdf"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/algorithms_notes/#document-distance-problem-compute-dd1-d2","text":"split each document into words count word frequencies \\(document vectors\\) compute dot product \\(& divide\\)","title":"Document Distance Problem \u2014 compute d\\(D1, D2\\)"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/algorithms_notes/#lecture-3-insertion-sort-merge-sort-pdf","text":"sorting make things easier, like binary srearch and find the median","title":"lecture 3: Insertion Sort, Merge Sort pdf"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/algorithms_notes/#finding-the-median-in-an-array","text":"simply sort the array, and look to the elemnt at n/2 . costs contatnt time o(1) if you start from sorted array.","title":"Finding the median in an array"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/algorithms_notes/#insertion-sort","text":"insert key A[j] into the \\(already sorted\\) sub-array A[1 .. j-1]. by pairwise key-swaps down to its right position. costs o(n^2) cause, o(n^2) for compares, o(n^2) for the swaps. => o(n) + o(n) = o(n^2)","title":"Insertion sort"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/algorithms_notes/#binary-insertion-sort","text":"insert key A[j] into the \\(already sorted\\) sub-array A[1 ..j-1]. Use binary search to find the right position. costs \\(Complexity\\) : \u0398(n log n) for comparisons, and \u0398(n^2) for swaps.","title":"Binary Insertion sort"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/algorithms_notes/#merge-sort","text":"recurrsion. split => sort splits => merge sorted splits. need to copy the array first, so it tskes more space than insert sort. costs o(n) extra aux space. costs o(n log n)","title":"Merge Sort"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/algorithms_notes/#in-place-sorting","text":"do sorting without copying the arrays, costs o(1) auxiliary space. used in insertion sort.","title":"In-place sorting"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/algorithms_notes/#heap","text":"priority queue","title":"Heap"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/week2/","text":"week2 \u00b6","title":"week2"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/week2/#week2","text":"","title":"week2"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/week2/week2/","text":"week2 \u00b6","title":"week2"},{"location":"knowledge-base/general/algorithms/examples/algorithm-toolbox/week2/week2/#week2","text":"","title":"week2"},{"location":"knowledge-base/general/api/","text":"","title":"Index"},{"location":"knowledge-base/general/api/api-list/","text":"List of Greet APIs \u00b6 Github Api: https://api.github.com/users/ahmad-ali14 dog images: https://dog.ceo/api/breeds/image/random greetings in all languages: https://codeyourfuture.herokuapp.com/api/greetings random facts about a number: http://numbersapi.com/42 facts about random year: http://numbersapi.com/random/year darksky: https://darksky.net/forecast/40.7127,-74.0059/us12/en code wars api: https://www.codewars.com/api/v1/users/ahmad.ali git lines of code for each repo: https://api.codetabs.com/v1/loc?github=:username/:repo","title":"List of Greet APIs"},{"location":"knowledge-base/general/api/api-list/#list-of-greet-apis","text":"Github Api: https://api.github.com/users/ahmad-ali14 dog images: https://dog.ceo/api/breeds/image/random greetings in all languages: https://codeyourfuture.herokuapp.com/api/greetings random facts about a number: http://numbersapi.com/42 facts about random year: http://numbersapi.com/random/year darksky: https://darksky.net/forecast/40.7127,-74.0059/us12/en code wars api: https://www.codewars.com/api/v1/users/ahmad.ali git lines of code for each repo: https://api.codetabs.com/v1/loc?github=:username/:repo","title":"List of Greet APIs"},{"location":"knowledge-base/general/api/packages/","text":"packages Index \u00b6 configStore : store your configurations in one object easly. colors : add colors to the console. inquirer : interact with users through your cli. BigNumber.js : good library to deal with expotential numbers n + e as a normal big integers. dg-url : handy functions to deal with urls. cheerio.io : jquery in the server. axios : http requests library. phantom : http library allows cors. react-chartjs-2 : displaying charts easily. parallelshell : execute commands in parallel. onChange : watch until files changes then triger a command or a task. rimraf : quickly clean a folder. copyfiles : quickly copy files between folders. imagemin-cli : quiqckly minimze images before deploy. UglifyJS 3 : minimize js before deploy. htmlmin : minimze html before deploy. usemin-cli : combine those previous three minimizors into cli. grunt-cli : cli for grunt. react-document-title : change pages titles dynamically react-helmet : manage head meta tags. ElGrapho : nice data representaions. NProgress.js : slim progress bar. loader : js library has a lot of shortcuts. react-n18 : clever library to concentrate your text in one single file, then use sortucts to represent that text, so your files will not be full of text |package | description | | :-: | :-: | | react-loading | nice loading svgs |","title":"packages Index"},{"location":"knowledge-base/general/api/packages/#packages-index","text":"configStore : store your configurations in one object easly. colors : add colors to the console. inquirer : interact with users through your cli. BigNumber.js : good library to deal with expotential numbers n + e as a normal big integers. dg-url : handy functions to deal with urls. cheerio.io : jquery in the server. axios : http requests library. phantom : http library allows cors. react-chartjs-2 : displaying charts easily. parallelshell : execute commands in parallel. onChange : watch until files changes then triger a command or a task. rimraf : quickly clean a folder. copyfiles : quickly copy files between folders. imagemin-cli : quiqckly minimze images before deploy. UglifyJS 3 : minimize js before deploy. htmlmin : minimze html before deploy. usemin-cli : combine those previous three minimizors into cli. grunt-cli : cli for grunt. react-document-title : change pages titles dynamically react-helmet : manage head meta tags. ElGrapho : nice data representaions. NProgress.js : slim progress bar. loader : js library has a lot of shortcuts. react-n18 : clever library to concentrate your text in one single file, then use sortucts to represent that text, so your files will not be full of text |package | description | | :-: | :-: | | react-loading | nice loading svgs |","title":"packages Index"},{"location":"knowledge-base/general/dataStructures/","text":"","title":"Index"},{"location":"knowledge-base/general/dataStructures/binary_trees/","text":"Binary search trees \u00b6 In problems like: Dictionary Search: Find all words that start with some given string. Date Ranges : Find all emails received in a given period. Closest Height:Find the person in your class whose height is closest to yours. Local Search : A Local Search Datastructure stores a number of elements each with a key coming from an ordered set. It supports operations: RangeSearch(x, y) : Returns all elements with keys between x and y. NearestNeighbors(z) : Returns the element with keys on either side of z. we can solve these problems by: Hash tables: Array: Sorted Array: Linked lists: search Tree \u00b6 for any Node X in the tree: X\u2019s key is larger than the key of any descendent of its left child, and smaller than the key of any descendant of its right child. search Tree functions \u00b6 find \\(key, root\\) : Find ( k , R ) : if R . Key = k : return R else if R . Key > k : if R . Left \u0338 = null : return Find ( k , R . Left ) return R else if R . Key < k : return Find ( k , R . Right ) Next \\(Node\\) : Next ( N ) if N . Right \u0338 = null : return LeftDescendant ( N . Right ) else : return RightAncestor ( N ) LeftDescendant \\(Node\\) : LeftDescendant ( N ) if N . Left = null return N else : return LeftDescendant ( N . Left ) Right Ancestor \\(Node\\) : RightAncestor ( N ) : if N . Key < N . Parent . Key return N . Parent else : return RightAncestor ( N . Parent ) RangeSearch \\(x = first element in search range , y = second element to search , R = tree or root\\) RangeSearch ( x , y , R ) : L \u2190 \u2205 N \u2190 Find ( x , R ) while N . Key \u2264 y if N . Key \u2265 x : L \u2190 L . Append ( N ) N \u2190 Next ( N ) return L Insert \\(key, tree or root\\) : Insert ( k , R ) : P \u2190 Find ( k , R ) Add new node with key k as child of P delete \\(Node\\) : Delete ( N ) : if N . Right = null : Remove N , promote N . Left else : X \u2190 Next ( N ) \u2216\u2216 X . Left = null Replace N by X , promote X . Right Example: deleting Node \\(1\\) : bring its next element \\(2\\) , to be in \\(1\\) place. bring \\(4\\) tree to be in \\(2\\) place. AVL trees \u00b6 to keep our trees balanced , \\(the hieght of left = hieght of Right\\) . hieght of tree : the maximum depth of any of its children. calculating the hieght of a tree: Height ( N ) : if N is a leaf : hieght = 1 else : hieght = 1 + max ( N . Left . Height , N . Right . Height ) example of Node after adding hieght property: updating trees can destroy their balance. Insertion into AVL tree \u00b6 We need a new insertion algorithm that involves rebalancing the tree to maintain the AVL property. insertion idea: AVLInsert ( k : key , R : root ) : Insert ( k , R ) N = Find ( k , R ) Rebalance ( N ) Rebalancing: Rebalance ( N : node ) : if | N . Left . Height \u2212 N . Right . Height | \u2264 1 return ; P = N . Parent if N . Left . Height > N . Right . Height + 1 : RebalanceRight ( N ) if N . Right . Height > N . Left . Height + 1 : RebalanceLeft ( N ) AdjustHeight ( N ) if P != null : Rebalance ( P ) Adjusting Hieght : recalculate height after rebalancing the tree AdjustHeight ( N ) : N . Height = 1 + max ( N . Left . Height , N . Right . Height ) exception: consider this case: where the left subtree too heavy. so we need to use different rebalancing function RebalanceRight(N) . RebalanceRight \\(Node\\) : RebalanceRight ( N : node ) : M = N . Left if M . Right . Height > M . Left . Height : RotateLeft ( M ) RotateRight ( N ) AdjustHeight () on affected nodes RotateLeft() Example: deleting from AVL tree \u00b6 Deletions can also change balance. deleting from AVL tree: AVLDelete ( N : node ) : Delete ( N ) M = Parent of node replacing N Rebalance ( M ) Merge AVL trees \u00b6 Merge Combines two binary search trees into a single one. If we got extra root T we do the merge over it: MergeWithRoot ( R1 : tree1 , R2 : tree2 , T : new element to merge over ) : T . Left = R1 T . Right = R2 R1 . Parent = T R2 . Parent = T return T if we didn't get that extra element, we need to search for it and the Get new root by removing largest element of left subtree. Merge ( R1 : tree1 , R2 : tree2 ) : T = Find ( \u221e , R1 ) // find largest element Delete ( T ) // remove that element from the tree MergeWithRoot ( R1 , R2 , T ) // use that T as extra element to merge return T to maintain the balance, we merge the smaller tree R2 with a subtree form the bigger tree R1 with the same height as R2 . we Go down side of the bigger tree until merge with a subtree of same height as the smaller tree. we need a new Merge() function: AVLTreeMergeWithRoot ( R1 : tree1 , R2 : tree2 , T : element to merge over ) : if | R1 . Height \u2212 R2 . Height | \u2264 1 : // both trees with same hieght MergeWithRoot ( R1 , R2 , T ) T . Ht = max ( R1 . Height , R2 . Height ) + 1 // hieght of the output merged tree return T else if R1 . Height > R2 . Height : // if R1 is bigger, we merge R2 on subtree of R1 R \u2032 = AVLTreeMergeWithRoot ( R1 . Right , R2 , T ) /* go down R1.right (bigger elements) untill you find a subtree with same hieght as R2. R\u2032 is the newly merged tree between R2 and r1.right */ R1 . Right = R \u2032 // put R\u2032 as right of R1 R \u2032 . Parent = R1 // assign R1 to be the parent of R\u2032 Rebalance ( R1 ) // Rebalance return root of the newly merged rebalanced tree of R1 else if R1 . Height < R2 . Height : // if R2 is bigger, we merge R1 on subtree of R2 R \u2032 = AVLTreeMergeWithRoot ( R1 , R2 . Right , T ) R2 . Right = R \u2032 R \u2032 . Parent = R2 Rebalance ( R2 ) return root of the newly merged rebalanced tree of R2 split AVL trees \u00b6 Split Breaks one binary search tree into two. we search for element x , then we merge all elements bigger than x into one tree, also merge all elements smaller than x into one another tree split(R: tree, X: element) function: Split ( R : tree , x : element to split over ) : if R = null : // tree is empty return ( null , null ) if x \u2264 R . Key : // we work on the left, Right keep untouched ( R1 , R2 ) = Split ( R . Left , x ) R3 = MergeWithRoot ( R2 , R . Right , R ) // we merge all bigger element comes from down with the untouched part of the Right. return ( R1 , R3 ) if x > R . Key : ( R1 , R2 ) = Split ( R . Right , x ) // work on the right, left untouched R3 = MergeWithRoot ( R2 , R . Left , R ) // we merge all smaller element comes from down with the untouched part of the Left. return ( R1 , R3 ) predecessor P of a node N is the node with the largest key smaller than the key of N splay tree \u00b6 animation: https://www.cs.usfca.edu/~galles/visualization/SplayTree.html","title":"Binary search trees"},{"location":"knowledge-base/general/dataStructures/binary_trees/#binary-search-trees","text":"In problems like: Dictionary Search: Find all words that start with some given string. Date Ranges : Find all emails received in a given period. Closest Height:Find the person in your class whose height is closest to yours. Local Search : A Local Search Datastructure stores a number of elements each with a key coming from an ordered set. It supports operations: RangeSearch(x, y) : Returns all elements with keys between x and y. NearestNeighbors(z) : Returns the element with keys on either side of z. we can solve these problems by: Hash tables: Array: Sorted Array: Linked lists:","title":"Binary search trees"},{"location":"knowledge-base/general/dataStructures/binary_trees/#search-tree","text":"for any Node X in the tree: X\u2019s key is larger than the key of any descendent of its left child, and smaller than the key of any descendant of its right child.","title":"search Tree"},{"location":"knowledge-base/general/dataStructures/binary_trees/#search-tree-functions","text":"find \\(key, root\\) : Find ( k , R ) : if R . Key = k : return R else if R . Key > k : if R . Left \u0338 = null : return Find ( k , R . Left ) return R else if R . Key < k : return Find ( k , R . Right ) Next \\(Node\\) : Next ( N ) if N . Right \u0338 = null : return LeftDescendant ( N . Right ) else : return RightAncestor ( N ) LeftDescendant \\(Node\\) : LeftDescendant ( N ) if N . Left = null return N else : return LeftDescendant ( N . Left ) Right Ancestor \\(Node\\) : RightAncestor ( N ) : if N . Key < N . Parent . Key return N . Parent else : return RightAncestor ( N . Parent ) RangeSearch \\(x = first element in search range , y = second element to search , R = tree or root\\) RangeSearch ( x , y , R ) : L \u2190 \u2205 N \u2190 Find ( x , R ) while N . Key \u2264 y if N . Key \u2265 x : L \u2190 L . Append ( N ) N \u2190 Next ( N ) return L Insert \\(key, tree or root\\) : Insert ( k , R ) : P \u2190 Find ( k , R ) Add new node with key k as child of P delete \\(Node\\) : Delete ( N ) : if N . Right = null : Remove N , promote N . Left else : X \u2190 Next ( N ) \u2216\u2216 X . Left = null Replace N by X , promote X . Right Example: deleting Node \\(1\\) : bring its next element \\(2\\) , to be in \\(1\\) place. bring \\(4\\) tree to be in \\(2\\) place.","title":"search Tree functions"},{"location":"knowledge-base/general/dataStructures/binary_trees/#avl-trees","text":"to keep our trees balanced , \\(the hieght of left = hieght of Right\\) . hieght of tree : the maximum depth of any of its children. calculating the hieght of a tree: Height ( N ) : if N is a leaf : hieght = 1 else : hieght = 1 + max ( N . Left . Height , N . Right . Height ) example of Node after adding hieght property: updating trees can destroy their balance.","title":"AVL trees"},{"location":"knowledge-base/general/dataStructures/binary_trees/#insertion-into-avl-tree","text":"We need a new insertion algorithm that involves rebalancing the tree to maintain the AVL property. insertion idea: AVLInsert ( k : key , R : root ) : Insert ( k , R ) N = Find ( k , R ) Rebalance ( N ) Rebalancing: Rebalance ( N : node ) : if | N . Left . Height \u2212 N . Right . Height | \u2264 1 return ; P = N . Parent if N . Left . Height > N . Right . Height + 1 : RebalanceRight ( N ) if N . Right . Height > N . Left . Height + 1 : RebalanceLeft ( N ) AdjustHeight ( N ) if P != null : Rebalance ( P ) Adjusting Hieght : recalculate height after rebalancing the tree AdjustHeight ( N ) : N . Height = 1 + max ( N . Left . Height , N . Right . Height ) exception: consider this case: where the left subtree too heavy. so we need to use different rebalancing function RebalanceRight(N) . RebalanceRight \\(Node\\) : RebalanceRight ( N : node ) : M = N . Left if M . Right . Height > M . Left . Height : RotateLeft ( M ) RotateRight ( N ) AdjustHeight () on affected nodes RotateLeft() Example:","title":"Insertion into AVL tree"},{"location":"knowledge-base/general/dataStructures/binary_trees/#deleting-from-avl-tree","text":"Deletions can also change balance. deleting from AVL tree: AVLDelete ( N : node ) : Delete ( N ) M = Parent of node replacing N Rebalance ( M )","title":"deleting from AVL tree"},{"location":"knowledge-base/general/dataStructures/binary_trees/#merge-avl-trees","text":"Merge Combines two binary search trees into a single one. If we got extra root T we do the merge over it: MergeWithRoot ( R1 : tree1 , R2 : tree2 , T : new element to merge over ) : T . Left = R1 T . Right = R2 R1 . Parent = T R2 . Parent = T return T if we didn't get that extra element, we need to search for it and the Get new root by removing largest element of left subtree. Merge ( R1 : tree1 , R2 : tree2 ) : T = Find ( \u221e , R1 ) // find largest element Delete ( T ) // remove that element from the tree MergeWithRoot ( R1 , R2 , T ) // use that T as extra element to merge return T to maintain the balance, we merge the smaller tree R2 with a subtree form the bigger tree R1 with the same height as R2 . we Go down side of the bigger tree until merge with a subtree of same height as the smaller tree. we need a new Merge() function: AVLTreeMergeWithRoot ( R1 : tree1 , R2 : tree2 , T : element to merge over ) : if | R1 . Height \u2212 R2 . Height | \u2264 1 : // both trees with same hieght MergeWithRoot ( R1 , R2 , T ) T . Ht = max ( R1 . Height , R2 . Height ) + 1 // hieght of the output merged tree return T else if R1 . Height > R2 . Height : // if R1 is bigger, we merge R2 on subtree of R1 R \u2032 = AVLTreeMergeWithRoot ( R1 . Right , R2 , T ) /* go down R1.right (bigger elements) untill you find a subtree with same hieght as R2. R\u2032 is the newly merged tree between R2 and r1.right */ R1 . Right = R \u2032 // put R\u2032 as right of R1 R \u2032 . Parent = R1 // assign R1 to be the parent of R\u2032 Rebalance ( R1 ) // Rebalance return root of the newly merged rebalanced tree of R1 else if R1 . Height < R2 . Height : // if R2 is bigger, we merge R1 on subtree of R2 R \u2032 = AVLTreeMergeWithRoot ( R1 , R2 . Right , T ) R2 . Right = R \u2032 R \u2032 . Parent = R2 Rebalance ( R2 ) return root of the newly merged rebalanced tree of R2","title":"Merge AVL trees"},{"location":"knowledge-base/general/dataStructures/binary_trees/#split-avl-trees","text":"Split Breaks one binary search tree into two. we search for element x , then we merge all elements bigger than x into one tree, also merge all elements smaller than x into one another tree split(R: tree, X: element) function: Split ( R : tree , x : element to split over ) : if R = null : // tree is empty return ( null , null ) if x \u2264 R . Key : // we work on the left, Right keep untouched ( R1 , R2 ) = Split ( R . Left , x ) R3 = MergeWithRoot ( R2 , R . Right , R ) // we merge all bigger element comes from down with the untouched part of the Right. return ( R1 , R3 ) if x > R . Key : ( R1 , R2 ) = Split ( R . Right , x ) // work on the right, left untouched R3 = MergeWithRoot ( R2 , R . Left , R ) // we merge all smaller element comes from down with the untouched part of the Left. return ( R1 , R3 ) predecessor P of a node N is the node with the largest key smaller than the key of N","title":"split AVL trees"},{"location":"knowledge-base/general/dataStructures/binary_trees/#splay-tree","text":"animation: https://www.cs.usfca.edu/~galles/visualization/SplayTree.html","title":"splay tree"},{"location":"knowledge-base/general/dataStructures/data_structures/","text":"data structures \u00b6 course here: https://www.coursera.org/learn/data-structures/home/welcome week1: Arrays \u00b6 one contiguous area of memory. equal sized elements, indexed by contiguous integers. constant time access for each element. In column-major ordering, the first index changes most rapidly. In row-major ordering, the second index changes most rapidly. costs of operations on arrays depending on the position of the element:","title":"data structures"},{"location":"knowledge-base/general/dataStructures/data_structures/#data-structures","text":"course here: https://www.coursera.org/learn/data-structures/home/welcome","title":"data structures"},{"location":"knowledge-base/general/dataStructures/data_structures/#week1-arrays","text":"one contiguous area of memory. equal sized elements, indexed by contiguous integers. constant time access for each element. In column-major ordering, the first index changes most rapidly. In row-major ordering, the second index changes most rapidly. costs of operations on arrays depending on the position of the element:","title":"week1: Arrays"},{"location":"knowledge-base/general/dataStructures/disjoint_setes/","text":"Disjoint Sets \u00b6 Naive Implementations \u00b6 pdf docs A disjoint-set data structure supports the following operations: MakeSet \\(x\\) creates a singleton set {x} Find \\(x\\) returns ID of the set containing x: if x and y lie in the same set, then Find(x) = Find(y) otherwise, Find \\(x\\) \u0338= Find \\(y\\) Union \\(x, y\\) merges two sets containing x and y union psedo: Union ( i , j ) : i_id \u2190 Find ( i ) j_id \u2190 Find ( j ) if i_id = j_id : return m \u2190 min ( i_id , j_id ) for k from 1 to n : if smallest [ k ] in { i_id , j_id } : smallest [ k ] \u2190 m we can do the implementaion with: linked lists id is the tail of each set. with time, sets is going to be longer and longer. * better solution \\(chaeper\\) using: trees of linked lists * id \\(tail\\) is the same for the 2 sets, after calling union() . Disjoint Sets: Efficient Implementations \u00b6 when merging \\(calling union() \\) on 2 trees, we choose to put the shortest \\(least hieght\\) tree under the longer tree, so the resulting tree as munch shallow as possible. unoin by Rank \u00b6 To quickly find a height of a tree, we will keep the height of each subtree in an array rank[1 . . . n] : rank[i] is the height of the subtree whose root is i. now we have 2 arrays: id : showing the root of each element in each subtree. rank : showing the hieght of the tree that this element belongs to. we also call this processure: union by rank heuristic union psedo with rank: Union ( i , j ) i_id \u2190 Find ( i ) j_id \u2190 Find ( j ) if i_id = j_id : return if rank [ i_id ] > rank [ j_id ] : parent [ j_id ] \u2190 i_id else : parent [ i_id ] \u2190 j_id if rank [ i_id ] = rank [ j_id ] : rank [ j_id ] \u2190 rank [ j_id ] + 1 path compression \u00b6 we introduce another array, that will attatch every element to it's final root. so this will save us time in the future. now we have 3 arrays: id : showing the root of the subtree of each element, then the final root of each subtree. rank : showing the hieght of the tree that this element belongs to. roots : showing the final roots for each single element.","title":"Disjoint Sets"},{"location":"knowledge-base/general/dataStructures/disjoint_setes/#disjoint-sets","text":"","title":"Disjoint Sets"},{"location":"knowledge-base/general/dataStructures/disjoint_setes/#naive-implementations","text":"pdf docs A disjoint-set data structure supports the following operations: MakeSet \\(x\\) creates a singleton set {x} Find \\(x\\) returns ID of the set containing x: if x and y lie in the same set, then Find(x) = Find(y) otherwise, Find \\(x\\) \u0338= Find \\(y\\) Union \\(x, y\\) merges two sets containing x and y union psedo: Union ( i , j ) : i_id \u2190 Find ( i ) j_id \u2190 Find ( j ) if i_id = j_id : return m \u2190 min ( i_id , j_id ) for k from 1 to n : if smallest [ k ] in { i_id , j_id } : smallest [ k ] \u2190 m we can do the implementaion with: linked lists id is the tail of each set. with time, sets is going to be longer and longer. * better solution \\(chaeper\\) using: trees of linked lists * id \\(tail\\) is the same for the 2 sets, after calling union() .","title":"Naive Implementations"},{"location":"knowledge-base/general/dataStructures/disjoint_setes/#disjoint-sets-efficient-implementations","text":"when merging \\(calling union() \\) on 2 trees, we choose to put the shortest \\(least hieght\\) tree under the longer tree, so the resulting tree as munch shallow as possible.","title":"Disjoint Sets: Efficient Implementations"},{"location":"knowledge-base/general/dataStructures/disjoint_setes/#unoin-by-rank","text":"To quickly find a height of a tree, we will keep the height of each subtree in an array rank[1 . . . n] : rank[i] is the height of the subtree whose root is i. now we have 2 arrays: id : showing the root of each element in each subtree. rank : showing the hieght of the tree that this element belongs to. we also call this processure: union by rank heuristic union psedo with rank: Union ( i , j ) i_id \u2190 Find ( i ) j_id \u2190 Find ( j ) if i_id = j_id : return if rank [ i_id ] > rank [ j_id ] : parent [ j_id ] \u2190 i_id else : parent [ i_id ] \u2190 j_id if rank [ i_id ] = rank [ j_id ] : rank [ j_id ] \u2190 rank [ j_id ] + 1","title":"unoin by Rank"},{"location":"knowledge-base/general/dataStructures/disjoint_setes/#path-compression","text":"we introduce another array, that will attatch every element to it's final root. so this will save us time in the future. now we have 3 arrays: id : showing the root of the subtree of each element, then the final root of each subtree. rank : showing the hieght of the tree that this element belongs to. roots : showing the final roots for each single element.","title":"path compression"},{"location":"knowledge-base/general/dataStructures/hash_tables/","text":"Hash Tables \u00b6 used in: hashing passwords, connecting files on the os to their physical location, IP address. naiive solution: create an array Arr of all possible IPs, increment Ar[IP] ++; when a new visitor vistits your website, you need another array to deal with time. optimization: use list instead of array, the elements being added corresponding to the time \\(old element in the start, last elemnts to the end\\) . every hour we delete the first list Nodes. hash function : a funtion that takes a universe and divide them into smaller universes. Maps : Store mapping from objects to other objects, Student ID \u2192 student name. Map from S to V is a data structure with methods HasKey \\(O\\) , Get \\(O\\) , Set \\(O, v\\) , where O \u2208 S, v \u2208 V . Set is a data structure with methods: Add \\(O\\) , Remove \\(O\\) , Find \\(O\\) . Two ways to implement a set using chaining: Set is equivalent to map from S to V = {true, false} Store just objects O instead of pairs \\(O, v\\) in chains. parameters in hash tables: n numer of elemnts in the universe m cardinality of the hash function \\(the keys\\) c length of the longest chain memory used O(n + m) \ud835\udefc = n /m is called load factor Operations run in time O(c + 1) Hash Functions \u00b6 univirsal family : Let U be the universe , the set of all possible keys . A set of hash functions \u210b = { h : U \u2192 { 0 , 1 , 2 , . . . , m \u2212 1 }} is called a universal family if for any two keys x , y \u2208 U , x \u0338 = y the probability of collision Pr [ h ( x ) = h ( y )] \u2264 1 / m pdf notes Hashing Integeres \u00b6 pdf notes Hashing strings \u00b6 pdf Notes hashing functions:","title":"Hash Tables"},{"location":"knowledge-base/general/dataStructures/hash_tables/#hash-tables","text":"used in: hashing passwords, connecting files on the os to their physical location, IP address. naiive solution: create an array Arr of all possible IPs, increment Ar[IP] ++; when a new visitor vistits your website, you need another array to deal with time. optimization: use list instead of array, the elements being added corresponding to the time \\(old element in the start, last elemnts to the end\\) . every hour we delete the first list Nodes. hash function : a funtion that takes a universe and divide them into smaller universes. Maps : Store mapping from objects to other objects, Student ID \u2192 student name. Map from S to V is a data structure with methods HasKey \\(O\\) , Get \\(O\\) , Set \\(O, v\\) , where O \u2208 S, v \u2208 V . Set is a data structure with methods: Add \\(O\\) , Remove \\(O\\) , Find \\(O\\) . Two ways to implement a set using chaining: Set is equivalent to map from S to V = {true, false} Store just objects O instead of pairs \\(O, v\\) in chains. parameters in hash tables: n numer of elemnts in the universe m cardinality of the hash function \\(the keys\\) c length of the longest chain memory used O(n + m) \ud835\udefc = n /m is called load factor Operations run in time O(c + 1)","title":"Hash Tables"},{"location":"knowledge-base/general/dataStructures/hash_tables/#hash-functions","text":"univirsal family : Let U be the universe , the set of all possible keys . A set of hash functions \u210b = { h : U \u2192 { 0 , 1 , 2 , . . . , m \u2212 1 }} is called a universal family if for any two keys x , y \u2208 U , x \u0338 = y the probability of collision Pr [ h ( x ) = h ( y )] \u2264 1 / m pdf notes","title":"Hash Functions"},{"location":"knowledge-base/general/dataStructures/hash_tables/#hashing-integeres","text":"pdf notes","title":"Hashing Integeres"},{"location":"knowledge-base/general/dataStructures/hash_tables/#hashing-strings","text":"pdf Notes hashing functions:","title":"Hashing strings"},{"location":"knowledge-base/general/dataStructures/heap/","text":"Heap \u00b6 heap is a tree with special charechters. Binary max-heap is a binary tree \\(each node has zero, one, or two children\\) where the value of each node is at least the values of its children. operations on binary max-heap \u00b6 GetMax : return the root. Insert : attach a new node to any leaf, this may violate the heap property, so we do siftUp . siftUp : swap the problematic node with its parent until the property is satisfied. this edge gets closer to the root while sifting up. costs o(tree height) . ExtractMax : replace the root with any leaf. then we do SiftDown if nessecary. SiftDown : we swap the problematic node with larger child until the heap property is satisfied. ChangePriority : change the priority and let the changed element sift up or down depending on whether its priority decreased or increased. Remove : change the priority of the element to \u221e, let it sift up, and then extract maximum by calling ExtractMax() costs O(tree height) . compelete binary tree \u00b6 A binary tree is complete if all its levels are filled except possibly the last one which is filled from left to right. A complete binary tree with n nodes has height at most O(log n) . we cav store a copmplete binary tree in array as follows: parent index of node i = Arr[round(i/2)] leftchild \\(i\\) = Arr[2i] RightChild \\(i\\) = Arr[2i+1] siftUp and down does not change the tree completeness. insert and extractMax and remove can violate tree completeness. we have to maintain our tree complete. Keeping the Tree Complete: to extract the maximum value, replace the root by the last leaf \\(by the right leaf of the most left child 2i+1 \\) and let it sift down. to insert an element, insert it as a leaf in the leftmost vacant position in the last level and let it sift up. binary heap psuedo code \u00b6 maxSize is the maximum number of elements in the heap size is the size of the heap H[1 . . . maxSize] is an array of length maxSize where the heap occupies the first size elements pdf docs sift Up: SiftUp ( i ) : while i > 1 and H [ Parent ( i )] < H [ i ] : swap H [ Parent ( i )] and H [ i ] i \u2190 Parent ( i ) sift down: SiftDown ( i ) maxIndex \u2190 i \u2113 \u2190 LeftChild ( i ) if \u2113 \u2264 size and H [ \u2113 ] > H [ maxIndex ] : maxIndex \u2190 \u2113 r \u2190 RightChild ( i ) if r \u2264 size and H [ r ] > H [ maxIndex ] : maxIndex \u2190 r if i \u0338 = maxIndex : swap H [ i ] and H [ maxIndex ] SiftDown ( maxIndex ) insert: Insert ( p ) if size = maxSize : return ERROR size \u2190 size + 1 H [ size ] \u2190 p SiftUp ( size ) Extract max: ExtractMax () result \u2190 H [ 1 ] H [ 1 ] \u2190 H [ size ] size \u2190 size \u2212 1 SiftDown ( 1 ) return result remove: Remove ( i ) H [ i ] \u2190 \u221e SiftUp ( i ) ExtractMax () change priority: ChangePriority ( i , p ) oldp \u2190 H [ i ] H [ i ] \u2190 p if p > oldp : SiftUp ( i ) else : SiftDown ( i ) Heap sort \u00b6 psudo code: HeapSort ( A [ 1 . . . n ]) create an empty priority queue for i from 1 to n : Insert ( A [ i ]) for i from n downto 1 : A [ i ] \u2190 ExtractMax () cost : 0(n log n) intro-sort algorithm \u00b6 in practice: you start using quick sort algorithm, if you find it a bit slow: you stop and change to heap sort . 0-based array heap \u00b6 Parent \\(i\\) : return Arr[i\u22121/2] LeftChild \\(i\\) : return Arr[2i + 1] RightChild \\(i\\) : return Arr[2i + 2] Binary min-heap \u00b6 Binary min-heap is a binary tree \\(each node has zero, one, or two children\\) where the value of each node is at most the values of its children. d-ary Heap \u00b6 In a d-ary heap nodes on all levels except for possibly the last one have exactly d children. The height of such a tree is about logd n. The running time of SiftUp is O \\(logd n\\) . The running time of SiftDown is O \\(d logd n\\) : on each level, we \u001cnd the largest value among d children.","title":"Heap"},{"location":"knowledge-base/general/dataStructures/heap/#heap","text":"heap is a tree with special charechters. Binary max-heap is a binary tree \\(each node has zero, one, or two children\\) where the value of each node is at least the values of its children.","title":"Heap"},{"location":"knowledge-base/general/dataStructures/heap/#operations-on-binary-max-heap","text":"GetMax : return the root. Insert : attach a new node to any leaf, this may violate the heap property, so we do siftUp . siftUp : swap the problematic node with its parent until the property is satisfied. this edge gets closer to the root while sifting up. costs o(tree height) . ExtractMax : replace the root with any leaf. then we do SiftDown if nessecary. SiftDown : we swap the problematic node with larger child until the heap property is satisfied. ChangePriority : change the priority and let the changed element sift up or down depending on whether its priority decreased or increased. Remove : change the priority of the element to \u221e, let it sift up, and then extract maximum by calling ExtractMax() costs O(tree height) .","title":"operations on binary max-heap"},{"location":"knowledge-base/general/dataStructures/heap/#compelete-binary-tree","text":"A binary tree is complete if all its levels are filled except possibly the last one which is filled from left to right. A complete binary tree with n nodes has height at most O(log n) . we cav store a copmplete binary tree in array as follows: parent index of node i = Arr[round(i/2)] leftchild \\(i\\) = Arr[2i] RightChild \\(i\\) = Arr[2i+1] siftUp and down does not change the tree completeness. insert and extractMax and remove can violate tree completeness. we have to maintain our tree complete. Keeping the Tree Complete: to extract the maximum value, replace the root by the last leaf \\(by the right leaf of the most left child 2i+1 \\) and let it sift down. to insert an element, insert it as a leaf in the leftmost vacant position in the last level and let it sift up.","title":"compelete binary tree"},{"location":"knowledge-base/general/dataStructures/heap/#binary-heap-psuedo-code","text":"maxSize is the maximum number of elements in the heap size is the size of the heap H[1 . . . maxSize] is an array of length maxSize where the heap occupies the first size elements pdf docs sift Up: SiftUp ( i ) : while i > 1 and H [ Parent ( i )] < H [ i ] : swap H [ Parent ( i )] and H [ i ] i \u2190 Parent ( i ) sift down: SiftDown ( i ) maxIndex \u2190 i \u2113 \u2190 LeftChild ( i ) if \u2113 \u2264 size and H [ \u2113 ] > H [ maxIndex ] : maxIndex \u2190 \u2113 r \u2190 RightChild ( i ) if r \u2264 size and H [ r ] > H [ maxIndex ] : maxIndex \u2190 r if i \u0338 = maxIndex : swap H [ i ] and H [ maxIndex ] SiftDown ( maxIndex ) insert: Insert ( p ) if size = maxSize : return ERROR size \u2190 size + 1 H [ size ] \u2190 p SiftUp ( size ) Extract max: ExtractMax () result \u2190 H [ 1 ] H [ 1 ] \u2190 H [ size ] size \u2190 size \u2212 1 SiftDown ( 1 ) return result remove: Remove ( i ) H [ i ] \u2190 \u221e SiftUp ( i ) ExtractMax () change priority: ChangePriority ( i , p ) oldp \u2190 H [ i ] H [ i ] \u2190 p if p > oldp : SiftUp ( i ) else : SiftDown ( i )","title":"binary heap psuedo code"},{"location":"knowledge-base/general/dataStructures/heap/#heap-sort","text":"psudo code: HeapSort ( A [ 1 . . . n ]) create an empty priority queue for i from 1 to n : Insert ( A [ i ]) for i from n downto 1 : A [ i ] \u2190 ExtractMax () cost : 0(n log n)","title":"Heap sort"},{"location":"knowledge-base/general/dataStructures/heap/#intro-sort-algorithm","text":"in practice: you start using quick sort algorithm, if you find it a bit slow: you stop and change to heap sort .","title":"intro-sort algorithm"},{"location":"knowledge-base/general/dataStructures/heap/#0-based-array-heap","text":"Parent \\(i\\) : return Arr[i\u22121/2] LeftChild \\(i\\) : return Arr[2i + 1] RightChild \\(i\\) : return Arr[2i + 2]","title":"0-based array heap"},{"location":"knowledge-base/general/dataStructures/heap/#binary-min-heap","text":"Binary min-heap is a binary tree \\(each node has zero, one, or two children\\) where the value of each node is at most the values of its children.","title":"Binary min-heap"},{"location":"knowledge-base/general/dataStructures/heap/#d-ary-heap","text":"In a d-ary heap nodes on all levels except for possibly the last one have exactly d children. The height of such a tree is about logd n. The running time of SiftUp is O \\(logd n\\) . The running time of SiftDown is O \\(d logd n\\) : on each level, we \u001cnd the largest value among d children.","title":"d-ary Heap"},{"location":"knowledge-base/general/dataStructures/linked_lists/","text":"Linked Lists \u00b6 intro \u00b6 Linked Lists is used for storing data when the size of the data can dynamically increase (is not known prior). pointers and strucures (classes) are critical in linked lists. Self Referential Structures \u00b6 A Self Referential Structure (we would call it SRS from now) is a special structure which contains a member variable that points to structure of its own kind [1]. //c++ struct book { member 1 ; member 2 ; // Other Members struct book * next ; } why linked lists ? \u00b6 we can use arrays of structures, but the array size should be predefined or give a big number to array size beforehand. defention \u00b6 the linked list is an array of structure objects where the structure needs to be self referential and the ith element in the list points to the (i+1)th element through its member pointer and the pointer of the last element of the list would point to NULL. The starting element will be called as the head node [1]. singley linked lists \u00b6 head pointer points to a node, whitch contains data \\(key\\) and another pointer to the next node. ops \\(read, add, delete\\) on the first node: cost o(1) . ops on the last: if the pointer of the last pointer is empty (no tail) : cost o(n) if there is a tail pointer : cost o(1) exept remove costs o(n) cause the pointers don't have a way to point backward. push front psuedo code: https://i.imgur.com/AICcQZm.png pop \\(remove\\) front psuedo: https://i.imgur.com/4OYm71r.png push back \\(last element\\) : https://i.imgur.com/EbKamiz.png pop back : https://i.imgur.com/6NlFNAz.png Add After a node: https://i.imgur.com/hf9t89q.png head == tail -> the list contains only one node. double linked lists \u00b6 head pointer points to a node, whitch contains data \\(key\\) and 2 pointers to the next and previous node. popBack and addBefore is now cheaper o(1) . pushBack psuedo: https://i.imgur.com/iSbGeCB.png popBack : https://i.imgur.com/1TgK8Eu.png addAfter: https://i.imgur.com/s5PP86S.png addBefore: https://i.imgur.com/A2FsCHA.png References \u00b6 [1] https://courses.edx.org/courses/course-v1:IITBombayX+CS101.2x+1T2020/courseware/a71234edfd4f465e8d3e7e0082d04540/7565e9f168914c6cab4d54f9ba443bca/?child=first","title":"Linked Lists"},{"location":"knowledge-base/general/dataStructures/linked_lists/#linked-lists","text":"","title":"Linked Lists"},{"location":"knowledge-base/general/dataStructures/linked_lists/#intro","text":"Linked Lists is used for storing data when the size of the data can dynamically increase (is not known prior). pointers and strucures (classes) are critical in linked lists.","title":"intro"},{"location":"knowledge-base/general/dataStructures/linked_lists/#self-referential-structures","text":"A Self Referential Structure (we would call it SRS from now) is a special structure which contains a member variable that points to structure of its own kind [1]. //c++ struct book { member 1 ; member 2 ; // Other Members struct book * next ; }","title":"Self Referential Structures"},{"location":"knowledge-base/general/dataStructures/linked_lists/#why-linked-lists","text":"we can use arrays of structures, but the array size should be predefined or give a big number to array size beforehand.","title":"why linked lists ?"},{"location":"knowledge-base/general/dataStructures/linked_lists/#defention","text":"the linked list is an array of structure objects where the structure needs to be self referential and the ith element in the list points to the (i+1)th element through its member pointer and the pointer of the last element of the list would point to NULL. The starting element will be called as the head node [1].","title":"defention"},{"location":"knowledge-base/general/dataStructures/linked_lists/#singley-linked-lists","text":"head pointer points to a node, whitch contains data \\(key\\) and another pointer to the next node. ops \\(read, add, delete\\) on the first node: cost o(1) . ops on the last: if the pointer of the last pointer is empty (no tail) : cost o(n) if there is a tail pointer : cost o(1) exept remove costs o(n) cause the pointers don't have a way to point backward. push front psuedo code: https://i.imgur.com/AICcQZm.png pop \\(remove\\) front psuedo: https://i.imgur.com/4OYm71r.png push back \\(last element\\) : https://i.imgur.com/EbKamiz.png pop back : https://i.imgur.com/6NlFNAz.png Add After a node: https://i.imgur.com/hf9t89q.png head == tail -> the list contains only one node.","title":"singley linked lists"},{"location":"knowledge-base/general/dataStructures/linked_lists/#double-linked-lists","text":"head pointer points to a node, whitch contains data \\(key\\) and 2 pointers to the next and previous node. popBack and addBefore is now cheaper o(1) . pushBack psuedo: https://i.imgur.com/iSbGeCB.png popBack : https://i.imgur.com/1TgK8Eu.png addAfter: https://i.imgur.com/s5PP86S.png addBefore: https://i.imgur.com/A2FsCHA.png","title":"double linked lists"},{"location":"knowledge-base/general/dataStructures/linked_lists/#references","text":"[1] https://courses.edx.org/courses/course-v1:IITBombayX+CS101.2x+1T2020/courseware/a71234edfd4f465e8d3e7e0082d04540/7565e9f168914c6cab4d54f9ba443bca/?child=first","title":"References"},{"location":"knowledge-base/general/dataStructures/priority_queue/","text":"priority queue \u00b6 A priority queue is a generalization of a queue where each element is assigned a priority and elements come out in order by priority. Priority queue is an abstract data type supporting the following main operations: Insert \\(p\\) adds a new element with priority p ExtractMax() extracts an element with maximum priority. Remove \\(it\\) removes an element pointed by an iterator it GetMax() returns an element with maximum priority \\(without changing the set of elements\\) ChangePriority \\(it, p\\) changes the priority of an element pointed by it to p more info on pdf: pdf slides","title":"priority queue"},{"location":"knowledge-base/general/dataStructures/priority_queue/#priority-queue","text":"A priority queue is a generalization of a queue where each element is assigned a priority and elements come out in order by priority. Priority queue is an abstract data type supporting the following main operations: Insert \\(p\\) adds a new element with priority p ExtractMax() extracts an element with maximum priority. Remove \\(it\\) removes an element pointed by an iterator it GetMax() returns an element with maximum priority \\(without changing the set of elements\\) ChangePriority \\(it, p\\) changes the priority of an element pointed by it to p more info on pdf: pdf slides","title":"priority queue"},{"location":"knowledge-base/general/dataStructures/queue/","text":"queue \u00b6 first in, first out. FIFO same as stack, except it will remove the first added element. the longest waiting person in line, the next to be served. Queue: Abstract data type with the following operations: Enqueue \\(Key\\) : adds key to collection Key Dequeue(): removes and returns least recently-added key Boolean Empty(): are there any elements? useful for servers we can do queues with arrays or linked lists. in a linked list queue : add using pushBack to the back (tail) of the list. enqueue remove using topFront to get the element, then popFront to remove e from the head of the list. dequeue queue with Arrays : normal array: adding is easy, removing first element costs o(n) to make dequeuing cheaper 0(1) : keep track of array as acercular array. we make a variablle read to track the index to remove from. variable write to track the index to add to. if read == write -> queue is empty. if write > arr.length && read != 0 -> read = 0 , start from the begining. if we start from the begining, we have to put a buffer of at least one index empty between the read and write indexes as in this example https://i.imgur.com/OYKISSU.png","title":"queue"},{"location":"knowledge-base/general/dataStructures/queue/#queue","text":"first in, first out. FIFO same as stack, except it will remove the first added element. the longest waiting person in line, the next to be served. Queue: Abstract data type with the following operations: Enqueue \\(Key\\) : adds key to collection Key Dequeue(): removes and returns least recently-added key Boolean Empty(): are there any elements? useful for servers we can do queues with arrays or linked lists. in a linked list queue : add using pushBack to the back (tail) of the list. enqueue remove using topFront to get the element, then popFront to remove e from the head of the list. dequeue queue with Arrays : normal array: adding is easy, removing first element costs o(n) to make dequeuing cheaper 0(1) : keep track of array as acercular array. we make a variablle read to track the index to remove from. variable write to track the index to add to. if read == write -> queue is empty. if write > arr.length && read != 0 -> read = 0 , start from the begining. if we start from the begining, we have to put a buffer of at least one index empty between the read and write indexes as in this example https://i.imgur.com/OYKISSU.png","title":"queue"},{"location":"knowledge-base/general/dataStructures/stack/","text":"stack \u00b6 last in, first out. LIFO Stack: Abstract data type with the following operations: Push \\(Key\\) : adds key to collection Key Top(): returns most recently-added key Key Pop(): removes and returns most recently-added key Boolean Empty(): are there any elements? solves balanced brackets problem : check if every prace has its own closing prace . psuedo code for balanced brackets: https://i.imgur.com/xIucIM7.png Stacks can be implemented with either an array or a linked list . Each stack operation is O(1) : Push, Pop, Top, Empty. Stacks are ocassionaly known as LIFO queues . in a Array stack : adding the new elemnt of the stack to the end of the array, remove the last e of the array. array is limmited to the number of elements initialized with \\(in some languages\\) . add using pushBack remove with popBack in a linked list stack : adding the new element to the begining \\(head\\) of the linked list, remove the head. add with pushFront remove with popfront","title":"stack"},{"location":"knowledge-base/general/dataStructures/stack/#stack","text":"last in, first out. LIFO Stack: Abstract data type with the following operations: Push \\(Key\\) : adds key to collection Key Top(): returns most recently-added key Key Pop(): removes and returns most recently-added key Boolean Empty(): are there any elements? solves balanced brackets problem : check if every prace has its own closing prace . psuedo code for balanced brackets: https://i.imgur.com/xIucIM7.png Stacks can be implemented with either an array or a linked list . Each stack operation is O(1) : Push, Pop, Top, Empty. Stacks are ocassionaly known as LIFO queues . in a Array stack : adding the new elemnt of the stack to the end of the array, remove the last e of the array. array is limmited to the number of elements initialized with \\(in some languages\\) . add using pushBack remove with popBack in a linked list stack : adding the new element to the begining \\(head\\) of the linked list, remove the head. add with pushFront remove with popfront","title":"stack"},{"location":"knowledge-base/general/dataStructures/trees/","text":"trees \u00b6 A Tree is: empty, or a node with: a key, and a list of child trees. Root: top node in the tree. A child has a line down directly from a parent. Ancestor: parent, or parent of parent, etc Descendant: child, or child of child, etc. Leaf : node with no children. Interior node \\(non-leaf\\) . Height: maximum depth of subtree node and farthest leaf Forest: collection of trees. binary tree : a tree with most 2 children, lift and right. Height \\(tree\\) if tree = nil : return 0 return 1 + Max ( Height ( tree . left ), Height ( tree . right )) Size \\(tree\\) if tree = nil return 0 return 1 + Size ( tree . left ) + Size ( tree . right ) Depth-first tree traversal \\(for binary trees\\) \u00b6 we use stack . we start from trees dont have children. tree = nill InOrderTraversal \\(tree\\) : traverse lift then key then right tree. if tree = nil : return InOrderTraversal ( tree . left ) Print ( tree . key ) InOrderTraversal ( tree . right ) PreOrderTraversal \\(tree\\) : we traverse the key then the left then right trees. if tree = nil : return Print ( tree . key ) PreOrderTraversal ( tree . left ) PreOrderTraversal ( tree . right ) PostOrderTraversal \\(tree\\) we traverse the left then the right trees and lastly the key. if tree = nil : return PostOrderTraversal ( tree . left ) PostOrderTraversal ( tree . right ) Print ( tree . key ) Breadth-first traversal \u00b6 we use queue LevelTraversal \\(tree\\) pre-order traversion level by level. if tree = nil : return Queue q q . Enqueue ( tree ) while not q . Empty () : node \u2190 q . Dequeue () Print ( node ) if node . left \u0338 = nil : q . Enqueue ( node . left ) if node . right \u0338 = nil : q . Enqueue ( node . right )","title":"trees"},{"location":"knowledge-base/general/dataStructures/trees/#trees","text":"A Tree is: empty, or a node with: a key, and a list of child trees. Root: top node in the tree. A child has a line down directly from a parent. Ancestor: parent, or parent of parent, etc Descendant: child, or child of child, etc. Leaf : node with no children. Interior node \\(non-leaf\\) . Height: maximum depth of subtree node and farthest leaf Forest: collection of trees. binary tree : a tree with most 2 children, lift and right. Height \\(tree\\) if tree = nil : return 0 return 1 + Max ( Height ( tree . left ), Height ( tree . right )) Size \\(tree\\) if tree = nil return 0 return 1 + Size ( tree . left ) + Size ( tree . right )","title":"trees"},{"location":"knowledge-base/general/dataStructures/trees/#depth-first-tree-traversal-for-binary-trees","text":"we use stack . we start from trees dont have children. tree = nill InOrderTraversal \\(tree\\) : traverse lift then key then right tree. if tree = nil : return InOrderTraversal ( tree . left ) Print ( tree . key ) InOrderTraversal ( tree . right ) PreOrderTraversal \\(tree\\) : we traverse the key then the left then right trees. if tree = nil : return Print ( tree . key ) PreOrderTraversal ( tree . left ) PreOrderTraversal ( tree . right ) PostOrderTraversal \\(tree\\) we traverse the left then the right trees and lastly the key. if tree = nil : return PostOrderTraversal ( tree . left ) PostOrderTraversal ( tree . right ) Print ( tree . key )","title":"Depth-first tree traversal \\(for binary trees\\)"},{"location":"knowledge-base/general/dataStructures/trees/#breadth-first-traversal","text":"we use queue LevelTraversal \\(tree\\) pre-order traversion level by level. if tree = nil : return Queue q q . Enqueue ( tree ) while not q . Empty () : node \u2190 q . Dequeue () Print ( node ) if node . left \u0338 = nil : q . Enqueue ( node . left ) if node . right \u0338 = nil : q . Enqueue ( node . right )","title":"Breadth-first traversal"},{"location":"knowledge-base/general/databases/","text":"","title":"Index"},{"location":"knowledge-base/general/databases/mongo/","text":"MongoDB \u00b6 Reference: mongo shell quick reference list all dbs in the mongo shell: show dbs work with one db: use dbs extracting the date from ObjectId : const date = new Date ( parseInt ( doc . _id . substring ( 0 , 8 ), 16 ) * 1000 ); // new date object when this object created. remove and element from a nested array: { $pull : { workshops : { _id : workshopId } } } collection functions \u00b6 collection.find() alaways need .toArray() after it.","title":"MongoDB"},{"location":"knowledge-base/general/databases/mongo/#mongodb","text":"Reference: mongo shell quick reference list all dbs in the mongo shell: show dbs work with one db: use dbs extracting the date from ObjectId : const date = new Date ( parseInt ( doc . _id . substring ( 0 , 8 ), 16 ) * 1000 ); // new date object when this object created. remove and element from a nested array: { $pull : { workshops : { _id : workshopId } } }","title":"MongoDB"},{"location":"knowledge-base/general/databases/mongo/#collection-functions","text":"collection.find() alaways need .toArray() after it.","title":"collection functions"},{"location":"knowledge-base/general/databases/mongoose/","text":"Mongoose \u00b6 Notes \u00b6 convert objectId to string doc . _id . toHexString (); doc . _id . toString ();","title":"Mongoose"},{"location":"knowledge-base/general/databases/mongoose/#mongoose","text":"","title":"Mongoose"},{"location":"knowledge-base/general/databases/mongoose/#notes","text":"convert objectId to string doc . _id . toHexString (); doc . _id . toString ();","title":"Notes"},{"location":"knowledge-base/general/databases/redis/","text":"Redis \u00b6 redis on windows will never work with docker unless you bind the IP of the docker machine to match the ip of your local machine or vice-versa. redis on windows, you need to start the service manually by: ctrl + alt + del go to service tab search for Redis and start the service. resources \u00b6 https://redis.io/documentation","title":"Redis"},{"location":"knowledge-base/general/databases/redis/#redis","text":"redis on windows will never work with docker unless you bind the IP of the docker machine to match the ip of your local machine or vice-versa. redis on windows, you need to start the service manually by: ctrl + alt + del go to service tab search for Redis and start the service.","title":"Redis"},{"location":"knowledge-base/general/databases/redis/#resources","text":"https://redis.io/documentation","title":"resources"},{"location":"knowledge-base/general/databases/sequilize/","text":"Sequalize \u00b6 sequlize is an object object relational mapper. that maps the normal RDS (relational data bases) tables into objects so that it will go along with OOP languages. saves you fromm writting SQL commands or query. pros \u00b6 less and more consistent code. No SQl queries. abstract DB engines. does a lot of things automatically. Migrations are easier. cons \u00b6 complicated queries might be slow. you might loss your DB knolowedge with time. usage \u00b6 install Sequalize, AND , install DB engine like: mysql, sqlite ... 1 - connection \u00b6 const Sequalize = require ( 'sequalize' ); // generator function const connection = new Sequalize ( \"DB name\" , \"username\" , \"password\" , { dialect : \"mysql as default\" /* optional */ }); 2 -creating a model \u00b6 const MyModel = connection . define ( \"MyModel\" , { // Model properties field1 : Sequalize . STRING , field2 : Sequalize . TEXT }) 3 - executing \u00b6 synchronization with DB: connecting to the DB automaticallu generate SQL and execute it to create the tables (if not already exists) From your defined Models. IMPORTANT: Sequalize will not create the DB automatically, you have to create the DB yourself. const options = { logging : true , force : true , // forcing updat tables ** NOT RECOMMNDED FOR PRODUCTIO** } connection . sync ( options ); // entry point for squakize work // this is async function, you have to await it or use promises 4 - create a doc (row) \u00b6 // you need to await connection.sync() first MyModel . create ({ // executes INSERT query filed1 : \"data\" , field2 : \"data .........\" }) 5- Find (SELECT) \u00b6 // await synch // Find one MyModel . findById ( id ). then ( function ( result ){ console . log ( result . dataValues ); // object // your results in results.dataValues }); // Find All MyModel . findAll (). then ( function ( result ){ console . log ( result . dataValues ); // Array<object> // your results in results.dataValues }); 6- Advanced Models \u00b6 ```js const MyModel = connection.define(\"MyModel\", { // Model properties field1: Sequalize.STRING, field2: { type: Sequalize.STRING, // type of field allowNull: true, // true or false unique: true, // true or false defaultValue: \"default value ..\", // if no value is provided validate: { // validate data before inserting is : \"string or regex\", // is the data equals to a string or regex len: [2,10], // string length should be between 2 and 10 /* respond with custom error message */ len : { args: [2.10], // value of the validation msg: \"please enter stiri ng with length between 2 and 10\", // this msg will be sent in case of error here }, /* cutom validations */ myValidationRuleName : function(fieldValue){ var validation; /* do you checks on the field value and return boolean store it on validation */ if(!validation){ throw new Error(' validation failed, the value you have entered is not accepted'); } else { // don't do nothing, but you have to write this else, it is a syntax. // if no error thrown => the value is valid } } }, primaryKey: true, // this field2 key, id field is deleted automatically. }, { // model options timestamps: false, freezeTableName: true, // prevent sequalize from giving our table a plural name hooks:{ // middlewares or function to applied to every doc. // all hooks are here beforeCreate: function(){ }, beforeValidate: () => {}, afterValidate: () => {}, afterCreate: (result) => { console.log(result.dataValues); }, } } }) ``` VERY IMPORTANT: connection.sync() will never update a table structure if it's already created. \u00b6 If you want to update the table structure ( like adding or removing columns) you can do one of the following: you have to delete the table first. (table data will be lost ) create new model with different name, in order to not lost the data. apply Migrations. apply force as following: NOT RECOMMENDED For Production js connection.sync({ force: true }); // all data in the table will be deleted IMPORTANT: allowNull \u00b6 allowNull is true by default if you DID NOT specify allowNull: false , any validation to this field will not be applied (as if you have NO VALIDATION at all). 7- CRUD \u00b6 ```js connection.sync(options).then(() => { const createOptions = { // options for create function fields:[\"field1\", \"field2\"], // white listed fields, fields that allowed to be submitted by user request } MyModel.create({ /* my doc data */ }, createOptions); // creating a doc then save to the database var doc1 = MyModel.build({ /* my doc data */ }); // create doc, Don't save. interact with the doc sync without waiting db saving doc1.save(); // save the doc created by build method const bulkCreateOptions = { // options for bulk create fields:[\"field1\",\"field2\"], validate: true, // bydefault createBulk skip validation, you have to specify the validate to true to run them ignoreDuplicate: true, // skip duplication error if one the documents already exists, continue creating other documents } MyModel.bulkCreate([doc1, doc2], bulkCreateOptions); // create multiple documents at once }) ``` 8- Realationships between documents \u00b6","title":"Sequalize"},{"location":"knowledge-base/general/databases/sequilize/#sequalize","text":"sequlize is an object object relational mapper. that maps the normal RDS (relational data bases) tables into objects so that it will go along with OOP languages. saves you fromm writting SQL commands or query.","title":"Sequalize"},{"location":"knowledge-base/general/databases/sequilize/#pros","text":"less and more consistent code. No SQl queries. abstract DB engines. does a lot of things automatically. Migrations are easier.","title":"pros"},{"location":"knowledge-base/general/databases/sequilize/#cons","text":"complicated queries might be slow. you might loss your DB knolowedge with time.","title":"cons"},{"location":"knowledge-base/general/databases/sequilize/#usage","text":"install Sequalize, AND , install DB engine like: mysql, sqlite ...","title":"usage"},{"location":"knowledge-base/general/databases/sequilize/#1-connection","text":"const Sequalize = require ( 'sequalize' ); // generator function const connection = new Sequalize ( \"DB name\" , \"username\" , \"password\" , { dialect : \"mysql as default\" /* optional */ });","title":"1 - connection"},{"location":"knowledge-base/general/databases/sequilize/#2-creating-a-model","text":"const MyModel = connection . define ( \"MyModel\" , { // Model properties field1 : Sequalize . STRING , field2 : Sequalize . TEXT })","title":"2 -creating a model"},{"location":"knowledge-base/general/databases/sequilize/#3-executing","text":"synchronization with DB: connecting to the DB automaticallu generate SQL and execute it to create the tables (if not already exists) From your defined Models. IMPORTANT: Sequalize will not create the DB automatically, you have to create the DB yourself. const options = { logging : true , force : true , // forcing updat tables ** NOT RECOMMNDED FOR PRODUCTIO** } connection . sync ( options ); // entry point for squakize work // this is async function, you have to await it or use promises","title":"3 - executing"},{"location":"knowledge-base/general/databases/sequilize/#4-create-a-doc-row","text":"// you need to await connection.sync() first MyModel . create ({ // executes INSERT query filed1 : \"data\" , field2 : \"data .........\" })","title":"4 - create a doc (row)"},{"location":"knowledge-base/general/databases/sequilize/#5-find-select","text":"// await synch // Find one MyModel . findById ( id ). then ( function ( result ){ console . log ( result . dataValues ); // object // your results in results.dataValues }); // Find All MyModel . findAll (). then ( function ( result ){ console . log ( result . dataValues ); // Array<object> // your results in results.dataValues });","title":"5- Find (SELECT)"},{"location":"knowledge-base/general/databases/sequilize/#6-advanced-models","text":"```js const MyModel = connection.define(\"MyModel\", { // Model properties field1: Sequalize.STRING, field2: { type: Sequalize.STRING, // type of field allowNull: true, // true or false unique: true, // true or false defaultValue: \"default value ..\", // if no value is provided validate: { // validate data before inserting is : \"string or regex\", // is the data equals to a string or regex len: [2,10], // string length should be between 2 and 10 /* respond with custom error message */ len : { args: [2.10], // value of the validation msg: \"please enter stiri ng with length between 2 and 10\", // this msg will be sent in case of error here }, /* cutom validations */ myValidationRuleName : function(fieldValue){ var validation; /* do you checks on the field value and return boolean store it on validation */ if(!validation){ throw new Error(' validation failed, the value you have entered is not accepted'); } else { // don't do nothing, but you have to write this else, it is a syntax. // if no error thrown => the value is valid } } }, primaryKey: true, // this field2 key, id field is deleted automatically. }, { // model options timestamps: false, freezeTableName: true, // prevent sequalize from giving our table a plural name hooks:{ // middlewares or function to applied to every doc. // all hooks are here beforeCreate: function(){ }, beforeValidate: () => {}, afterValidate: () => {}, afterCreate: (result) => { console.log(result.dataValues); }, } } }) ```","title":"6- Advanced Models"},{"location":"knowledge-base/general/databases/sequilize/#very-important-connectionsync-will-never-update-a-table-structure-if-its-already-created","text":"If you want to update the table structure ( like adding or removing columns) you can do one of the following: you have to delete the table first. (table data will be lost ) create new model with different name, in order to not lost the data. apply Migrations. apply force as following: NOT RECOMMENDED For Production js connection.sync({ force: true }); // all data in the table will be deleted","title":"VERY IMPORTANT: connection.sync() will never update a table structure if it's already created."},{"location":"knowledge-base/general/databases/sequilize/#important-allownull","text":"allowNull is true by default if you DID NOT specify allowNull: false , any validation to this field will not be applied (as if you have NO VALIDATION at all).","title":"IMPORTANT: allowNull"},{"location":"knowledge-base/general/databases/sequilize/#7-crud","text":"```js connection.sync(options).then(() => { const createOptions = { // options for create function fields:[\"field1\", \"field2\"], // white listed fields, fields that allowed to be submitted by user request } MyModel.create({ /* my doc data */ }, createOptions); // creating a doc then save to the database var doc1 = MyModel.build({ /* my doc data */ }); // create doc, Don't save. interact with the doc sync without waiting db saving doc1.save(); // save the doc created by build method const bulkCreateOptions = { // options for bulk create fields:[\"field1\",\"field2\"], validate: true, // bydefault createBulk skip validation, you have to specify the validate to true to run them ignoreDuplicate: true, // skip duplication error if one the documents already exists, continue creating other documents } MyModel.bulkCreate([doc1, doc2], bulkCreateOptions); // create multiple documents at once }) ```","title":"7- CRUD"},{"location":"knowledge-base/general/databases/sequilize/#8-realationships-between-documents","text":"","title":"8- Realationships between documents"},{"location":"knowledge-base/general/design/","text":"","title":"Index"},{"location":"knowledge-base/general/design/css/","text":"CSS \u00b6 Notes \u00b6 1 line layouts article: https://1linelayouts.glitch.me/ No select \u00b6 prevent users from selecting text => give the elemnt class of \"noselect\" : . noselect { -webkit- touch-callout : none ; /* iOS Safari */ -webkit- user-select : none ; /* Safari */ -khtml- user-select : none ; /* Konqueror HTML */ -moz- user-select : none ; /* Old versions of Firefox */ -ms- user-select : none ; /* Internet Explorer/Edge */ user-select : none ; /* Non-prefixed version, currently supported by Chrome, Opera and Firefox */ } one Third \u00b6 make all elements \\(divs\\) third of the screen with same height: # wrapper { width : 100 % ; display : flex ; flex-wrap : wrap ; } . third { width : 33 % ; } Flexbox \u00b6 flexbox: boxes = inner divs, parrent = parrent div. add display:flex to the parent => boxes will be same height \\( EQUAL HEIGHT COLUMNS \\) . add display:flex to parent + add flex:1 to each box => boxes will take same width and height \\( EQUAL HEIGHT + WIDTH COLUMNS \\) . add display:flex; justify-content: space-between; to parent + add width:32%; margin:1%; to each box => EQUAL HEIGHT + WIDTH COLUMNS WITH MARGINS add flex-wrap: wrap to the parent, with all previous code => EQUAL HEIGHT COLUMNS WITH MARGINS IN MULTIPLE ROWS for all text thing of those css rules as mandatory : . form-header { font-weight : 300 ; font-size : 60 px ; line-height : 82 px ; letter-spacing : -0.5 px ; } super centered \u00b6 center everything with no problems, add this class to the parent element. . parent { display : grid ; place-items : center ; } The Deconstructed Pancake \u00b6 Elements beside each other on desktop, stacked on phone without meia queries flex: 0 1 <baseWidth> ; . parent { display : flex ; flex-wrap : wrap ; justify-content : center ; } . element { flex : 1 1 150 px ; /* Stretching: */ flex : 0 1 150 px ; /* No stretching: */ /* choose only one of this 2 rules */ } sidebar width \u00b6 determain min and max width of sidebar. will not go down the min. the max is percentage, so it will change with the screen width . parent { display : grid ; grid-template-columns : minmax ( 150 px , 25 % ) 1 fr ; /* minmax(min sidebar, percentage of width when we are over min width) prerecentage of width for the main view */ }","title":"Css"},{"location":"knowledge-base/general/design/css/#css","text":"","title":"CSS"},{"location":"knowledge-base/general/design/css/#notes","text":"1 line layouts article: https://1linelayouts.glitch.me/","title":"Notes"},{"location":"knowledge-base/general/design/css/#no-select","text":"prevent users from selecting text => give the elemnt class of \"noselect\" : . noselect { -webkit- touch-callout : none ; /* iOS Safari */ -webkit- user-select : none ; /* Safari */ -khtml- user-select : none ; /* Konqueror HTML */ -moz- user-select : none ; /* Old versions of Firefox */ -ms- user-select : none ; /* Internet Explorer/Edge */ user-select : none ; /* Non-prefixed version, currently supported by Chrome, Opera and Firefox */ }","title":"No select"},{"location":"knowledge-base/general/design/css/#one-third","text":"make all elements \\(divs\\) third of the screen with same height: # wrapper { width : 100 % ; display : flex ; flex-wrap : wrap ; } . third { width : 33 % ; }","title":"one Third"},{"location":"knowledge-base/general/design/css/#flexbox","text":"flexbox: boxes = inner divs, parrent = parrent div. add display:flex to the parent => boxes will be same height \\( EQUAL HEIGHT COLUMNS \\) . add display:flex to parent + add flex:1 to each box => boxes will take same width and height \\( EQUAL HEIGHT + WIDTH COLUMNS \\) . add display:flex; justify-content: space-between; to parent + add width:32%; margin:1%; to each box => EQUAL HEIGHT + WIDTH COLUMNS WITH MARGINS add flex-wrap: wrap to the parent, with all previous code => EQUAL HEIGHT COLUMNS WITH MARGINS IN MULTIPLE ROWS for all text thing of those css rules as mandatory : . form-header { font-weight : 300 ; font-size : 60 px ; line-height : 82 px ; letter-spacing : -0.5 px ; }","title":"Flexbox"},{"location":"knowledge-base/general/design/css/#super-centered","text":"center everything with no problems, add this class to the parent element. . parent { display : grid ; place-items : center ; }","title":"super centered"},{"location":"knowledge-base/general/design/css/#the-deconstructed-pancake","text":"Elements beside each other on desktop, stacked on phone without meia queries flex: 0 1 <baseWidth> ; . parent { display : flex ; flex-wrap : wrap ; justify-content : center ; } . element { flex : 1 1 150 px ; /* Stretching: */ flex : 0 1 150 px ; /* No stretching: */ /* choose only one of this 2 rules */ }","title":"The Deconstructed Pancake"},{"location":"knowledge-base/general/design/css/#sidebar-width","text":"determain min and max width of sidebar. will not go down the min. the max is percentage, so it will change with the screen width . parent { display : grid ; grid-template-columns : minmax ( 150 px , 25 % ) 1 fr ; /* minmax(min sidebar, percentage of width when we are over min width) prerecentage of width for the main view */ }","title":"sidebar width"},{"location":"knowledge-base/general/frameworks/","text":"","title":"Index"},{"location":"knowledge-base/general/frameworks/bootstrap/","text":"Bootstrap \u00b6 bootsrap : <i class=\"material-icons\">Ahmad Ali</i> div .col-12 col-sm-6 : 12 on xs, 6 on sm and above. div .col : 12 on all. div .col-sm-4 , div .col-sm , div .col-sm-3 : from sm and above, first div 4, last div 3, middle div will take the rest. item .d-sm-none d-md-block : hide from sm and above, until find another d will show from md and above. class order : EX: .sm-order-first and .sm-order-last . center vertically: .align-items-center or align-self-center center Horizontally : text-center or justify-content-center navbar: nav: .navbar navbar-dark navbar-expand-sm bg-primary : dark blue nav, stacked on sm. nav > ul : .navbar-nav mr-auto : left side menu. nav > ul > li : .nav-item : nav > ul > li: .active : highlight current page link. nav > ul > li > a : .nav-link navbar menu button: <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#menu-id\" > menu button > span : .navbar-toggler-icon surround ul menu with div: <div class=\"collapse navbar-collapse\" id=\"#menu-id\"> push elemnt as much as you cant to the left \\(margin right\\) : .mr-auto . push elemnt as much as you cant to the right \\(margin left\\) : .ml-auto . bootsrap breadcrumbs: <ol class=\"col-12 breadcrumb\"> <li class=\"breadcrumb-item\"><a href=\"index.html\">Home</a></li> <li class=\"breadcrumb-item active\">About</li> </ol> .table-responsive : should added in a wrapper div, not applied to the table directly. addtional html tags: dl dt dd blockqoute embed object bootsrap images: .umg-fluid .rounded .rounded-corners .rounded-circle Bootstrap javascript: tabs and pills navigation. accordion navigation tooltips popover modals tooltips code: data-toggle=\"tooltip\" data-html=\"true\" title=\"string\" data-placement=\"bottom\"","title":"Bootstrap"},{"location":"knowledge-base/general/frameworks/bootstrap/#bootstrap","text":"bootsrap : <i class=\"material-icons\">Ahmad Ali</i> div .col-12 col-sm-6 : 12 on xs, 6 on sm and above. div .col : 12 on all. div .col-sm-4 , div .col-sm , div .col-sm-3 : from sm and above, first div 4, last div 3, middle div will take the rest. item .d-sm-none d-md-block : hide from sm and above, until find another d will show from md and above. class order : EX: .sm-order-first and .sm-order-last . center vertically: .align-items-center or align-self-center center Horizontally : text-center or justify-content-center navbar: nav: .navbar navbar-dark navbar-expand-sm bg-primary : dark blue nav, stacked on sm. nav > ul : .navbar-nav mr-auto : left side menu. nav > ul > li : .nav-item : nav > ul > li: .active : highlight current page link. nav > ul > li > a : .nav-link navbar menu button: <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#menu-id\" > menu button > span : .navbar-toggler-icon surround ul menu with div: <div class=\"collapse navbar-collapse\" id=\"#menu-id\"> push elemnt as much as you cant to the left \\(margin right\\) : .mr-auto . push elemnt as much as you cant to the right \\(margin left\\) : .ml-auto . bootsrap breadcrumbs: <ol class=\"col-12 breadcrumb\"> <li class=\"breadcrumb-item\"><a href=\"index.html\">Home</a></li> <li class=\"breadcrumb-item active\">About</li> </ol> .table-responsive : should added in a wrapper div, not applied to the table directly. addtional html tags: dl dt dd blockqoute embed object bootsrap images: .umg-fluid .rounded .rounded-corners .rounded-circle Bootstrap javascript: tabs and pills navigation. accordion navigation tooltips popover modals tooltips code: data-toggle=\"tooltip\" data-html=\"true\" title=\"string\" data-placement=\"bottom\"","title":"Bootstrap"},{"location":"knowledge-base/general/frameworks/react/","text":"React \u00b6 Intro \u00b6 JSX is a syntax extension to JavaScript that allows React Elements to be written inside JavaScript using HTML tags. var element = < h1 > Hello World !< /h1> / / jsx var element = React . createElement ( // without jsx 'h1' , null , 'Hello World!' ) // render ReactDOM . render ( element , document . getElementById ( \"root\" ) ) A React Component is an independent reusable component that outputs a React Element based on its properties and state. General Notes \u00b6 when you design your routes, make sure that front end routes differe from back-end routes. especially when using react-node app. to apply a css file to one component only: wrap you component into <div className=\"Component-Name\"> . use that class before the css rules you want to apply for this component only as: .component-name p{ //css rules } handle 404 route: add this code as the last Route in the switch statement: < Route path = \"*\" component = { NotFound } /> ; { /* OR */ } < Route component = { NotFound } /> ; component that will simplify your routes: \u00b6 usage <SimpleRoute path={path} component={component_to_render} / > import React from 'react' import { Route } from 'react-router-dom' const SimpleRoute ({ component : Component , ... rest }) => ( < Route {... rest } render = { props => { return ( < div > < Component {... props } /> < /div> ) }} /> ) export default SimpleRoute component will recieve props \u00b6 check if we are still on the same component OR ifwe are coming from another component: componentWillReceiveProps = newProps => { if ( newProps . location !== this . props . location ) { console . log ( 'here should we gos' ) } /* * typical props.location at the first mount of the copmonent, where the props.location == newprops.location */ this . props . window = { pathname : \"/page\" , search : \"\" , hash : \"\" , state : undefined , key : \"yooxfy\" } /* * if the component re-rendered in the same page, the { this.props.window.key } will change, * so they are no longer equal. */ passing functions to the events should be by reference , but if we call the function inside the event listener it will be called imediatley and not waiting until the event is bieng fired. < Component onClick = { myFunction } /> // will wait untill you click to fire the function < Component onClick = { myFunction ()} /> // the function will be fired immediatley. < Component onClick = {( e , someVar ) => myFunction ( someVar )} /> // this way we can pass parameters to that handlers passing functions as props should be by reference. Hide dom element by click outside of it \u00b6 componentWillMount () { document . addEventListener ( \"mousedown\" , this . handleClickOutside , false ); } componentWillUnmount () { document . removeEventListener ( \"mousedown\" , this . handleClickOutside , false ); } handleClickOutside = e => { if ( this . node . contains ( e . target )) { return ; } if ( this . state . divshow ) { this . setState ({ divshow : false }); } }; render () { return this . state . divshow ? ( < div ref = { node => ( this . node = node )} style = {{ backgroundColor : \"red\" , height : \"200px\" }} /> ) : ( \"Hided\" ); } hook that responds to changes In screen dementions \u00b6 ```js import React, { useLayoutEffect, useState } from 'react'; function useWindowSize() { const [size, setSize] = useState([0, 0]); useLayoutEffect(() => { function updateSize() { setSize([window.innerWidth, window.innerHeight]); } window.addEventListener('resize', updateSize); updateSize(); return () => window.removeEventListener('resize', updateSize); }, []); return size; } export default useWindowSize; ```","title":"React"},{"location":"knowledge-base/general/frameworks/react/#react","text":"","title":"React"},{"location":"knowledge-base/general/frameworks/react/#intro","text":"JSX is a syntax extension to JavaScript that allows React Elements to be written inside JavaScript using HTML tags. var element = < h1 > Hello World !< /h1> / / jsx var element = React . createElement ( // without jsx 'h1' , null , 'Hello World!' ) // render ReactDOM . render ( element , document . getElementById ( \"root\" ) ) A React Component is an independent reusable component that outputs a React Element based on its properties and state.","title":"Intro"},{"location":"knowledge-base/general/frameworks/react/#general-notes","text":"when you design your routes, make sure that front end routes differe from back-end routes. especially when using react-node app. to apply a css file to one component only: wrap you component into <div className=\"Component-Name\"> . use that class before the css rules you want to apply for this component only as: .component-name p{ //css rules } handle 404 route: add this code as the last Route in the switch statement: < Route path = \"*\" component = { NotFound } /> ; { /* OR */ } < Route component = { NotFound } /> ;","title":"General Notes"},{"location":"knowledge-base/general/frameworks/react/#component-that-will-simplify-your-routes","text":"usage <SimpleRoute path={path} component={component_to_render} / > import React from 'react' import { Route } from 'react-router-dom' const SimpleRoute ({ component : Component , ... rest }) => ( < Route {... rest } render = { props => { return ( < div > < Component {... props } /> < /div> ) }} /> ) export default SimpleRoute","title":"component that will simplify your routes:"},{"location":"knowledge-base/general/frameworks/react/#component-will-recieve-props","text":"check if we are still on the same component OR ifwe are coming from another component: componentWillReceiveProps = newProps => { if ( newProps . location !== this . props . location ) { console . log ( 'here should we gos' ) } /* * typical props.location at the first mount of the copmonent, where the props.location == newprops.location */ this . props . window = { pathname : \"/page\" , search : \"\" , hash : \"\" , state : undefined , key : \"yooxfy\" } /* * if the component re-rendered in the same page, the { this.props.window.key } will change, * so they are no longer equal. */ passing functions to the events should be by reference , but if we call the function inside the event listener it will be called imediatley and not waiting until the event is bieng fired. < Component onClick = { myFunction } /> // will wait untill you click to fire the function < Component onClick = { myFunction ()} /> // the function will be fired immediatley. < Component onClick = {( e , someVar ) => myFunction ( someVar )} /> // this way we can pass parameters to that handlers passing functions as props should be by reference.","title":"component will recieve props"},{"location":"knowledge-base/general/frameworks/react/#hide-dom-element-by-click-outside-of-it","text":"componentWillMount () { document . addEventListener ( \"mousedown\" , this . handleClickOutside , false ); } componentWillUnmount () { document . removeEventListener ( \"mousedown\" , this . handleClickOutside , false ); } handleClickOutside = e => { if ( this . node . contains ( e . target )) { return ; } if ( this . state . divshow ) { this . setState ({ divshow : false }); } }; render () { return this . state . divshow ? ( < div ref = { node => ( this . node = node )} style = {{ backgroundColor : \"red\" , height : \"200px\" }} /> ) : ( \"Hided\" ); }","title":"Hide dom element by click outside of it"},{"location":"knowledge-base/general/frameworks/react/#hook-that-responds-to-changes-in-screen-dementions","text":"```js import React, { useLayoutEffect, useState } from 'react'; function useWindowSize() { const [size, setSize] = useState([0, 0]); useLayoutEffect(() => { function updateSize() { setSize([window.innerWidth, window.innerHeight]); } window.addEventListener('resize', updateSize); updateSize(); return () => window.removeEventListener('resize', updateSize); }, []); return size; } export default useWindowSize; ```","title":"hook that responds to changes In screen dementions"},{"location":"knowledge-base/general/frameworks/redux/","text":"Redux \u00b6 redux articuture \u00b6 store all of our application state into one javascript object called store. the store is the single source of truth, and it is accessible by all parts of the ui. redux follows the functional programming principles. the store is immutable, so we need a function to mutate it, this function is the reducer . the reducer copies the store and returns a new updated version of this store. the reducer return the new updated store after applying an action to it. reducers are pure functions, they don't change any state or do any side effects. function reducer ( store , action ) { const updatedStore = { ... store } // copy the store updatedStore . something = dispatch ( action , store . something ); // update the copied store return updatedStore ; // return the updated copied store } start with redux \u00b6 design the store. define the actions. create the reducer. set up the store. 1. design the store \u00b6 store is smiply an immutable object, that contains all of our state. const store = { state1 : [], state2 : {}, state3 : false , ... } 2. define the actions \u00b6 action is an object with at least one single property called type . payload is an objct contains all the data associated with one action. const action = { type : \"ACTION_TYPE\" , payload : { id : 44 , description : \" ... \" , }, }, 3. create the reducer \u00b6 reducer is a function that takes a state and apply an action to it. const id = 0 ; function reducer ( state = store . state1 , action ) { if ( action . type === \"ACTION_TYPE1\" ) { return [ ... state , { id : ++ id , description : action . payload . description , .... } ]; } else if ( action . type === \"ACTION_TYPE2\" ) { return ...; } return state ; // default } 4. set up the store \u00b6 lastly connecting everything together. import { createStore } from \"redux\" ; import reducer from \"./reducer\" ; const reducers = combineReducers ([ reducer ]); const store = createStore ( reducers ); export default store ;","title":"Redux"},{"location":"knowledge-base/general/frameworks/redux/#redux","text":"","title":"Redux"},{"location":"knowledge-base/general/frameworks/redux/#redux-articuture","text":"store all of our application state into one javascript object called store. the store is the single source of truth, and it is accessible by all parts of the ui. redux follows the functional programming principles. the store is immutable, so we need a function to mutate it, this function is the reducer . the reducer copies the store and returns a new updated version of this store. the reducer return the new updated store after applying an action to it. reducers are pure functions, they don't change any state or do any side effects. function reducer ( store , action ) { const updatedStore = { ... store } // copy the store updatedStore . something = dispatch ( action , store . something ); // update the copied store return updatedStore ; // return the updated copied store }","title":"redux articuture"},{"location":"knowledge-base/general/frameworks/redux/#start-with-redux","text":"design the store. define the actions. create the reducer. set up the store.","title":"start with redux"},{"location":"knowledge-base/general/frameworks/redux/#1-design-the-store","text":"store is smiply an immutable object, that contains all of our state. const store = { state1 : [], state2 : {}, state3 : false , ... }","title":"1. design the store"},{"location":"knowledge-base/general/frameworks/redux/#2-define-the-actions","text":"action is an object with at least one single property called type . payload is an objct contains all the data associated with one action. const action = { type : \"ACTION_TYPE\" , payload : { id : 44 , description : \" ... \" , }, },","title":"2. define the actions"},{"location":"knowledge-base/general/frameworks/redux/#3-create-the-reducer","text":"reducer is a function that takes a state and apply an action to it. const id = 0 ; function reducer ( state = store . state1 , action ) { if ( action . type === \"ACTION_TYPE1\" ) { return [ ... state , { id : ++ id , description : action . payload . description , .... } ]; } else if ( action . type === \"ACTION_TYPE2\" ) { return ...; } return state ; // default }","title":"3. create the reducer"},{"location":"knowledge-base/general/frameworks/redux/#4-set-up-the-store","text":"lastly connecting everything together. import { createStore } from \"redux\" ; import reducer from \"./reducer\" ; const reducers = combineReducers ([ reducer ]); const store = createStore ( reducers ); export default store ;","title":"4. set up the store"},{"location":"knowledge-base/general/interviews/","text":"","title":"Index"},{"location":"knowledge-base/general/interviews/js_general/","text":"General Javascript Interview questions \u00b6 Index \u00b6 why do you use es6?, what\u2019re your favourite features of es6 functions declarations vs functions expressions 1. why do you use es6?, what\u2019re your favourite features of es6 \u00b6 JavaScript is a lightweight, interpreted, object-oriented programming language with first-class functions most commonly known as a scripting language for web pages. scripting language, which means that its code is interpreted instead of compiled. ES6 has Object-Oriented Classes. ES6 has Arrow Functions arrow functions don't have this so you don't need to rebind. Modules are built into ES6. ES6 has template literal strings. ES6 uses Yarn ES6 let, const over var. ES6 has spread operators. ES6 has Promises ES6 for..in, for..of 2. functions declarations vs functions expressions \u00b6 function x ( args ) { /*code */ } //declaration const x = function ( args ) { /*code */ } //expression function expressions are not hoisted , so you can't use it before declaration. 3. Class declarations vs class expressions \u00b6 class myClass { constructor () { } // class declaration } var myVar = class myClass { constructor () { } // class expression } you can't use class (instanciate) a class before its declaration. 4. what's hoisting? \u00b6 hoisting: mechanism only moves the declaration. The assignments are left in place. if the variable is not declared: console . log ( type of myVar ); // undefined console . log ( myVar ); // ReferenceError all variable and function declarations are hoisted to the top of their scope, efore any code execution. undeclared variables do not exist until code assigning them is executed. Therefore, assigning a value to an undeclared variable implicitly creates it as a global variable when the assignment is executed. This means that, all undeclared variables are global variables. function hoist () { a = 20 ; var b = 100 ; } hoist (); console . log ( a ); // 20 // Accessible as a global variable outside hoist() function console . log ( b ); // ReferenceError /* Since it was declared, it is confined to the hoist() function scope. We can't print it out outside the confines of the hoist() function. Output: ReferenceError: b is not defined */ always declare variables regardless of whether they are in a function or global scope. if the variable declared down the app, call its value on top will not give referenceError because its declaration has being hoisted before the execution , while its value still undefined untill you reach the assigment expression . then it will gets a value. console . log ( hoist ); // Output: undefined var hoist = 'hoist' ; console . log ( hoist ); // Output: hoist whten using strict mode, no hoisting, using variables before their declaration will throw an error: 'use strict' ; console . log ( hoist ); // Output: ReferenceError: hoist is not defined hoist = 'Hoisted' ; when using let or const keyword, the variable is hoisted to the top of the block, using variables before their declaration will throw an error, as if it is a var in strict mode : console . log ( hoist ); // Output: ReferenceError: hoist is not defined let hoist = 'Hoisted' ; constant variable must be both declared and initialised before use. function expressions are not hoisted , so you can't use it before declaration. while function declaration are hoisted . There's a bit of an argument to be made as to whether Javascript es6 let, const variables and classes are actually hoisted, roughly hoisted or not hoisted. Some argue that they are actually hoisted but uninitialised whilst some argue that they are not hoisted at all. 5. class hoisting \u00b6 class declarations are hoisted. However, they remain uninitialised until evaluation. class expressions are not hoisted. in both cases, you can't use a class before its declaration. var x = new myClass (); // ReferenceError: myClass is not defined // class is hoisted, but uninitialized. class myClass { constructor () { } // class declaration } var y = new myNewClass (); // TypeError: myNewClass is not a constructor // class is not hoisted. var myVar = class myNewClass { constructor () { } // class expression } 6. Order of precedence \u00b6 Variable assignment over function declaration var double = 22 ; function double ( num ) { return ( num * 2 ); } console . log ( typeof double ); // Output: number Function declarations over variable declarations var double ; function double ( num ) { return ( num * 2 ); } console . log ( typeof double ); // Output: function Even if we reversed the position of the declarations, the JavaScript interpreter would still consider the Order of precedence . 7. shallow copying vs deep copying? \u00b6 A copy just looks like the old thing, but isn\u2019t. When you change the copy, you expect the original thing to stay the same, whereas the copy changes. deep copy means that all of the values of the new variable are copied and disconnected from the original variable. A shallow copy means that certain (sub-)values are still connected to the original variable. let x = [ 1 , 2 , 3 ]; let y = { x : 1 , z : 2 } let n = x ; // shallow copy, changing n values will change x values let m = y // shallow copy let c = []; for ( i in x ) { c [ i ] = x [ i ] } // deep copy, c now is copmletely disconnected from x. let d = {}; for ( j in y ){ d [ j ] = y [ j ] } // deep copy // Also, deep copies // Arrays let a = [... x ]; let a = Array . from ( x ); let a = x . map ( el => el ); let a = x . slice (); // Objects let b = { ... y }; let b = Object . assign ({}, y ); if the object or the array yhat you'r copying contains another no-permative types like if they contains nested objects or arrays, those elements will get a shallow copy even if you do a deep copy to the main object let x = { a : 1 , b : [ 1 , 2 ], c : { x : 1 , y : 2 } }; let y = { ... x } // shallow copy for x.a and x.b let y = { ... x . b , ... x . c , ... x } // deep copy if you don't know how deep your elements are, you can stringify the element first , then parse it to get deep copy. let x = { a : 1 , b : [ 1 , 2 ], c : { x : 1 , y : 2 } }; let y = JSON . parse ( JSON . stringify ( x )) // deep copy 8. async vs sync? \u00b6 5. when do you use promises vs Callbacks? \u00b6 6. pros and cons of js? \u00b6 7. why \"use strict\"? \u00b6 when using strict mode we opt into a restricted variant of JavaScript that will not tolerate the usage of variables before they are declared. Running our code in strict mode: Eliminates some silent JavaScript errors by changing them to explicit throw errors which will be spit out by the interpreter. Fixes mistakes that make it difficult for JavaScript engines to perform optimisations. Prohibits some syntax likely to be defined in future versions of JavaScript. 8. global scope vs block scope? \u00b6 9. what's fe or functional expression?? \u00b6 10. coding style; singilton pattern or factory patterns or revealing module pattern? \u00b6 11. how do you right readable code? \u00b6 12. const vs let? \u00b6 13. var vs let? \u00b6 14. document. cookie?? \u00b6 15. null vs undefined? \u00b6 16. attributes vs properties?? \u00b6 17. session vs loacalstorage?, \u00b6 18. Window vs document? \u00b6 19. event bubbling?? \u00b6 20. nan?? \u00b6 21. primitive types passed to a function by value, others passed by reference. \u00b6 22. by value: creating a copy of the original. \u00b6 23. parseInt? \u00b6 24. Prompt ? \u00b6 25. func.call() vs func.apply() \u00b6 Call: pass the args individually as ..args Apply: pass the args as an array [args] 26. Empty an array \u00b6 arr = []; assigns a reference to a new array to a variable, while any other references are unaffected. which means that references to the contents of the previous array are still kept in memory, leading to memory leaks. arr.length = 0; deletes everything in the array, which does hit other references. arr.splice(0, arr.length); while(arr.length){ arr.pop(); } 27. Delete x \u00b6 delete opereator: deletes an entety from an object. will not affect any other types of vars. 28. Truthy and falsy values \u00b6 29. delete an entity of an object will set it to undefined. \u00b6 30. Object.create(obj) \u00b6 31. const func = function(){ //code } \u00b6 reference error 32. First-class Functions \u00b6 JavaScript treats functions as first-class citizens, meaning you can pass functions as parameters to other functions just like you would any other variable. 33. JavaScript Is Prototype-based \u00b6 it supports classes and has some inheretance features similar to other kangs like c++ .. Prototype-based programming is a style of object-oriented programming in which behavior reuse (known as inheritance) is performed via a process of reusing existing objects via delegations that serve as prototypes. 34. JavaScript Event Loops \u00b6 event listener listening to an event, when the event is fired it goes to queue of events. the queue gets executed FIFO and sync. every element of the queue gets fully executed with its function before strating executing the second elemnt. if a function contains other function calls, they are all performed prior to processing a new message from the queue. This is called run-to-completion. JavaScript is non-blocking, meaning that when an asynchronous operation is being performed, the program is able to process other things, such as receiving user input, while waiting for the asynchronous operation to complete, not blocking the main execution thread. Design patterns \u00b6 35. Constructor Pattern \u00b6 you define a constructor, then you get instances of this constructor. changing some of constructor properties will not change the already created instances, it will only change the instances that will be created after the change to the constructor. 36. Prototype pattern \u00b6 define a cornstructor with some prototype properties. instantiatipon of this constructor will lead to create a new instance, but the prototypes properties still holded in the constructor. changes to the constructor will be copied to all instnces of this constructor. 37. arrow funcs vs normal funcs \u00b6 ```js class myclass { y(){ this; // refers to the function y, not the class. } that = this; x(){ this; // refers to the function x that ;// refers to the class } z = () => { this; // refers to fnction z } } // OR var n = { x: function(){ console.log(this) }, // n that: this, // window y: function(){ console.log(this, this.that) }, //n , window z: () => console.log(this) // window } ``` references \u00b6 design patterens hoisting","title":"Js general"},{"location":"knowledge-base/general/interviews/js_general/#general-javascript-interview-questions","text":"","title":"General Javascript Interview questions"},{"location":"knowledge-base/general/interviews/js_general/#index","text":"why do you use es6?, what\u2019re your favourite features of es6 functions declarations vs functions expressions","title":"Index"},{"location":"knowledge-base/general/interviews/js_general/#1-why-do-you-use-es6-whatre-your-favourite-features-of-es6","text":"JavaScript is a lightweight, interpreted, object-oriented programming language with first-class functions most commonly known as a scripting language for web pages. scripting language, which means that its code is interpreted instead of compiled. ES6 has Object-Oriented Classes. ES6 has Arrow Functions arrow functions don't have this so you don't need to rebind. Modules are built into ES6. ES6 has template literal strings. ES6 uses Yarn ES6 let, const over var. ES6 has spread operators. ES6 has Promises ES6 for..in, for..of","title":"1. why do you use es6?, what\u2019re your favourite features of es6"},{"location":"knowledge-base/general/interviews/js_general/#2-functions-declarations-vs-functions-expressions","text":"function x ( args ) { /*code */ } //declaration const x = function ( args ) { /*code */ } //expression function expressions are not hoisted , so you can't use it before declaration.","title":"2. functions declarations vs functions expressions"},{"location":"knowledge-base/general/interviews/js_general/#3-class-declarations-vs-class-expressions","text":"class myClass { constructor () { } // class declaration } var myVar = class myClass { constructor () { } // class expression } you can't use class (instanciate) a class before its declaration.","title":"3. Class declarations vs class expressions"},{"location":"knowledge-base/general/interviews/js_general/#4-whats-hoisting","text":"hoisting: mechanism only moves the declaration. The assignments are left in place. if the variable is not declared: console . log ( type of myVar ); // undefined console . log ( myVar ); // ReferenceError all variable and function declarations are hoisted to the top of their scope, efore any code execution. undeclared variables do not exist until code assigning them is executed. Therefore, assigning a value to an undeclared variable implicitly creates it as a global variable when the assignment is executed. This means that, all undeclared variables are global variables. function hoist () { a = 20 ; var b = 100 ; } hoist (); console . log ( a ); // 20 // Accessible as a global variable outside hoist() function console . log ( b ); // ReferenceError /* Since it was declared, it is confined to the hoist() function scope. We can't print it out outside the confines of the hoist() function. Output: ReferenceError: b is not defined */ always declare variables regardless of whether they are in a function or global scope. if the variable declared down the app, call its value on top will not give referenceError because its declaration has being hoisted before the execution , while its value still undefined untill you reach the assigment expression . then it will gets a value. console . log ( hoist ); // Output: undefined var hoist = 'hoist' ; console . log ( hoist ); // Output: hoist whten using strict mode, no hoisting, using variables before their declaration will throw an error: 'use strict' ; console . log ( hoist ); // Output: ReferenceError: hoist is not defined hoist = 'Hoisted' ; when using let or const keyword, the variable is hoisted to the top of the block, using variables before their declaration will throw an error, as if it is a var in strict mode : console . log ( hoist ); // Output: ReferenceError: hoist is not defined let hoist = 'Hoisted' ; constant variable must be both declared and initialised before use. function expressions are not hoisted , so you can't use it before declaration. while function declaration are hoisted . There's a bit of an argument to be made as to whether Javascript es6 let, const variables and classes are actually hoisted, roughly hoisted or not hoisted. Some argue that they are actually hoisted but uninitialised whilst some argue that they are not hoisted at all.","title":"4. what's hoisting?"},{"location":"knowledge-base/general/interviews/js_general/#5-class-hoisting","text":"class declarations are hoisted. However, they remain uninitialised until evaluation. class expressions are not hoisted. in both cases, you can't use a class before its declaration. var x = new myClass (); // ReferenceError: myClass is not defined // class is hoisted, but uninitialized. class myClass { constructor () { } // class declaration } var y = new myNewClass (); // TypeError: myNewClass is not a constructor // class is not hoisted. var myVar = class myNewClass { constructor () { } // class expression }","title":"5. class hoisting"},{"location":"knowledge-base/general/interviews/js_general/#6-order-of-precedence","text":"Variable assignment over function declaration var double = 22 ; function double ( num ) { return ( num * 2 ); } console . log ( typeof double ); // Output: number Function declarations over variable declarations var double ; function double ( num ) { return ( num * 2 ); } console . log ( typeof double ); // Output: function Even if we reversed the position of the declarations, the JavaScript interpreter would still consider the Order of precedence .","title":"6. Order of precedence"},{"location":"knowledge-base/general/interviews/js_general/#7-shallow-copying-vs-deep-copying","text":"A copy just looks like the old thing, but isn\u2019t. When you change the copy, you expect the original thing to stay the same, whereas the copy changes. deep copy means that all of the values of the new variable are copied and disconnected from the original variable. A shallow copy means that certain (sub-)values are still connected to the original variable. let x = [ 1 , 2 , 3 ]; let y = { x : 1 , z : 2 } let n = x ; // shallow copy, changing n values will change x values let m = y // shallow copy let c = []; for ( i in x ) { c [ i ] = x [ i ] } // deep copy, c now is copmletely disconnected from x. let d = {}; for ( j in y ){ d [ j ] = y [ j ] } // deep copy // Also, deep copies // Arrays let a = [... x ]; let a = Array . from ( x ); let a = x . map ( el => el ); let a = x . slice (); // Objects let b = { ... y }; let b = Object . assign ({}, y ); if the object or the array yhat you'r copying contains another no-permative types like if they contains nested objects or arrays, those elements will get a shallow copy even if you do a deep copy to the main object let x = { a : 1 , b : [ 1 , 2 ], c : { x : 1 , y : 2 } }; let y = { ... x } // shallow copy for x.a and x.b let y = { ... x . b , ... x . c , ... x } // deep copy if you don't know how deep your elements are, you can stringify the element first , then parse it to get deep copy. let x = { a : 1 , b : [ 1 , 2 ], c : { x : 1 , y : 2 } }; let y = JSON . parse ( JSON . stringify ( x )) // deep copy","title":"7. shallow copying vs deep copying?"},{"location":"knowledge-base/general/interviews/js_general/#8-async-vs-sync","text":"","title":"8. async vs sync?"},{"location":"knowledge-base/general/interviews/js_general/#5-when-do-you-use-promises-vs-callbacks","text":"","title":"5. when do you use promises vs Callbacks?"},{"location":"knowledge-base/general/interviews/js_general/#6-pros-and-cons-of-js","text":"","title":"6. pros and cons of js?"},{"location":"knowledge-base/general/interviews/js_general/#7-why-use-strict","text":"when using strict mode we opt into a restricted variant of JavaScript that will not tolerate the usage of variables before they are declared. Running our code in strict mode: Eliminates some silent JavaScript errors by changing them to explicit throw errors which will be spit out by the interpreter. Fixes mistakes that make it difficult for JavaScript engines to perform optimisations. Prohibits some syntax likely to be defined in future versions of JavaScript.","title":"7. why \"use strict\"?"},{"location":"knowledge-base/general/interviews/js_general/#8-global-scope-vs-block-scope","text":"","title":"8. global scope vs block scope?"},{"location":"knowledge-base/general/interviews/js_general/#9-whats-fe-or-functional-expression","text":"","title":"9. what's fe or functional expression??"},{"location":"knowledge-base/general/interviews/js_general/#10-coding-style-singilton-pattern-or-factory-patterns-or-revealing-module-pattern","text":"","title":"10. coding style; singilton pattern or factory patterns or revealing module pattern?"},{"location":"knowledge-base/general/interviews/js_general/#11-how-do-you-right-readable-code","text":"","title":"11. how do you right readable code?"},{"location":"knowledge-base/general/interviews/js_general/#12-const-vs-let","text":"","title":"12. const vs let?"},{"location":"knowledge-base/general/interviews/js_general/#13-var-vs-let","text":"","title":"13. var vs let?"},{"location":"knowledge-base/general/interviews/js_general/#14-document-cookie","text":"","title":"14. document. cookie??"},{"location":"knowledge-base/general/interviews/js_general/#15-null-vs-undefined","text":"","title":"15. null vs undefined?"},{"location":"knowledge-base/general/interviews/js_general/#16-attributes-vs-properties","text":"","title":"16. attributes vs properties??"},{"location":"knowledge-base/general/interviews/js_general/#17-session-vs-loacalstorage","text":"","title":"17. session vs loacalstorage?,"},{"location":"knowledge-base/general/interviews/js_general/#18-window-vs-document","text":"","title":"18. Window vs document?"},{"location":"knowledge-base/general/interviews/js_general/#19-event-bubbling","text":"","title":"19. event bubbling??"},{"location":"knowledge-base/general/interviews/js_general/#20-nan","text":"","title":"20. nan??"},{"location":"knowledge-base/general/interviews/js_general/#21-primitive-types-passed-to-a-function-by-value-others-passed-by-reference","text":"","title":"21. primitive types passed to a function by value, others passed by reference."},{"location":"knowledge-base/general/interviews/js_general/#22-by-value-creating-a-copy-of-the-original","text":"","title":"22. by value: creating a copy of the original."},{"location":"knowledge-base/general/interviews/js_general/#23-parseint","text":"","title":"23. parseInt?"},{"location":"knowledge-base/general/interviews/js_general/#24-prompt","text":"","title":"24. Prompt ?"},{"location":"knowledge-base/general/interviews/js_general/#25-funccall-vs-funcapply","text":"Call: pass the args individually as ..args Apply: pass the args as an array [args]","title":"25. func.call() vs func.apply()"},{"location":"knowledge-base/general/interviews/js_general/#26-empty-an-array","text":"arr = []; assigns a reference to a new array to a variable, while any other references are unaffected. which means that references to the contents of the previous array are still kept in memory, leading to memory leaks. arr.length = 0; deletes everything in the array, which does hit other references. arr.splice(0, arr.length); while(arr.length){ arr.pop(); }","title":"26. Empty an array"},{"location":"knowledge-base/general/interviews/js_general/#27-delete-x","text":"delete opereator: deletes an entety from an object. will not affect any other types of vars.","title":"27. Delete x"},{"location":"knowledge-base/general/interviews/js_general/#28-truthy-and-falsy-values","text":"","title":"28. Truthy and falsy values"},{"location":"knowledge-base/general/interviews/js_general/#29-delete-an-entity-of-an-object-will-set-it-to-undefined","text":"","title":"29. delete an entity of an object will set it to undefined."},{"location":"knowledge-base/general/interviews/js_general/#30-objectcreateobj","text":"","title":"30. Object.create(obj)"},{"location":"knowledge-base/general/interviews/js_general/#31-const-func-function-code","text":"reference error","title":"31. const func = function(){ //code }"},{"location":"knowledge-base/general/interviews/js_general/#32-first-class-functions","text":"JavaScript treats functions as first-class citizens, meaning you can pass functions as parameters to other functions just like you would any other variable.","title":"32. First-class Functions"},{"location":"knowledge-base/general/interviews/js_general/#33-javascript-is-prototype-based","text":"it supports classes and has some inheretance features similar to other kangs like c++ .. Prototype-based programming is a style of object-oriented programming in which behavior reuse (known as inheritance) is performed via a process of reusing existing objects via delegations that serve as prototypes.","title":"33. JavaScript Is Prototype-based"},{"location":"knowledge-base/general/interviews/js_general/#34-javascript-event-loops","text":"event listener listening to an event, when the event is fired it goes to queue of events. the queue gets executed FIFO and sync. every element of the queue gets fully executed with its function before strating executing the second elemnt. if a function contains other function calls, they are all performed prior to processing a new message from the queue. This is called run-to-completion. JavaScript is non-blocking, meaning that when an asynchronous operation is being performed, the program is able to process other things, such as receiving user input, while waiting for the asynchronous operation to complete, not blocking the main execution thread.","title":"34. JavaScript Event Loops"},{"location":"knowledge-base/general/interviews/js_general/#design-patterns","text":"","title":"Design patterns"},{"location":"knowledge-base/general/interviews/js_general/#35-constructor-pattern","text":"you define a constructor, then you get instances of this constructor. changing some of constructor properties will not change the already created instances, it will only change the instances that will be created after the change to the constructor.","title":"35. Constructor Pattern"},{"location":"knowledge-base/general/interviews/js_general/#36-prototype-pattern","text":"define a cornstructor with some prototype properties. instantiatipon of this constructor will lead to create a new instance, but the prototypes properties still holded in the constructor. changes to the constructor will be copied to all instnces of this constructor.","title":"36. Prototype pattern"},{"location":"knowledge-base/general/interviews/js_general/#37-arrow-funcs-vs-normal-funcs","text":"```js class myclass { y(){ this; // refers to the function y, not the class. } that = this; x(){ this; // refers to the function x that ;// refers to the class } z = () => { this; // refers to fnction z } } // OR var n = { x: function(){ console.log(this) }, // n that: this, // window y: function(){ console.log(this, this.that) }, //n , window z: () => console.log(this) // window } ```","title":"37. arrow funcs vs normal funcs"},{"location":"knowledge-base/general/interviews/js_general/#references","text":"design patterens hoisting","title":"references"},{"location":"knowledge-base/general/interviews/js_oop/","text":"OOP in JS \u00b6 1. concepts of oop \u00b6 Abstraction Polymorphism Inheretance Encapsulation 2. procedural vs oop \u00b6 in procedural programming you write functions. change one function then you need to change everywhere spaggitti code functions have more parameters in oop: you write classes. functions have less params. change only the code in the class and it will changes every where simply. 3. Abstraction \u00b6 reduce the impact of change. hide un-necceary data. function Square ( a , b ){ this . x = a ; this . y = b ; this . area = function (){ console . log ( this . x * this . y ) }; let defaultLocation = { x : 1 , y : 1 } // private let defaultArea = function (){ // code, this method is private // we can call defaultLocation directly without this } } defaultLocation and defaultArea are private. you can't access them from the outer program. defaultLocation and defaultArea are local variables in the constructor function, so we can think of them as private but they are not . the square class interface does not contain defaultArea and defaultLocation and contain onlu x,y,area (the words with this). the square class is now abstracted, because we hide the default data so no body can miss around with them. 4. Inheretance \u00b6 eleminate redandant code. 5. polymorphism \u00b6 the object behaves differently depending on the class we are referincing. refactor ugly switch/case statments. 6. Encapsulation \u00b6 reduce complexity. increase reusability. 7. objects lterals \u00b6 const square = { x : 1 , y : 2 , area : function (){ console . log ( this . x * this . y ) } } object is collection of key value pairs. 8. Factory functions \u00b6 function createSquare ( a , b ){ return { x : a , y : b , area : function (){ console . log ( a * b ) } } } // so let q1 = createSquare ( 2 , 2 ); q1 . area () //4 9. Constructors \u00b6 the first letter should be uppercase. it's like creating an instance of class, but in js there is no classes. function Square ( a , b ){ this . x = a ; this . y = b ; this . area = function (){ console . log ( this . x * this . y ) }; } // so q2 = new Square ( 2 , 2 ); // if we remove new, // 1) this will refer to the global object. // 2) q2 will be undefined. q2 . area () //4 10. This \u00b6 by default this refers to the global object, eg. window . by using the new keyword before a constructor, this will refer to the new object. 11. every object has a constructor refers to the function that used to create this object. \u00b6 12. Default constructors \u00b6 let q3 = new Object () q3 . x = 2 ; q3 . y = 2 ; q3 . area = function (){ console . log ( this . x * this . y ) }; // OR let q4 = new Object ({ x : 2 , y : 2 , area : function (){ console . log ( this . x * this . y ) } }) // so q3 . area () //4 q4 . area () //4 other default constructors: new String() , new Boolean() , new Number() ... 13. Functions are objects \u00b6 const square = new Function ( 'a , b' , ` this.x = a; this.y = b; this.area = function (){ console.log(this.x * this.y) }; ` ) // So q5 = new square ( 2 , 2 ); q5 . area () //4 14. Value Types (pirmiatives) vs Reference Types \u00b6 value types (pirimatives): Number String Boolean Symbol undefined null Reference types: Object Array Map Set Function copying a pirimative type will copy its value to the new variable. copying a refernce type will not copy its value to the new var, instead the memory address for the first var is stored in the new var. so copying a reference type is actually pointing to tits memory address. // value let x = 10 ; let y = x ; x = 20 ; // x is 20, y still 10. // reference let x = { value : 10 }; let y = x ; x . value = 20 ; // y.value = x.value = 20. passing a var by its value to a function will not change its original value. passing a var by its reference will change its original value directly. // primiatves (by value) let x = 10 ; function increase ( num ) { num ++ ; } increase ( x ); console . log ( x ); // 10, copied by value, origin don't change. // Objects (by reference) let y = { value : 10 }; function increase ( obj ) { obj . value ++ ; } increase ( y ); console . log ( y ); // { value: 11 }, copied by ref, origin changed directly. 15. add/remove properties \u00b6 let o = { x : 1 } // add o . y = 2 ; // o is now {x:1, y:2} o . x // 1, do notation 0 [ \"x\" ] // 1, bracket notation // remove delete o . y ; 16. enamurating objects \u00b6 // constructor function Square ( a , b ){ this . x = a ; this . y = b ; this . area = function (){ console . log ( this . x * this . y ) }; } // new object let o = new Square ( 1 , 2 ) // for .. in for ( key in o ){ console . log ( key ) // x , y , area console . log ( o [ key ]) // 1, 2, funcion } // Object methods const keys = Object . keys ( o ); // [x, y, area] const values = Object . values ( o ); // [1, 2, function] // check if a key existed in an oject 'x' in o ; // true 'area' in o ; // true x in o ; // false 17. setters and getters \u00b6 function Square ( a , b ){ this . x = a ; this . y = b ; this . area = function (){ console . log ( this . x * this . y ) }; let defaultLocation = { x : 1 , y : 1 } // private // 1) old way this . getDefaultLocation = function (){ return defaultLocation // getter } this . setDefaultLocation = function ( obj ){ defaultLocation . x = obj . x ; defaultLocation . y = obj . y ; // this is setter function } // to access it; // obj.getDefalutLocation() fo getter // obj.setDefalutLocation(objet) fo setter // 2) Another way Object . definePropoerty ( this , 'defaultLocation' , { get : function (){ return defaultLocation } set : function ( obj ){ defaultLocation . x = obj . x ; defaultLocation . y = obj . y ; // this is setter function } }); // to access it; // obj.defalutLocation fo getter // obj.defalutLocation = objet fo setter } accessing private properties should be only using methods setters and getters.","title":"OOP in JS"},{"location":"knowledge-base/general/interviews/js_oop/#oop-in-js","text":"","title":"OOP in JS"},{"location":"knowledge-base/general/interviews/js_oop/#1-concepts-of-oop","text":"Abstraction Polymorphism Inheretance Encapsulation","title":"1. concepts of oop"},{"location":"knowledge-base/general/interviews/js_oop/#2-procedural-vs-oop","text":"in procedural programming you write functions. change one function then you need to change everywhere spaggitti code functions have more parameters in oop: you write classes. functions have less params. change only the code in the class and it will changes every where simply.","title":"2. procedural vs oop"},{"location":"knowledge-base/general/interviews/js_oop/#3-abstraction","text":"reduce the impact of change. hide un-necceary data. function Square ( a , b ){ this . x = a ; this . y = b ; this . area = function (){ console . log ( this . x * this . y ) }; let defaultLocation = { x : 1 , y : 1 } // private let defaultArea = function (){ // code, this method is private // we can call defaultLocation directly without this } } defaultLocation and defaultArea are private. you can't access them from the outer program. defaultLocation and defaultArea are local variables in the constructor function, so we can think of them as private but they are not . the square class interface does not contain defaultArea and defaultLocation and contain onlu x,y,area (the words with this). the square class is now abstracted, because we hide the default data so no body can miss around with them.","title":"3. Abstraction"},{"location":"knowledge-base/general/interviews/js_oop/#4-inheretance","text":"eleminate redandant code.","title":"4. Inheretance"},{"location":"knowledge-base/general/interviews/js_oop/#5-polymorphism","text":"the object behaves differently depending on the class we are referincing. refactor ugly switch/case statments.","title":"5. polymorphism"},{"location":"knowledge-base/general/interviews/js_oop/#6-encapsulation","text":"reduce complexity. increase reusability.","title":"6. Encapsulation"},{"location":"knowledge-base/general/interviews/js_oop/#7-objects-lterals","text":"const square = { x : 1 , y : 2 , area : function (){ console . log ( this . x * this . y ) } } object is collection of key value pairs.","title":"7. objects lterals"},{"location":"knowledge-base/general/interviews/js_oop/#8-factory-functions","text":"function createSquare ( a , b ){ return { x : a , y : b , area : function (){ console . log ( a * b ) } } } // so let q1 = createSquare ( 2 , 2 ); q1 . area () //4","title":"8. Factory functions"},{"location":"knowledge-base/general/interviews/js_oop/#9-constructors","text":"the first letter should be uppercase. it's like creating an instance of class, but in js there is no classes. function Square ( a , b ){ this . x = a ; this . y = b ; this . area = function (){ console . log ( this . x * this . y ) }; } // so q2 = new Square ( 2 , 2 ); // if we remove new, // 1) this will refer to the global object. // 2) q2 will be undefined. q2 . area () //4","title":"9. Constructors"},{"location":"knowledge-base/general/interviews/js_oop/#10-this","text":"by default this refers to the global object, eg. window . by using the new keyword before a constructor, this will refer to the new object.","title":"10. This"},{"location":"knowledge-base/general/interviews/js_oop/#11-every-object-has-a-constructor-refers-to-the-function-that-used-to-create-this-object","text":"","title":"11. every object has a constructor refers to the function that used to create this object."},{"location":"knowledge-base/general/interviews/js_oop/#12-default-constructors","text":"let q3 = new Object () q3 . x = 2 ; q3 . y = 2 ; q3 . area = function (){ console . log ( this . x * this . y ) }; // OR let q4 = new Object ({ x : 2 , y : 2 , area : function (){ console . log ( this . x * this . y ) } }) // so q3 . area () //4 q4 . area () //4 other default constructors: new String() , new Boolean() , new Number() ...","title":"12. Default constructors"},{"location":"knowledge-base/general/interviews/js_oop/#13-functions-are-objects","text":"const square = new Function ( 'a , b' , ` this.x = a; this.y = b; this.area = function (){ console.log(this.x * this.y) }; ` ) // So q5 = new square ( 2 , 2 ); q5 . area () //4","title":"13. Functions are objects"},{"location":"knowledge-base/general/interviews/js_oop/#14-value-types-pirmiatives-vs-reference-types","text":"value types (pirimatives): Number String Boolean Symbol undefined null Reference types: Object Array Map Set Function copying a pirimative type will copy its value to the new variable. copying a refernce type will not copy its value to the new var, instead the memory address for the first var is stored in the new var. so copying a reference type is actually pointing to tits memory address. // value let x = 10 ; let y = x ; x = 20 ; // x is 20, y still 10. // reference let x = { value : 10 }; let y = x ; x . value = 20 ; // y.value = x.value = 20. passing a var by its value to a function will not change its original value. passing a var by its reference will change its original value directly. // primiatves (by value) let x = 10 ; function increase ( num ) { num ++ ; } increase ( x ); console . log ( x ); // 10, copied by value, origin don't change. // Objects (by reference) let y = { value : 10 }; function increase ( obj ) { obj . value ++ ; } increase ( y ); console . log ( y ); // { value: 11 }, copied by ref, origin changed directly.","title":"14. Value Types (pirmiatives) vs Reference Types"},{"location":"knowledge-base/general/interviews/js_oop/#15-addremove-properties","text":"let o = { x : 1 } // add o . y = 2 ; // o is now {x:1, y:2} o . x // 1, do notation 0 [ \"x\" ] // 1, bracket notation // remove delete o . y ;","title":"15. add/remove properties"},{"location":"knowledge-base/general/interviews/js_oop/#16-enamurating-objects","text":"// constructor function Square ( a , b ){ this . x = a ; this . y = b ; this . area = function (){ console . log ( this . x * this . y ) }; } // new object let o = new Square ( 1 , 2 ) // for .. in for ( key in o ){ console . log ( key ) // x , y , area console . log ( o [ key ]) // 1, 2, funcion } // Object methods const keys = Object . keys ( o ); // [x, y, area] const values = Object . values ( o ); // [1, 2, function] // check if a key existed in an oject 'x' in o ; // true 'area' in o ; // true x in o ; // false","title":"16. enamurating objects"},{"location":"knowledge-base/general/interviews/js_oop/#17-setters-and-getters","text":"function Square ( a , b ){ this . x = a ; this . y = b ; this . area = function (){ console . log ( this . x * this . y ) }; let defaultLocation = { x : 1 , y : 1 } // private // 1) old way this . getDefaultLocation = function (){ return defaultLocation // getter } this . setDefaultLocation = function ( obj ){ defaultLocation . x = obj . x ; defaultLocation . y = obj . y ; // this is setter function } // to access it; // obj.getDefalutLocation() fo getter // obj.setDefalutLocation(objet) fo setter // 2) Another way Object . definePropoerty ( this , 'defaultLocation' , { get : function (){ return defaultLocation } set : function ( obj ){ defaultLocation . x = obj . x ; defaultLocation . y = obj . y ; // this is setter function } }); // to access it; // obj.defalutLocation fo getter // obj.defalutLocation = objet fo setter } accessing private properties should be only using methods setters and getters.","title":"17. setters and getters"},{"location":"knowledge-base/general/interviews/references/","text":"Interview Preperation \u00b6 References \u00b6 https://www.fullstack.cafe/javascript/what-is-coercion-in-javascript https://www.fullstack.cafe/design%20patterns https://medium.com/basecs https://github.com/kdn251/interviews perfect plan and scedule to master algorithms and data structures common behavioral questions, reply with STAR method questions to ask algorithms What I Wish I'd Known About Equity Before Joining A Unicorn leet code more algorithms","title":"Interview Preperation"},{"location":"knowledge-base/general/interviews/references/#interview-preperation","text":"","title":"Interview Preperation"},{"location":"knowledge-base/general/interviews/references/#references","text":"https://www.fullstack.cafe/javascript/what-is-coercion-in-javascript https://www.fullstack.cafe/design%20patterns https://medium.com/basecs https://github.com/kdn251/interviews perfect plan and scedule to master algorithms and data structures common behavioral questions, reply with STAR method questions to ask algorithms What I Wish I'd Known About Equity Before Joining A Unicorn leet code more algorithms","title":"References"},{"location":"knowledge-base/general/interviews/self_introduction/","text":"Self Introduction \u00b6 make it short, direct, attention grapping. you should mention the following: name, past companies, noteworthy projects (best if it's a public consumer product that they might have heard of). KISS (Keep It Simple and Sweet). Tell the interviewer why you would make a good hire. Format \u00b6 sentence about your current or most recent role. few sentences about your (academic) background. What did you focus on? Some sentences about your professional experience after school/university. Where did you work? What projects did you deal with? What were the typical challenges and tasks? Which technologies did you use? Finish with a statement saying why you are seeking a new job opportunity and why you are interested in the role you applied for. cuases or motivatio to like this role \u00b6 - create products for users to improve their life. \u00b6","title":"Self Introduction"},{"location":"knowledge-base/general/interviews/self_introduction/#self-introduction","text":"make it short, direct, attention grapping. you should mention the following: name, past companies, noteworthy projects (best if it's a public consumer product that they might have heard of). KISS (Keep It Simple and Sweet). Tell the interviewer why you would make a good hire.","title":"Self Introduction"},{"location":"knowledge-base/general/interviews/self_introduction/#format","text":"sentence about your current or most recent role. few sentences about your (academic) background. What did you focus on? Some sentences about your professional experience after school/university. Where did you work? What projects did you deal with? What were the typical challenges and tasks? Which technologies did you use? Finish with a statement saying why you are seeking a new job opportunity and why you are interested in the role you applied for.","title":"Format"},{"location":"knowledge-base/general/interviews/self_introduction/#cuases-or-motivatio-to-like-this-role","text":"","title":"cuases or motivatio to like this role"},{"location":"knowledge-base/general/interviews/self_introduction/#-create-products-for-users-to-improve-their-life","text":"","title":"- create products for users to improve their life."},{"location":"knowledge-base/general/javascript/","text":"","title":"Index"},{"location":"knowledge-base/general/javascript/arrays/","text":"Arrays \u00b6 notes \u00b6 create an array of length n filled with random numbers less than max: Array . from ({ length : n }, () => Math . floor ( Math . random () * max )); use localCompare() to sort nested structures. example: arr3 = arr3 . sort (( a , b ) => { return a [ 1 ]. localeCompare ( b [ 1 ]); //a and b are both arrays. }); updating arrays with immutability in mind \u00b6 we don't change the original array, we deep copy the array, then update the copy. const arr = [ 1 , 2 , 3 ]; // add to the first or the end of an array const newArr = [... arr , 4 ] /* OR */ [ 4 , ... arr ]; // add at a specific position const x = arr . indexOf ( 2 ); const newArr [... arr . slice ( 0 , x ), 4 , ... arr . slice ( x )]; // [1,4,2,3] // removing a specific element const newArr = arr . filter ( e => e !== 2 ); // [1,3]; // updating const newArr = arr . map ( e => { if ( e === 2 ) { return e = 20 } else { return e } } ); // [ 1,20,3] const newArr = arr . map ( e => e === 2 ? 20 : e ); // [1,20,3] // arr.map() will create a new copy, arr.forEach() will update the original array. - if the array contains objects, u need to deep copy them to comply with the immutabilty princible.","title":"Arrays"},{"location":"knowledge-base/general/javascript/arrays/#arrays","text":"","title":"Arrays"},{"location":"knowledge-base/general/javascript/arrays/#notes","text":"create an array of length n filled with random numbers less than max: Array . from ({ length : n }, () => Math . floor ( Math . random () * max )); use localCompare() to sort nested structures. example: arr3 = arr3 . sort (( a , b ) => { return a [ 1 ]. localeCompare ( b [ 1 ]); //a and b are both arrays. });","title":"notes"},{"location":"knowledge-base/general/javascript/arrays/#updating-arrays-with-immutability-in-mind","text":"we don't change the original array, we deep copy the array, then update the copy. const arr = [ 1 , 2 , 3 ]; // add to the first or the end of an array const newArr = [... arr , 4 ] /* OR */ [ 4 , ... arr ]; // add at a specific position const x = arr . indexOf ( 2 ); const newArr [... arr . slice ( 0 , x ), 4 , ... arr . slice ( x )]; // [1,4,2,3] // removing a specific element const newArr = arr . filter ( e => e !== 2 ); // [1,3]; // updating const newArr = arr . map ( e => { if ( e === 2 ) { return e = 20 } else { return e } } ); // [ 1,20,3] const newArr = arr . map ( e => e === 2 ? 20 : e ); // [1,20,3] // arr.map() will create a new copy, arr.forEach() will update the original array. - if the array contains objects, u need to deep copy them to comply with the immutabilty princible.","title":"updating arrays with immutability in mind"},{"location":"knowledge-base/general/javascript/cli/","text":"CLI \u00b6 to grap key from user command in your cli, use proccess.argv witch gives you an array and your first arg is at index 2 . tricky chain of functions : take user input, convert to string, split over ' ', extract numbers from str and assign them to item1 and item2. const [ item1 , item2 ] = input . toString (). split ( \" \" ). map ( Number );","title":"CLI"},{"location":"knowledge-base/general/javascript/cli/#cli","text":"to grap key from user command in your cli, use proccess.argv witch gives you an array and your first arg is at index 2 . tricky chain of functions : take user input, convert to string, split over ' ', extract numbers from str and assign them to item1 and item2. const [ item1 , item2 ] = input . toString (). split ( \" \" ). map ( Number );","title":"CLI"},{"location":"knowledge-base/general/javascript/datetime/","text":"Date and Time \u00b6 Notes \u00b6 get the date as number: Date.parse(ddate); get a readable date: Date(ddate).toLocalString() basic Date Time operations \u00b6 // you can pass a date or duration (number) or empty to get the time now > x = new Date ( \"2020-05-26T12:19:38.430Z\" ) //Tue May 26 2020 13:19:38 GMT+0100 (British Summer Time) > x . getFullYear () // 2020 > x . getMonth () //4 > const month = x . toLocaleString ( 'default' , { month : 'long' }); // May > let duration = ( new Date () ). getTime () - x . getTime (); //208426207 // distance between 2 dates > duration . toLocaleString () // \"208,426,207\" > duration . toString () //\"208426207\" > x . getDay () // 2 // second day on the week Tuesday > x . getDate () // 26 // day of the month Duration \u00b6 display the distance between a date and the moment const calculateDuration = ( num ) => { let durationDate = new Date ( num ); var epoch = new Date ( '1970-01-01 00:00:00-0600' ); var diff_years = durationDate . getYear () - epoch . getYear (); var diff_month = durationDate . getMonth () - epoch . getMonth (); var diff_days = durationDate . getDate () - epoch . getDate (); let yy = diff_years ? diff_years + ' years ' : '' ; let mm = diff_month ? diff_month + ' months ' : '' ; let dd = diff_days ? diff_days + ' days ' : '' ; return ` ${ yy } ${ mm } ${ dd } ` } /* * if num = \"208426207\" => calculateDuration is `2 days`. */ using day.js: // duration from now > dayjs ( date ). fromNow () // 2 years ago > dayjs ( date ). fromNow ( true ) // 2 years > dayjs ( date ). fromNow () // 2 days ago // time from now up to ${date} // duration between 2 dates > dayjs ( date1 ). from ( date2 ) // 2 years ago > daysjs ( date1 ). from ( date2 , true ) // 2 years //format date > dayjs ( date ). format ( 'DD MMMM YYYY' ) // 20 May 2020 resources \u00b6 https://day.js.org/docs/en/display/display","title":"Date and Time"},{"location":"knowledge-base/general/javascript/datetime/#date-and-time","text":"","title":"Date and Time"},{"location":"knowledge-base/general/javascript/datetime/#notes","text":"get the date as number: Date.parse(ddate); get a readable date: Date(ddate).toLocalString()","title":"Notes"},{"location":"knowledge-base/general/javascript/datetime/#basic-date-time-operations","text":"// you can pass a date or duration (number) or empty to get the time now > x = new Date ( \"2020-05-26T12:19:38.430Z\" ) //Tue May 26 2020 13:19:38 GMT+0100 (British Summer Time) > x . getFullYear () // 2020 > x . getMonth () //4 > const month = x . toLocaleString ( 'default' , { month : 'long' }); // May > let duration = ( new Date () ). getTime () - x . getTime (); //208426207 // distance between 2 dates > duration . toLocaleString () // \"208,426,207\" > duration . toString () //\"208426207\" > x . getDay () // 2 // second day on the week Tuesday > x . getDate () // 26 // day of the month","title":"basic Date Time operations"},{"location":"knowledge-base/general/javascript/datetime/#duration","text":"display the distance between a date and the moment const calculateDuration = ( num ) => { let durationDate = new Date ( num ); var epoch = new Date ( '1970-01-01 00:00:00-0600' ); var diff_years = durationDate . getYear () - epoch . getYear (); var diff_month = durationDate . getMonth () - epoch . getMonth (); var diff_days = durationDate . getDate () - epoch . getDate (); let yy = diff_years ? diff_years + ' years ' : '' ; let mm = diff_month ? diff_month + ' months ' : '' ; let dd = diff_days ? diff_days + ' days ' : '' ; return ` ${ yy } ${ mm } ${ dd } ` } /* * if num = \"208426207\" => calculateDuration is `2 days`. */ using day.js: // duration from now > dayjs ( date ). fromNow () // 2 years ago > dayjs ( date ). fromNow ( true ) // 2 years > dayjs ( date ). fromNow () // 2 days ago // time from now up to ${date} // duration between 2 dates > dayjs ( date1 ). from ( date2 ) // 2 years ago > daysjs ( date1 ). from ( date2 , true ) // 2 years //format date > dayjs ( date ). format ( 'DD MMMM YYYY' ) // 20 May 2020","title":"Duration"},{"location":"knowledge-base/general/javascript/datetime/#resources","text":"https://day.js.org/docs/en/display/display","title":"resources"},{"location":"knowledge-base/general/javascript/functions/","text":"Functions \u00b6 hiegher oreder functions \u00b6 function that take another function as an argument or return it or both function func1 ( func ) { func () } // as an argument function func2 () { return function func (){} } // return a func function func3 ( func ) { func (); return function (){} } // both handle function arguments \u00b6 get all arguments to a function if you don't know the number of args in advance: let argss = [... arguments ]; //args will not work in all cases. //OR let argss = []. slice . call ( arguments ); Functional Composition \u00b6 combining two or more functions to produce a new function. Composing functions together is like snapping together a series of pipes for our data to flow through. const func1 = str => str . trim (); const func2 = str => str . toLowerCase (); const func3 = str => `<div> ${ str } </div>` ; const result = func3 ( func2 ( func1 ( \" hello \" ) ) ); // compostion problems: you need to read code from right to left you ended up with a lot of pranctecess. so there is a cleaner way to compose functions, compose using lodash: import { compose , pipe } from \"lodash\" ; const func1 = str => str . trim (); const func2 = str => str . toLowerCase (); const func3 = str => `<div> ${ str } </div>` ; // composing the 3 functions const funcCompose = compose ( func3 , func2 , func1 ); // most right will be applyed first ( func1 ), then it goes left (func2) and so on const result = funcCompose ( \" hello \" ); // no parentecess, but we still read from right // OR const funcCompose = pipe ( func1 , func2 , func3 ); // most left function will be applied first, then to the right const result = funcCompose ( \" hello \" ); // no parentecess, reading from left. Function currying \u00b6 technique to reduces the number of arguments that a function needs. const func1 = str => `<div> ${ str } </div>` ; const func2 = str => `<span> ${ str } </span>` ; // we can write them in one function const func = ( str , htmlEle ) => { return `< ${ htmlEle } > ${ str } </ ${ htmlEle } >` }; // 2 args const func1 = str => func ( str , \"div\" ); // 1 argument only, currying const func2 = str => func ( str , \"span\" ); const result = func1 ( \" hello \" ); // OR const func = htmleEle => str => { return `< ${ htmlEle } > ${ str } </ ${ htmlEle } >` }; const result = func ( \"div\" )( \" hello \" ); // currying pure functions \u00b6 a function that alaways give us the same results if we gave it the same arguments. const func = x => x * Math . random (); // not pure, random const func = x => x * 2 ; // pure const func = x => x + Date (). toLocalString (); // not pure, contains date const func = x => x = X * 2 ; // NOT pure, it mutate its value, change the value of x. const func = x => x > y ; // NOT pure, y is global, if y changes the func result changes as well. const func = ( x , y ) => x > y ; // pure, x and y specified, so it alaways give us the same result. pure functions don't contain: random date or time global state (DOM, file, DB ..) pure function DO NOT change (mutate) the value of its arguments. benifits of pure funcs: self documenting: every thin is existed in the func. easily testable concurrency: since we don't need global state we can call these funcs in parrallel. cachable: sice we know that result will not change, we can cache the result of this function, and when the result needed again we can retrive this result from cache rather than do the computation one more time, this is useful when the pure func do heavy computation to compute the result.","title":"Functions"},{"location":"knowledge-base/general/javascript/functions/#functions","text":"","title":"Functions"},{"location":"knowledge-base/general/javascript/functions/#hiegher-oreder-functions","text":"function that take another function as an argument or return it or both function func1 ( func ) { func () } // as an argument function func2 () { return function func (){} } // return a func function func3 ( func ) { func (); return function (){} } // both","title":"hiegher oreder functions"},{"location":"knowledge-base/general/javascript/functions/#handle-function-arguments","text":"get all arguments to a function if you don't know the number of args in advance: let argss = [... arguments ]; //args will not work in all cases. //OR let argss = []. slice . call ( arguments );","title":"handle function arguments"},{"location":"knowledge-base/general/javascript/functions/#functional-composition","text":"combining two or more functions to produce a new function. Composing functions together is like snapping together a series of pipes for our data to flow through. const func1 = str => str . trim (); const func2 = str => str . toLowerCase (); const func3 = str => `<div> ${ str } </div>` ; const result = func3 ( func2 ( func1 ( \" hello \" ) ) ); // compostion problems: you need to read code from right to left you ended up with a lot of pranctecess. so there is a cleaner way to compose functions, compose using lodash: import { compose , pipe } from \"lodash\" ; const func1 = str => str . trim (); const func2 = str => str . toLowerCase (); const func3 = str => `<div> ${ str } </div>` ; // composing the 3 functions const funcCompose = compose ( func3 , func2 , func1 ); // most right will be applyed first ( func1 ), then it goes left (func2) and so on const result = funcCompose ( \" hello \" ); // no parentecess, but we still read from right // OR const funcCompose = pipe ( func1 , func2 , func3 ); // most left function will be applied first, then to the right const result = funcCompose ( \" hello \" ); // no parentecess, reading from left.","title":"Functional Composition"},{"location":"knowledge-base/general/javascript/functions/#function-currying","text":"technique to reduces the number of arguments that a function needs. const func1 = str => `<div> ${ str } </div>` ; const func2 = str => `<span> ${ str } </span>` ; // we can write them in one function const func = ( str , htmlEle ) => { return `< ${ htmlEle } > ${ str } </ ${ htmlEle } >` }; // 2 args const func1 = str => func ( str , \"div\" ); // 1 argument only, currying const func2 = str => func ( str , \"span\" ); const result = func1 ( \" hello \" ); // OR const func = htmleEle => str => { return `< ${ htmlEle } > ${ str } </ ${ htmlEle } >` }; const result = func ( \"div\" )( \" hello \" ); // currying","title":"Function currying"},{"location":"knowledge-base/general/javascript/functions/#pure-functions","text":"a function that alaways give us the same results if we gave it the same arguments. const func = x => x * Math . random (); // not pure, random const func = x => x * 2 ; // pure const func = x => x + Date (). toLocalString (); // not pure, contains date const func = x => x = X * 2 ; // NOT pure, it mutate its value, change the value of x. const func = x => x > y ; // NOT pure, y is global, if y changes the func result changes as well. const func = ( x , y ) => x > y ; // pure, x and y specified, so it alaways give us the same result. pure functions don't contain: random date or time global state (DOM, file, DB ..) pure function DO NOT change (mutate) the value of its arguments. benifits of pure funcs: self documenting: every thin is existed in the func. easily testable concurrency: since we don't need global state we can call these funcs in parrallel. cachable: sice we know that result will not change, we can cache the result of this function, and when the result needed again we can retrive this result from cache rather than do the computation one more time, this is useful when the pure func do heavy computation to compute the result.","title":"pure functions"},{"location":"knowledge-base/general/javascript/generaljs/","text":"General JS \u00b6 js functions index \u00b6 Boolean(any) arr.find() Number() arr.every() str.startsWith() str.endsWith() Notes \u00b6 arr.pop() and arr.unshift() returns the single Elemnt that they worked on. if you are returning a single element from an array, use arr.find() instead of arr.filter() make a new promise \u00b6 function sleep ( ms = 0 ) { return new Promise (( resolve ) => setTimeout ( resolve , ms )); } //get the result after a second sleep ( 1000 ). then (( data ) => console . log ( \"data\" , data )); this and that \u00b6 this and that in js, that refering to the main this in a scope, while you are going deeper into a scope this will refer to the deeper element, while that will refer to the parent: function MyConstructor ( options ) { let that = this ; this . someprop = options . someprop || \"defaultprop\" ; document . addEventListener ( \"click\" , ( event ) => { alert ( that . someprop ); }); } new MyConstructor ({ someprop : \"Hello World\" , }); swap quickly \u00b6 x = 1 ; y = 240 ; [ x , y ] = [ y , x ] >>> x , y // 240 , 1 double '!!' \u00b6 we can use double exclamation points !! to get a boolean value from non-boolean: let x = \"abc\" ; // x is truthy; !x = false; !!x = true; let y = null ; // y is falsey; !y = true; !!y= false; show Nice json \u00b6 < pre > { JSON . stringify ( values , null , 2 )} < /pre>","title":"General JS"},{"location":"knowledge-base/general/javascript/generaljs/#general-js","text":"","title":"General JS"},{"location":"knowledge-base/general/javascript/generaljs/#js-functions-index","text":"Boolean(any) arr.find() Number() arr.every() str.startsWith() str.endsWith()","title":"js functions index"},{"location":"knowledge-base/general/javascript/generaljs/#notes","text":"arr.pop() and arr.unshift() returns the single Elemnt that they worked on. if you are returning a single element from an array, use arr.find() instead of arr.filter()","title":"Notes"},{"location":"knowledge-base/general/javascript/generaljs/#make-a-new-promise","text":"function sleep ( ms = 0 ) { return new Promise (( resolve ) => setTimeout ( resolve , ms )); } //get the result after a second sleep ( 1000 ). then (( data ) => console . log ( \"data\" , data ));","title":"make a new promise"},{"location":"knowledge-base/general/javascript/generaljs/#this-and-that","text":"this and that in js, that refering to the main this in a scope, while you are going deeper into a scope this will refer to the deeper element, while that will refer to the parent: function MyConstructor ( options ) { let that = this ; this . someprop = options . someprop || \"defaultprop\" ; document . addEventListener ( \"click\" , ( event ) => { alert ( that . someprop ); }); } new MyConstructor ({ someprop : \"Hello World\" , });","title":"this and that"},{"location":"knowledge-base/general/javascript/generaljs/#swap-quickly","text":"x = 1 ; y = 240 ; [ x , y ] = [ y , x ] >>> x , y // 240 , 1","title":"swap quickly"},{"location":"knowledge-base/general/javascript/generaljs/#double","text":"we can use double exclamation points !! to get a boolean value from non-boolean: let x = \"abc\" ; // x is truthy; !x = false; !!x = true; let y = null ; // y is falsey; !y = true; !!y= false;","title":"double '!!'"},{"location":"knowledge-base/general/javascript/generaljs/#show-nice-json","text":"< pre > { JSON . stringify ( values , null , 2 )} < /pre>","title":"show Nice json"},{"location":"knowledge-base/general/javascript/is/","text":"IS? \u00b6 is string ? \u00b6 /** * Checks whether given value's type is a string * * @param wat A value to be checked. * @returns A boolean representing the result. */ function isString ( str ) { return Object . prototype . toString . call ( str ) === \"[object String]\" ; }","title":"IS?"},{"location":"knowledge-base/general/javascript/is/#is","text":"","title":"IS?"},{"location":"knowledge-base/general/javascript/is/#is-string","text":"/** * Checks whether given value's type is a string * * @param wat A value to be checked. * @returns A boolean representing the result. */ function isString ( str ) { return Object . prototype . toString . call ( str ) === \"[object String]\" ; }","title":"is string ?"},{"location":"knowledge-base/general/javascript/numbers/","text":"Numbers \u00b6 e == 0 will return true if e = flase or any other falsey value, e === 0 strict for 0 only. useful number formatter \\(espesially for currency\\) /** * initializing currency formatter */ const formatter = Intl . NumberFormat ( \"en-US\" , { style : \"currency\" , currency : \"GBP\" , }); find Min and Max elemnt of an Array: var numbers = [ 1 , 2 , 3 , 4 ]; Math . max . apply ( null , numbers ); // 4 Math . min . apply ( null , numbers ); // 1 //OR Math . max (... numbers ); // 4 Math . min (... numbers ); // 1 generating random number between 0 and max: Math . floor ( Math . random () * Math . floor ( max ));","title":"Numbers"},{"location":"knowledge-base/general/javascript/numbers/#numbers","text":"e == 0 will return true if e = flase or any other falsey value, e === 0 strict for 0 only. useful number formatter \\(espesially for currency\\) /** * initializing currency formatter */ const formatter = Intl . NumberFormat ( \"en-US\" , { style : \"currency\" , currency : \"GBP\" , }); find Min and Max elemnt of an Array: var numbers = [ 1 , 2 , 3 , 4 ]; Math . max . apply ( null , numbers ); // 4 Math . min . apply ( null , numbers ); // 1 //OR Math . max (... numbers ); // 4 Math . min (... numbers ); // 1 generating random number between 0 and max: Math . floor ( Math . random () * Math . floor ( max ));","title":"Numbers"},{"location":"knowledge-base/general/javascript/objects/","text":"Objects \u00b6 notes \u00b6 looping throgh an object: - for (let el in Obj) will loop over the keys. - for (let el of bj) will loop over the valuse ?? Immutabilty \u00b6 once we create an object we can not change it. and if we want to change it we have to copy it and mutate the copy. ex: strings is immutable, whan u change a string u actually create a new string. ex: objects in js are mutable. pros of immutability: makes our programs more predictable. faster in change detection. concurrency. cons of immutability: copying every elemnt in order to change it can slow ur performance. memory proplems: also associated with copying. the solution for this is structural sharing , when copying the elemnt only the changed elemnts will be copied, others will be shared, like shared by reference . updating objects \u00b6 updating an object with immutability priciples in mind: const obj = { name : \"john\" }; obj . name = \"Ahmad\" ; // wrong, we shouldn't do that, we need to creaete a copy first. // Object.assign() const newObj = Object . assign ({}, obj , { name : \"Ahmad\" , age : 27 }); // copy obj into newObj, and then update newObj with the new values. // spread operator const newObj = { ... obj } // make a copy newObj . name = \"Ahmad\" ; update the copy . spraed operator will make a shallow copy of any nested object, we need to create a new deep copy to apply our changes to the copy only const obj1 = { name : \"ahmad\" , address : { country : \"UK\" , city : \"London\" } }; const newObj = { ... obj1 } //copy, shallow copy for address. newObj . address . city = \"Bristol\" ; // the address of obj will changed as well. obj . address . city // Bristol. // To solve the problem const newObj = { ... obj , address : { ... obj . address }}; newObj . address . city = \"Bristol\" ; // the address of obj will NOT changed becaue we deep copied obj. obj . address . city // London. update object keys \u00b6 const changeKey = ( obj , old_key , new_key ) => { // if the key will change if ( old_key !== new_key ) { Object . defineProperty ( obj , new_key , // add a new property to obj with the obj[new_key] = same value. Object . getOwnPropertyDescriptor ( obj , old_key )); // descriptor contains the valeu of the property delete obj [ old_key ]; // delete old key and its value. } } Delete object key \u00b6 //1 delete obj . keyName ; //2 obj . keyName = undefined ; check if object \u00b6 const isObject = obj => !! obj && typeof obj === 'object' ;","title":"Objects"},{"location":"knowledge-base/general/javascript/objects/#objects","text":"","title":"Objects"},{"location":"knowledge-base/general/javascript/objects/#notes","text":"looping throgh an object: - for (let el in Obj) will loop over the keys. - for (let el of bj) will loop over the valuse ??","title":"notes"},{"location":"knowledge-base/general/javascript/objects/#immutabilty","text":"once we create an object we can not change it. and if we want to change it we have to copy it and mutate the copy. ex: strings is immutable, whan u change a string u actually create a new string. ex: objects in js are mutable. pros of immutability: makes our programs more predictable. faster in change detection. concurrency. cons of immutability: copying every elemnt in order to change it can slow ur performance. memory proplems: also associated with copying. the solution for this is structural sharing , when copying the elemnt only the changed elemnts will be copied, others will be shared, like shared by reference .","title":"Immutabilty"},{"location":"knowledge-base/general/javascript/objects/#updating-objects","text":"updating an object with immutability priciples in mind: const obj = { name : \"john\" }; obj . name = \"Ahmad\" ; // wrong, we shouldn't do that, we need to creaete a copy first. // Object.assign() const newObj = Object . assign ({}, obj , { name : \"Ahmad\" , age : 27 }); // copy obj into newObj, and then update newObj with the new values. // spread operator const newObj = { ... obj } // make a copy newObj . name = \"Ahmad\" ; update the copy . spraed operator will make a shallow copy of any nested object, we need to create a new deep copy to apply our changes to the copy only const obj1 = { name : \"ahmad\" , address : { country : \"UK\" , city : \"London\" } }; const newObj = { ... obj1 } //copy, shallow copy for address. newObj . address . city = \"Bristol\" ; // the address of obj will changed as well. obj . address . city // Bristol. // To solve the problem const newObj = { ... obj , address : { ... obj . address }}; newObj . address . city = \"Bristol\" ; // the address of obj will NOT changed becaue we deep copied obj. obj . address . city // London.","title":"updating objects"},{"location":"knowledge-base/general/javascript/objects/#update-object-keys","text":"const changeKey = ( obj , old_key , new_key ) => { // if the key will change if ( old_key !== new_key ) { Object . defineProperty ( obj , new_key , // add a new property to obj with the obj[new_key] = same value. Object . getOwnPropertyDescriptor ( obj , old_key )); // descriptor contains the valeu of the property delete obj [ old_key ]; // delete old key and its value. } }","title":"update object keys"},{"location":"knowledge-base/general/javascript/objects/#delete-object-key","text":"//1 delete obj . keyName ; //2 obj . keyName = undefined ;","title":"Delete object key"},{"location":"knowledge-base/general/javascript/objects/#check-if-object","text":"const isObject = obj => !! obj && typeof obj === 'object' ;","title":"check if object"},{"location":"knowledge-base/general/javascript/strings/","text":"Strings \u00b6 if you use string.split(/(regex)/) will split the str keeping the regex match element. use this to pass variables to the regex. let viraible_passes_to_regex = new RegExp ( `string contains a ${ var } ` , 'gi' );","title":"Strings"},{"location":"knowledge-base/general/javascript/strings/#strings","text":"if you use string.split(/(regex)/) will split the str keeping the regex match element. use this to pass variables to the regex. let viraible_passes_to_regex = new RegExp ( `string contains a ${ var } ` , 'gi' );","title":"Strings"},{"location":"knowledge-base/general/node/","text":"","title":"Index"},{"location":"knowledge-base/general/node/argv/","text":"process.argv \u00b6 using this variable, we can grab the all vars and flags passed to the current process. in this example, we will create a function to grab a specific flag when passed to a process. when passed to a process means that you specify the flag when calling node filename -flag flagValue // grab content of a specific flag const grab = ( flag ) => { let indexAfterFlag = process . argv . indexOf ( flag ) + 1 ; // calculate the indexAfterFlag return process . argv [ indexAfterFlag ]; }; // in our file const myFlagContent1 = grab ( \"-flag1\" ); // grab content of `-flag1` const myFlagContent2 = grab ( \"--flag2\" ); console . log ( myFlagContent1 , myFlagContent2 ); grab content of flags passed from terminal \u00b6 this function will grab the particular flag, no matter the order of the flags is. // grab content of a specific flag const grab = flag => { let indexAfterFlag = process . argv . indexOf ( flag ) + 1 ; return process . argv [ indexAfterFlag ]; } // in our file const myFlagContent1 = grab ( '-flag1' ); const myFlagContent2 = grab ( '--flag2' ); console . log ( myFlagContent1 , myFlagContent2 ); // in terminal >> node file . js - flag1 test -- flag2 \"test with spaces\" /* logs */ test test with space","title":"Argv"},{"location":"knowledge-base/general/node/argv/#processargv","text":"using this variable, we can grab the all vars and flags passed to the current process. in this example, we will create a function to grab a specific flag when passed to a process. when passed to a process means that you specify the flag when calling node filename -flag flagValue // grab content of a specific flag const grab = ( flag ) => { let indexAfterFlag = process . argv . indexOf ( flag ) + 1 ; // calculate the indexAfterFlag return process . argv [ indexAfterFlag ]; }; // in our file const myFlagContent1 = grab ( \"-flag1\" ); // grab content of `-flag1` const myFlagContent2 = grab ( \"--flag2\" ); console . log ( myFlagContent1 , myFlagContent2 );","title":"process.argv"},{"location":"knowledge-base/general/node/argv/#grab-content-of-flags-passed-from-terminal","text":"this function will grab the particular flag, no matter the order of the flags is. // grab content of a specific flag const grab = flag => { let indexAfterFlag = process . argv . indexOf ( flag ) + 1 ; return process . argv [ indexAfterFlag ]; } // in our file const myFlagContent1 = grab ( '-flag1' ); const myFlagContent2 = grab ( '--flag2' ); console . log ( myFlagContent1 , myFlagContent2 ); // in terminal >> node file . js - flag1 test -- flag2 \"test with spaces\" /* logs */ test test with space","title":"grab content of flags passed from terminal"},{"location":"knowledge-base/general/node/node/","text":"Node.js \u00b6 notes \u00b6 if your request is empty, check if you have body-parser res.send() can't send a number, you should respond with object { result: number } post request with fetch \u00b6 fetch ( \"https://cyf-chat-server--ahmadali5.repl.co/messages/newMessage\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" }, body : JSON . stringify ( newMsg ) }) Get the full url of the route \u00b6 var fullUrl = req . protocol + '://' + req . get ( 'host' ) + req . originalUrl ; error anatomy \u00b6 var err = { message : \"error message\" , // accessed by err or err.message stack : \"c:\\pth\\to\\file\" , errors : { /* more error data */ } } global object \u00b6 global object contains functions, vars and objects we can use without requiring any external module. and it contains: console: console.log(any) __dirname: gives us the absolute path to the current directory. __filename: gives us the absolute path to the current file. require() function : import other modules. process object : contains info about the current process. Require() function \u00b6 node core modules. our own modules. third party modules path module \u00b6 path.basename(): get the file name only. path.basename(__filename) process object \u00b6 process.pid : get current process id. process.versions.node : get current node version. get environment information and vars. communicate with the terminal or parent processes through standard input and standard output. process.argv : get an array of all arguments passed to this process. process.stdout, process.stdin : regulates interactions between terminal and our program. exit the current process. process.exit()","title":"Node"},{"location":"knowledge-base/general/node/node/#nodejs","text":"","title":"Node.js"},{"location":"knowledge-base/general/node/node/#notes","text":"if your request is empty, check if you have body-parser res.send() can't send a number, you should respond with object { result: number }","title":"notes"},{"location":"knowledge-base/general/node/node/#post-request-with-fetch","text":"fetch ( \"https://cyf-chat-server--ahmadali5.repl.co/messages/newMessage\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" }, body : JSON . stringify ( newMsg ) })","title":"post request with fetch"},{"location":"knowledge-base/general/node/node/#get-the-full-url-of-the-route","text":"var fullUrl = req . protocol + '://' + req . get ( 'host' ) + req . originalUrl ;","title":"Get the full url of the route"},{"location":"knowledge-base/general/node/node/#error-anatomy","text":"var err = { message : \"error message\" , // accessed by err or err.message stack : \"c:\\pth\\to\\file\" , errors : { /* more error data */ } }","title":"error anatomy"},{"location":"knowledge-base/general/node/node/#global-object","text":"global object contains functions, vars and objects we can use without requiring any external module. and it contains: console: console.log(any) __dirname: gives us the absolute path to the current directory. __filename: gives us the absolute path to the current file. require() function : import other modules. process object : contains info about the current process.","title":"global object"},{"location":"knowledge-base/general/node/node/#require-function","text":"node core modules. our own modules. third party modules","title":"Require() function"},{"location":"knowledge-base/general/node/node/#path-module","text":"path.basename(): get the file name only. path.basename(__filename)","title":"path module"},{"location":"knowledge-base/general/node/node/#process-object","text":"process.pid : get current process id. process.versions.node : get current node version. get environment information and vars. communicate with the terminal or parent processes through standard input and standard output. process.argv : get an array of all arguments passed to this process. process.stdout, process.stdin : regulates interactions between terminal and our program. exit the current process. process.exit()","title":"process object"},{"location":"knowledge-base/general/node/stdout/","text":"process.stdout, process.stdin \u00b6 stdout and stdin regulate communications between terminal and program. console.log is actually uses the process.stdout to print out messages. here is a complete example of program to ask a set of questions and record answers and print them back to the terminal. const questions = [ \"what's your name?\" , \"what's your favorite language?\" , \"where do you live?\" , \"are you happy?\" , ]; const ask = ( index = null ) => { if ( index === null ) { index = Math . floor ( Math . random () * Math . floor ( questions . length )); // generate random index no greater than questions.length } process . stdout . write ( ` ${ questions [ index ] } \\n` ); // write a question to stdout process . stdout . write ( \">\" ); // prompting for answer }; ask ( 0 ); const answers = []; process . stdin . on ( \"data\" , ( data ) => { answers . push ( data . toString (). trim ()); // adding answer to answers array if ( answers . length < questions . length ) { ask ( answers . length ); } if ( answers . length === questions . length ) { process . exit ( 0 ); // exit the process when we answered all questions, firing exit event } }); process . on ( \"exit\" , () => { // listening to exit event const [ name , fav , live , happy ] = answers ; process . stdout . write ( ` Thank you ${ name } For your answers you are living in ${ live } and you like coding in ${ fav } you are ${ happy === \"yes\" ? \"happy\" : \"NOT happy\" } ` ); }); another Example: Async functions with timers, and progress bar \u00b6 const waitTime = 3000 ; const intervalTime = 500 ; let currentTime = 0 ; const incTime = () => { currentTime += intervalTime ; const p = Math . floor (( currentTime / waitTime ) * 100 ); // calculate done percentage process . stdout . clearLine (); // delete the previous line in the stdout process . stdout . cursorTo ( 0 ); // move the cursor to the beginning of the line process . stdout . write ( `waiting .. ${ p } %` ); }; console . log ( `setting wait of ${ waitTime / 1000 } seconds` ); const timerFinished = () => { clearInterval ( interval ); process . stdout . clearLine (); // delete the waiting percentage log line process . stdout . cursorTo ( 0 ); console . log ( `done` ); }; setTimeout ( timerFinished , waitTime ); // call timerFinished after waitTime milliseconds const interval = setInterval ( incTime , intervalTime ); // returns an interval var we can use it when clearing","title":"process.stdout, process.stdin"},{"location":"knowledge-base/general/node/stdout/#processstdout-processstdin","text":"stdout and stdin regulate communications between terminal and program. console.log is actually uses the process.stdout to print out messages. here is a complete example of program to ask a set of questions and record answers and print them back to the terminal. const questions = [ \"what's your name?\" , \"what's your favorite language?\" , \"where do you live?\" , \"are you happy?\" , ]; const ask = ( index = null ) => { if ( index === null ) { index = Math . floor ( Math . random () * Math . floor ( questions . length )); // generate random index no greater than questions.length } process . stdout . write ( ` ${ questions [ index ] } \\n` ); // write a question to stdout process . stdout . write ( \">\" ); // prompting for answer }; ask ( 0 ); const answers = []; process . stdin . on ( \"data\" , ( data ) => { answers . push ( data . toString (). trim ()); // adding answer to answers array if ( answers . length < questions . length ) { ask ( answers . length ); } if ( answers . length === questions . length ) { process . exit ( 0 ); // exit the process when we answered all questions, firing exit event } }); process . on ( \"exit\" , () => { // listening to exit event const [ name , fav , live , happy ] = answers ; process . stdout . write ( ` Thank you ${ name } For your answers you are living in ${ live } and you like coding in ${ fav } you are ${ happy === \"yes\" ? \"happy\" : \"NOT happy\" } ` ); });","title":"process.stdout, process.stdin"},{"location":"knowledge-base/general/node/stdout/#another-example-async-functions-with-timers-and-progress-bar","text":"const waitTime = 3000 ; const intervalTime = 500 ; let currentTime = 0 ; const incTime = () => { currentTime += intervalTime ; const p = Math . floor (( currentTime / waitTime ) * 100 ); // calculate done percentage process . stdout . clearLine (); // delete the previous line in the stdout process . stdout . cursorTo ( 0 ); // move the cursor to the beginning of the line process . stdout . write ( `waiting .. ${ p } %` ); }; console . log ( `setting wait of ${ waitTime / 1000 } seconds` ); const timerFinished = () => { clearInterval ( interval ); process . stdout . clearLine (); // delete the waiting percentage log line process . stdout . cursorTo ( 0 ); console . log ( `done` ); }; setTimeout ( timerFinished , waitTime ); // call timerFinished after waitTime milliseconds const interval = setInterval ( incTime , intervalTime ); // returns an interval var we can use it when clearing","title":"another Example: Async functions with timers, and progress bar"},{"location":"knowledge-base/general/programming-languages/","text":"","title":"Index"},{"location":"knowledge-base/general/programming-languages/go/","text":"GoLang Docs \u00b6 commands \u00b6 go version go env : lists all en variables. run \u00b6 go run file.go : will compile and build then execute your file $GOPATH/bin/<file> : execute file.exe file after file.go being compiled. compile \u00b6 go install : build (compile) all apps into GOPATH/bin go install <dir>/<file> compile that file into GOPATH/bin go build : build (compile) this project inside this folder init \u00b6 go mod init <projectName> : create go.mod file for you, similar to package.json test \u00b6 go test -cover ./... will run all the tests around the project. Definitions \u00b6 GOROOT : the folder where goLang is installed GOPATH : the folder or workspace that contains all your go code. by default, on widows, GOPATH =C:/Users/<user>/go , GOROOT=C:/Go package clause \u00b6 the first line of the package: package main : entry point package <packageName> : package name should match the directory they are in, multiple files can form one package. notes \u00b6 will not run if unused var. prettier will remove the unsued imports automatically. if you want to compile with unuseed var, rename it to _ makefile is similar to npm scripts when importing local package, never start with / , alawyas start with the project name. when appending a struct or slice or map with a new item, never pass the added item by reference, alaways exeplicitly pass its value. only capitalized functions or Vars are exported from packages. Math \u00b6 math.sqrt2 >>> 1.4142135623730951 math.Floor(2.7) >>> round down // 2 math.Ceil(2.7) >>>> round up // 3 maps \u00b6 key/value pairs (like objects) use map of maps: var mapOfMaps = make ( map [ string ] map [ string ] string ) } // { \"_key1\" : { \"key1\" : \"value1\", \"key2\":\"value2\" } } slices \u00b6 array with no predefined length Functions \u00b6 strconv.Itoa : convert to string fmt.Sprintln(a,b) : converting to one string a + \" \" + b \\n fmt.Sprint(a,b) : return one s a+b pointers with go \u00b6 Go: var x int ; // assign var named x & x // give us the memory addres where x stored, &x is of type pointer to T of x (*int) * ( & x ) // give us the value that stored in address (&x) x == * ( & x ) //true Example: var x int ; fmt . Printf ( \"%T\" , & x ) // *int, type of pointer to int, Reserves 4 bytes in memory. fmt . Printf ( \"%p\" , & x ) // 0xc00002c008, momory address for the the pointer to x fmt . Printf ( \"%T\" , x ) // int, type of int. fmt . Printf ( \"%p\" , x ) // 0, value of x fmt . Printf ( \"%T\" , * ( & x )) // int, type of the value that stored in the momory address (&x) // writtin to memory using memory addres * ( & x ) = 4 ; fmt . Printf ( \"%v\\n\" , x ) // x is now 4; pass pointers to functions \u00b6 you can pass vars to functions in go : 1. by pointers: 1. the function gets access to the memory address (location) to the passed var, so it can directly change it's value. 2. in the function declaration: specify the type of var as *varType . 3. when calling the function: pass the var as &varName . pass by value: just pass a normal var, There is no passing variables by reference. because there are no 2 variables pointing to the same memory address. you can store the value of a specific address in more than one address but you can't have 2 varaibles pointing to the same memory address. ```go func main() { var x, y int = 5,6; SwapByPointers(&x,&y) // call the swapper passing memory addreses of x and y. fmt.Println(x,\"\\n\",y) // x now 6, y is 5 SwapByValue(x,y) // call the swapper passing reference for x and y. fmt.Println(x,\"\\n\",y) // x now 6, y is 5 } // passing *int which is a pointer to int func SwapByPointers(a *int, b *int){ var temp int = *(a) // storing the value in the stored memory (&a) into temp. *(a) = *(b) // swapping the values existed in the memory. *(b) = temp; } // passing the var, and then use its reference to do the swap func SwapByValue(a int, b int){ var temp int = *(&a) *(&a) = *(&b) *(&b) = temp; } ``` stack allocation is cheap, heap allocation is expensive. Packages \u00b6 log : logging to the cosole. net/http : create a server encoding/json : stringify, destringify json github.com/gorilla/mux : lite weight server strconv : handle strings web Apps \u00b6 read request body in pure net/http: ```go router.HandleFunc(\"/test\", func(res http.ResponseWriter, req *http.Request) { d, _ := ioutil.ReadAll(req.Body) log.Printf(\"data passed is %s\", d) }) ```` references \u00b6 https://segment.com/blog/allocation-efficiency-in-high-performance-go-services/#:~:text=Go%20allocates%20memory%20in%20two,will%20be%20on%20the%20stack. https://blog.golang.org/pprof","title":"GoLang Docs"},{"location":"knowledge-base/general/programming-languages/go/#golang-docs","text":"","title":"GoLang Docs"},{"location":"knowledge-base/general/programming-languages/go/#commands","text":"go version go env : lists all en variables.","title":"commands"},{"location":"knowledge-base/general/programming-languages/go/#run","text":"go run file.go : will compile and build then execute your file $GOPATH/bin/<file> : execute file.exe file after file.go being compiled.","title":"run"},{"location":"knowledge-base/general/programming-languages/go/#compile","text":"go install : build (compile) all apps into GOPATH/bin go install <dir>/<file> compile that file into GOPATH/bin go build : build (compile) this project inside this folder","title":"compile"},{"location":"knowledge-base/general/programming-languages/go/#init","text":"go mod init <projectName> : create go.mod file for you, similar to package.json","title":"init"},{"location":"knowledge-base/general/programming-languages/go/#test","text":"go test -cover ./... will run all the tests around the project.","title":"test"},{"location":"knowledge-base/general/programming-languages/go/#definitions","text":"GOROOT : the folder where goLang is installed GOPATH : the folder or workspace that contains all your go code. by default, on widows, GOPATH =C:/Users/<user>/go , GOROOT=C:/Go","title":"Definitions"},{"location":"knowledge-base/general/programming-languages/go/#package-clause","text":"the first line of the package: package main : entry point package <packageName> : package name should match the directory they are in, multiple files can form one package.","title":"package clause"},{"location":"knowledge-base/general/programming-languages/go/#notes","text":"will not run if unused var. prettier will remove the unsued imports automatically. if you want to compile with unuseed var, rename it to _ makefile is similar to npm scripts when importing local package, never start with / , alawyas start with the project name. when appending a struct or slice or map with a new item, never pass the added item by reference, alaways exeplicitly pass its value. only capitalized functions or Vars are exported from packages.","title":"notes"},{"location":"knowledge-base/general/programming-languages/go/#math","text":"math.sqrt2 >>> 1.4142135623730951 math.Floor(2.7) >>> round down // 2 math.Ceil(2.7) >>>> round up // 3","title":"Math"},{"location":"knowledge-base/general/programming-languages/go/#maps","text":"key/value pairs (like objects) use map of maps: var mapOfMaps = make ( map [ string ] map [ string ] string ) } // { \"_key1\" : { \"key1\" : \"value1\", \"key2\":\"value2\" } }","title":"maps"},{"location":"knowledge-base/general/programming-languages/go/#slices","text":"array with no predefined length","title":"slices"},{"location":"knowledge-base/general/programming-languages/go/#functions","text":"strconv.Itoa : convert to string fmt.Sprintln(a,b) : converting to one string a + \" \" + b \\n fmt.Sprint(a,b) : return one s a+b","title":"Functions"},{"location":"knowledge-base/general/programming-languages/go/#pointers-with-go","text":"Go: var x int ; // assign var named x & x // give us the memory addres where x stored, &x is of type pointer to T of x (*int) * ( & x ) // give us the value that stored in address (&x) x == * ( & x ) //true Example: var x int ; fmt . Printf ( \"%T\" , & x ) // *int, type of pointer to int, Reserves 4 bytes in memory. fmt . Printf ( \"%p\" , & x ) // 0xc00002c008, momory address for the the pointer to x fmt . Printf ( \"%T\" , x ) // int, type of int. fmt . Printf ( \"%p\" , x ) // 0, value of x fmt . Printf ( \"%T\" , * ( & x )) // int, type of the value that stored in the momory address (&x) // writtin to memory using memory addres * ( & x ) = 4 ; fmt . Printf ( \"%v\\n\" , x ) // x is now 4;","title":"pointers with go"},{"location":"knowledge-base/general/programming-languages/go/#pass-pointers-to-functions","text":"you can pass vars to functions in go : 1. by pointers: 1. the function gets access to the memory address (location) to the passed var, so it can directly change it's value. 2. in the function declaration: specify the type of var as *varType . 3. when calling the function: pass the var as &varName . pass by value: just pass a normal var, There is no passing variables by reference. because there are no 2 variables pointing to the same memory address. you can store the value of a specific address in more than one address but you can't have 2 varaibles pointing to the same memory address. ```go func main() { var x, y int = 5,6; SwapByPointers(&x,&y) // call the swapper passing memory addreses of x and y. fmt.Println(x,\"\\n\",y) // x now 6, y is 5 SwapByValue(x,y) // call the swapper passing reference for x and y. fmt.Println(x,\"\\n\",y) // x now 6, y is 5 } // passing *int which is a pointer to int func SwapByPointers(a *int, b *int){ var temp int = *(a) // storing the value in the stored memory (&a) into temp. *(a) = *(b) // swapping the values existed in the memory. *(b) = temp; } // passing the var, and then use its reference to do the swap func SwapByValue(a int, b int){ var temp int = *(&a) *(&a) = *(&b) *(&b) = temp; } ``` stack allocation is cheap, heap allocation is expensive.","title":"pass pointers to functions"},{"location":"knowledge-base/general/programming-languages/go/#packages","text":"log : logging to the cosole. net/http : create a server encoding/json : stringify, destringify json github.com/gorilla/mux : lite weight server strconv : handle strings","title":"Packages"},{"location":"knowledge-base/general/programming-languages/go/#web-apps","text":"read request body in pure net/http: ```go router.HandleFunc(\"/test\", func(res http.ResponseWriter, req *http.Request) { d, _ := ioutil.ReadAll(req.Body) log.Printf(\"data passed is %s\", d) }) ````","title":"web Apps"},{"location":"knowledge-base/general/programming-languages/go/#references","text":"https://segment.com/blog/allocation-efficiency-in-high-performance-go-services/#:~:text=Go%20allocates%20memory%20in%20two,will%20be%20on%20the%20stack. https://blog.golang.org/pprof","title":"references"},{"location":"knowledge-base/general/programming-languages/python/","text":"python \u00b6 ternary operations: \u00b6 state = \"nice\" if is_nice else \"not nice\" import files \u00b6 from fileName import module # same root directory from folder.fileName import module # if the file existed in a sub directory General \u00b6 function description print(any, sep=\"\\t\") print any number of arguments separated by Tab pprint(any) pretty print Dictionary \u00b6 function description d = dict() create empty dict d.get(key, defaultValue) get the value of a key in dictionary, if not found will assign defaultValue to it d.keys() return a list of all keys in dictionary d.values() return a list of all values in dictionary d.items() return a list of items as key, value Tuple Random \u00b6 function description access random by import random random.random() generates random real (float) numbers between 0.0 and 1.0 (not included) random.randint(low, high) generates random integer between low and high (both included) random.choice(list) selects a random element from the list. List \u00b6 function description t = list() create empty list t.sort( reverse=True ) sort list descending order t.extend(list) take every element of list and add it to end of t by order","title":"python"},{"location":"knowledge-base/general/programming-languages/python/#python","text":"","title":"python"},{"location":"knowledge-base/general/programming-languages/python/#ternary-operations","text":"state = \"nice\" if is_nice else \"not nice\"","title":"ternary operations:"},{"location":"knowledge-base/general/programming-languages/python/#import-files","text":"from fileName import module # same root directory from folder.fileName import module # if the file existed in a sub directory","title":"import files"},{"location":"knowledge-base/general/programming-languages/python/#general","text":"function description print(any, sep=\"\\t\") print any number of arguments separated by Tab pprint(any) pretty print","title":"General"},{"location":"knowledge-base/general/programming-languages/python/#dictionary","text":"function description d = dict() create empty dict d.get(key, defaultValue) get the value of a key in dictionary, if not found will assign defaultValue to it d.keys() return a list of all keys in dictionary d.values() return a list of all values in dictionary d.items() return a list of items as key, value Tuple","title":"Dictionary"},{"location":"knowledge-base/general/programming-languages/python/#random","text":"function description access random by import random random.random() generates random real (float) numbers between 0.0 and 1.0 (not included) random.randint(low, high) generates random integer between low and high (both included) random.choice(list) selects a random element from the list.","title":"Random"},{"location":"knowledge-base/general/programming-languages/python/#list","text":"function description t = list() create empty list t.sort( reverse=True ) sort list descending order t.extend(list) take every element of list and add it to end of t by order","title":"List"},{"location":"knowledge-base/general/software-development/","text":"software_development \u00b6","title":"software\\_development"},{"location":"knowledge-base/general/software-development/#software_development","text":"","title":"software_development"},{"location":"knowledge-base/general/software-development/agile/","text":"Agile process \u00b6 Working software is a key aspect of Agile. software development is all about iterations, if you succeed or failed this week does not mean that the things will be the same next week. so we need to be adaptable. quotes \u00b6 General George S. Patton once said, tell people where to go, but not how to get there, and you'll be amazed by the results. Henrik Kniberg, an Agile trainer and author, said: \u201cThe important thing is not your process. The important thing is your process for improving your process.\u201d Alan Lakein, author og famous books on personal time management. He says, \"Planning is bringing the future into the present so you can do something about it now.\" terminology \u00b6 verified product : means that the product was \"done right\" and meets the standards laid out by the development team. validated product : meets the needs of the client. We say that a validated product is the \"right product\". hacking : getting the things work without propper pre-planning. Ad hoc development: coding from your mind without a specific plan. Agile Manifesto \u00b6 team of developers gathered to come up with better ways to manage the process of producing great software. They established foGeneral George S. Patton once said, tell people where to go, but not how to get there, and you'll be amazed by the results.ur core value statements about what is most important in software development. To further expand on the core values, the Alliance also agreed on 12 supporting principles. 4 core values of Agile: Individuals and interactions over processes and tools. Working software over comprehensive documentation. Customer collaboration over contract negotiation. Responding to change over following a plan. from these 4 core vluaes we can say: Your developers and clients will be more productive and effective in a collaborative relationship. You need to facilitate communication between the people wanting the product and the people producing the product. documentation should still be created constantly throughout the project. But when tough choices have to be made, a piece of working software is a lot more valuable than a document that outlines what the software should do. you need to foster a positive relationship with your customer that is more concerned with what they want and not as concerned with what is laid out in some contract. The client is the center of design and it is important to make sure that their vision is what is being developed. Software is constantly changing and it is important to develop your software in such a way that changes are easy to respond to. Agile 12 principles: \u00b6 Our highest priority is to satisfy the customer through early and continuous delivery of valuable software. Deliver working software frequently, from a couple of weeks to a couple of months with a preference to the shorter timescale. Working software is the primary measure of progress. welcome changing requirements even late in development. Agile processes harness change for the customer's competitive advantage. continuous attention to technical excellence and good design enhances agility. Having readable, simple code and more flexible designs will allow changes to be easily implemented. Having a good design can let you know what components are dependent on each other. agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. simplicity, the art of maximizing the amount of work not done, is essential. This does not mean that Agile is about delivering less. It means that Agile is about delivering what is essential and reducing unnecessary work. build projects around motivated individuals. Give them the environment and support that they need, and trust them to get the job done. the best architectures, requirements, and designs emerge from self-organizing teams client-developer communication, business people and developers must work together daily throughout the project. the most efficient and effective method for conveying information to and within a development team is face-to-face conversation. reviewing what's been done, at regular intervals, the team reflects on how to become more effective, then tunes and adjusts it's behavior accordingly. good ways to make your software product more change friendly \u00b6 Frequent client communication. Neat, commented source code. Continuously reviewing and improving your project. Updated, prioritized list of features. Development team that is open to change. practices follow the concept of simplicity from the Agile Manifesto \u00b6 minimal code that satisfies the desired functionality. essential documentation over elaborate documentation. those are NOT following the simplicity: 1. no comments at all in the code. 2. only developing essential features : he team should develop features not based on their own criteria of what is important, but what is going to satisfy the client. problems will be solved through monitoring \u00b6 Adapting to changing product requirements. Meeting project plan deadlines. those are NOT solved by monitoring: 1. Avoiding having to fire his least productive developer: monitoring should not be seen as a way to assess the career progress. Keep it light. 2. Working without interruptions from the client: monitoring should not be a substitute for good communication with your client. resources \u00b6 what is exactly a product manager Tedx: the first secret of design Tedx: how to manage for collective creativity survey result comparng software engenniring methodologies Agile manifesto website","title":"Agile process"},{"location":"knowledge-base/general/software-development/agile/#agile-process","text":"Working software is a key aspect of Agile. software development is all about iterations, if you succeed or failed this week does not mean that the things will be the same next week. so we need to be adaptable.","title":"Agile process"},{"location":"knowledge-base/general/software-development/agile/#quotes","text":"General George S. Patton once said, tell people where to go, but not how to get there, and you'll be amazed by the results. Henrik Kniberg, an Agile trainer and author, said: \u201cThe important thing is not your process. The important thing is your process for improving your process.\u201d Alan Lakein, author og famous books on personal time management. He says, \"Planning is bringing the future into the present so you can do something about it now.\"","title":"quotes"},{"location":"knowledge-base/general/software-development/agile/#terminology","text":"verified product : means that the product was \"done right\" and meets the standards laid out by the development team. validated product : meets the needs of the client. We say that a validated product is the \"right product\". hacking : getting the things work without propper pre-planning. Ad hoc development: coding from your mind without a specific plan.","title":"terminology"},{"location":"knowledge-base/general/software-development/agile/#agile-manifesto","text":"team of developers gathered to come up with better ways to manage the process of producing great software. They established foGeneral George S. Patton once said, tell people where to go, but not how to get there, and you'll be amazed by the results.ur core value statements about what is most important in software development. To further expand on the core values, the Alliance also agreed on 12 supporting principles. 4 core values of Agile: Individuals and interactions over processes and tools. Working software over comprehensive documentation. Customer collaboration over contract negotiation. Responding to change over following a plan. from these 4 core vluaes we can say: Your developers and clients will be more productive and effective in a collaborative relationship. You need to facilitate communication between the people wanting the product and the people producing the product. documentation should still be created constantly throughout the project. But when tough choices have to be made, a piece of working software is a lot more valuable than a document that outlines what the software should do. you need to foster a positive relationship with your customer that is more concerned with what they want and not as concerned with what is laid out in some contract. The client is the center of design and it is important to make sure that their vision is what is being developed. Software is constantly changing and it is important to develop your software in such a way that changes are easy to respond to.","title":"Agile Manifesto"},{"location":"knowledge-base/general/software-development/agile/#agile-12-principles","text":"Our highest priority is to satisfy the customer through early and continuous delivery of valuable software. Deliver working software frequently, from a couple of weeks to a couple of months with a preference to the shorter timescale. Working software is the primary measure of progress. welcome changing requirements even late in development. Agile processes harness change for the customer's competitive advantage. continuous attention to technical excellence and good design enhances agility. Having readable, simple code and more flexible designs will allow changes to be easily implemented. Having a good design can let you know what components are dependent on each other. agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. simplicity, the art of maximizing the amount of work not done, is essential. This does not mean that Agile is about delivering less. It means that Agile is about delivering what is essential and reducing unnecessary work. build projects around motivated individuals. Give them the environment and support that they need, and trust them to get the job done. the best architectures, requirements, and designs emerge from self-organizing teams client-developer communication, business people and developers must work together daily throughout the project. the most efficient and effective method for conveying information to and within a development team is face-to-face conversation. reviewing what's been done, at regular intervals, the team reflects on how to become more effective, then tunes and adjusts it's behavior accordingly.","title":"Agile 12 principles:"},{"location":"knowledge-base/general/software-development/agile/#good-ways-to-make-your-software-product-more-change-friendly","text":"Frequent client communication. Neat, commented source code. Continuously reviewing and improving your project. Updated, prioritized list of features. Development team that is open to change.","title":"good ways to make your software product more change friendly"},{"location":"knowledge-base/general/software-development/agile/#practices-follow-the-concept-of-simplicity-from-the-agile-manifesto","text":"minimal code that satisfies the desired functionality. essential documentation over elaborate documentation. those are NOT following the simplicity: 1. no comments at all in the code. 2. only developing essential features : he team should develop features not based on their own criteria of what is important, but what is going to satisfy the client.","title":"practices follow the concept of simplicity from the Agile Manifesto"},{"location":"knowledge-base/general/software-development/agile/#problems-will-be-solved-through-monitoring","text":"Adapting to changing product requirements. Meeting project plan deadlines. those are NOT solved by monitoring: 1. Avoiding having to fire his least productive developer: monitoring should not be seen as a way to assess the career progress. Keep it light. 2. Working without interruptions from the client: monitoring should not be a substitute for good communication with your client.","title":"problems will be solved through monitoring"},{"location":"knowledge-base/general/software-development/agile/#resources","text":"what is exactly a product manager Tedx: the first secret of design Tedx: how to manage for collective creativity survey result comparng software engenniring methodologies Agile manifesto website","title":"resources"},{"location":"knowledge-base/general/software-development/requirments/","text":"Requirments \u00b6 understanding of the problems anser what rather than how distingush right system from 'wrong` system. Resources \u00b6 http://www.agilemodeling.com/essays/costOfChange.htm http://www.agile-process.org/change.html","title":"Requirments"},{"location":"knowledge-base/general/software-development/requirments/#requirments","text":"understanding of the problems anser what rather than how distingush right system from 'wrong` system.","title":"Requirments"},{"location":"knowledge-base/general/software-development/requirments/#resources","text":"http://www.agilemodeling.com/essays/costOfChange.htm http://www.agile-process.org/change.html","title":"Resources"},{"location":"knowledge-base/general/software-development/terminology/","text":"programming terminoloy \u00b6 Programming Paradigms \u00b6 various standard techniques used in programming [2]. 1. imperative \u00b6 oldeset style. functions are built out of statements which are generally phrased as imperative commands [1]. disadvantges: code reusability. unsafe nature of goto statements for implementing iterative blocks of code [2]. 2. procedural \u00b6 procedures (functions) are stitched together to form a program solves the problems that were encountered in the imperative programming. all programs can be seen as composed of the following control-structures [2]: sequential - ordered statements executed in a sequence selective - execute a selected block of statements based on the state of a program (Eg. if then else... ) iterative - execute a block repeatedly till some state is reached (Eg. while/for loop...) recursion - a statement is executed by repeatedly calling itself until some termination conditions are met 3. functional programming \u00b6 use pure functions. Pure functions are those that do not use any state from other entities. Every function is a self-sufficient entity that just performs a complex transformation on given inputs to produce outputs. It doesn't modify the input and is practically isolated from rest of the program [2]. The need of this pure functions arose from the fact that impure functions lead to various problems. Since they don't produce the same output on the same input, it makes the code harder for others to understand, debug and extend. advantages [2]: Lazy Evaluations: Since pure functions directly operate on input and behave like a transformation, one can actually remember transformations and apply them only when needed. So, when an input goes through series of pure functions sequentially, one can evaluate all those later only when the result is asked for. Parallelization: Pure functions can be easily parallelized as they are independent of each other. 4. Object-oriented Programming \u00b6 OOP is a programming paradigm based on a concept of entities (objects), which contains data (member variables) and behaviors (member functions) [2]. In OOP, computer programs are designed by making them out of objects interacting with one another. Every behavior of an object can possibly change the state ( member variables) of that object [2]. advantages Code reusability - Features like classes and inheritance make a lot of code reusable Encapsulation - Objects have abilities to hide a certain part of themselves from programmers. This prevents programmers tampering with things they shouldn't. Also, classes are designed to group related data implementing encapsulation [2]. Intuitiveness and Easy Design - It allows programmers to intuitively create large programs in a structured fashion. Extensible - Code in OOP is readable and much easier to maintain and modify compared to other paradigms programming languge features \u00b6 a programming language can described as [1]: modular : programs are made up of packages, which are made up of files. program broken into many modules which are independent of each other and interact only through a well-defined interface. 3. object-oriented statically-typed garbage-collected compiled: compiler takes the source code and translate it into machine language before execution. imperative. procedural. general \u00b6 statment : sequence of instructions. expression : the computation of a value by applying operators and functions to operands . functions \u00b6 parameters: vars in between parcanteces when you define the function. arguments: vars inside parantheces when you call the function. Memory and pointers \u00b6 Main memory is a sequence of storage locations Each location contains a value (content) and has a unique address A pointer is an address of a location allocated in main memory to store a value Pointer valued variables can store addresses of memory locations when a program sart executing, enough ammount of memory is reserved to this program. these reserved memory is bieng segmented to 3 segments 1 (segmentation) : stack segment. data segment code segment trying to access address of memory that is not bieng reserved to this program will give you segmentaion fault or error use of activation records in the stack to manage all local variables in this program. all local vars of all functions will be saved or reserved in the stack segment. Example: Dynamic vs Static \u00b6 static var: the var defined and has its own value before the program starts executing, therefor it will be placed in the stack when the program starts executing. dynamic: the var is not defined or has no value when the program starts executing, therefor the program will dynamically allocate memory for this var, this var will be placed in the heap. References \u00b6 [1] https://courses.edx.org/courses/course-v1:IITBombayX+CS101.2x+1T2020/course/ [2] https://courses.edx.org/courses/course-v1:IITBombayX+CS101.2x+1T2020/courseware/0bedc4e4756c42a9a704850dc93cffcf/ccab98a4b067400dabfe1ba3fd53a694/?activate_block_id=block-v1%3AIITBombayX%2BCS101.2x%2B1T2020%2Btype%40sequential%2Bblock%40ccab98a4b067400dabfe1ba3fd53a694","title":"programming terminoloy"},{"location":"knowledge-base/general/software-development/terminology/#programming-terminoloy","text":"","title":"programming terminoloy"},{"location":"knowledge-base/general/software-development/terminology/#programming-paradigms","text":"various standard techniques used in programming [2].","title":"Programming Paradigms"},{"location":"knowledge-base/general/software-development/terminology/#1-imperative","text":"oldeset style. functions are built out of statements which are generally phrased as imperative commands [1]. disadvantges: code reusability. unsafe nature of goto statements for implementing iterative blocks of code [2].","title":"1. imperative"},{"location":"knowledge-base/general/software-development/terminology/#2-procedural","text":"procedures (functions) are stitched together to form a program solves the problems that were encountered in the imperative programming. all programs can be seen as composed of the following control-structures [2]: sequential - ordered statements executed in a sequence selective - execute a selected block of statements based on the state of a program (Eg. if then else... ) iterative - execute a block repeatedly till some state is reached (Eg. while/for loop...) recursion - a statement is executed by repeatedly calling itself until some termination conditions are met","title":"2. procedural"},{"location":"knowledge-base/general/software-development/terminology/#3-functional-programming","text":"use pure functions. Pure functions are those that do not use any state from other entities. Every function is a self-sufficient entity that just performs a complex transformation on given inputs to produce outputs. It doesn't modify the input and is practically isolated from rest of the program [2]. The need of this pure functions arose from the fact that impure functions lead to various problems. Since they don't produce the same output on the same input, it makes the code harder for others to understand, debug and extend. advantages [2]: Lazy Evaluations: Since pure functions directly operate on input and behave like a transformation, one can actually remember transformations and apply them only when needed. So, when an input goes through series of pure functions sequentially, one can evaluate all those later only when the result is asked for. Parallelization: Pure functions can be easily parallelized as they are independent of each other.","title":"3. functional programming"},{"location":"knowledge-base/general/software-development/terminology/#4-object-oriented-programming","text":"OOP is a programming paradigm based on a concept of entities (objects), which contains data (member variables) and behaviors (member functions) [2]. In OOP, computer programs are designed by making them out of objects interacting with one another. Every behavior of an object can possibly change the state ( member variables) of that object [2]. advantages Code reusability - Features like classes and inheritance make a lot of code reusable Encapsulation - Objects have abilities to hide a certain part of themselves from programmers. This prevents programmers tampering with things they shouldn't. Also, classes are designed to group related data implementing encapsulation [2]. Intuitiveness and Easy Design - It allows programmers to intuitively create large programs in a structured fashion. Extensible - Code in OOP is readable and much easier to maintain and modify compared to other paradigms","title":"4. Object-oriented Programming"},{"location":"knowledge-base/general/software-development/terminology/#programming-languge-features","text":"a programming language can described as [1]: modular : programs are made up of packages, which are made up of files. program broken into many modules which are independent of each other and interact only through a well-defined interface. 3. object-oriented statically-typed garbage-collected compiled: compiler takes the source code and translate it into machine language before execution. imperative. procedural.","title":"programming languge features"},{"location":"knowledge-base/general/software-development/terminology/#general","text":"statment : sequence of instructions. expression : the computation of a value by applying operators and functions to operands .","title":"general"},{"location":"knowledge-base/general/software-development/terminology/#functions","text":"parameters: vars in between parcanteces when you define the function. arguments: vars inside parantheces when you call the function.","title":"functions"},{"location":"knowledge-base/general/software-development/terminology/#memory-and-pointers","text":"Main memory is a sequence of storage locations Each location contains a value (content) and has a unique address A pointer is an address of a location allocated in main memory to store a value Pointer valued variables can store addresses of memory locations when a program sart executing, enough ammount of memory is reserved to this program. these reserved memory is bieng segmented to 3 segments 1 (segmentation) : stack segment. data segment code segment trying to access address of memory that is not bieng reserved to this program will give you segmentaion fault or error use of activation records in the stack to manage all local variables in this program. all local vars of all functions will be saved or reserved in the stack segment. Example:","title":"Memory and pointers"},{"location":"knowledge-base/general/software-development/terminology/#dynamic-vs-static","text":"static var: the var defined and has its own value before the program starts executing, therefor it will be placed in the stack when the program starts executing. dynamic: the var is not defined or has no value when the program starts executing, therefor the program will dynamically allocate memory for this var, this var will be placed in the heap.","title":"Dynamic vs Static"},{"location":"knowledge-base/general/software-development/terminology/#references","text":"[1] https://courses.edx.org/courses/course-v1:IITBombayX+CS101.2x+1T2020/course/ [2] https://courses.edx.org/courses/course-v1:IITBombayX+CS101.2x+1T2020/courseware/0bedc4e4756c42a9a704850dc93cffcf/ccab98a4b067400dabfe1ba3fd53a694/?activate_block_id=block-v1%3AIITBombayX%2BCS101.2x%2B1T2020%2Btype%40sequential%2Bblock%40ccab98a4b067400dabfe1ba3fd53a694","title":"References"},{"location":"knowledge-base/general/think/","text":"","title":"Index"},{"location":"knowledge-base/general/think/general/","text":"General \u00b6 keyword Description Markov Analysis clacualtes the propability of the next word after a specific word","title":"General"},{"location":"knowledge-base/general/think/general/#general","text":"keyword Description Markov Analysis clacualtes the propability of the next word after a specific word","title":"General"},{"location":"knowledge-base/general/think/thinking_logically_games/","text":"thinking_logically_games \u00b6 These interactive puzzles are taken from the Introduction to Discrete Mathematics for Computer Science specialization . They are optional, but we strongly encourage you to solve them: they will help you to \"invent\" the key algorithmic ideas on your own and will help you to solve the programming challenges. Even if you fail to solve some puzzles, the time will not be lost as you will better appreciate the beauty and power of the underlying ideas. games list: \u00b6 Balls in Boxes : \u00b6 Distribute N black balls among 10 boxes so that every two boxes have different number of balls \\(you can put 0 balls in some box if you want to\\) . Fill in numbers of black balls in each box below. http://dm.compsciclub.ru/app/quiz-balls-in-boxes Activity selection \u00b6 Select as many non-overlapping segments as possible http://dm.compsciclub.ru/app/quiz-activity-selection Touch all segments \u00b6 Touch the given k segments with minimum number of vertical lines http://dm.compsciclub.ru/app/quiz-touch-all-segments Two Adjacent Cells of Opposite Colors \u00b6 You are given 20 black and white cells. The leftmost one is white, the rightmost one is black, the colors of all other cells are hidden. You can reveal the color of a cell by clicking on it. Your goal is to find two adjacent cells of different colors by using at most 5 clicks. http://dm.compsciclub.ru/app/quiz-opposite-colors","title":"thinking\\_logically\\_games"},{"location":"knowledge-base/general/think/thinking_logically_games/#thinking_logically_games","text":"These interactive puzzles are taken from the Introduction to Discrete Mathematics for Computer Science specialization . They are optional, but we strongly encourage you to solve them: they will help you to \"invent\" the key algorithmic ideas on your own and will help you to solve the programming challenges. Even if you fail to solve some puzzles, the time will not be lost as you will better appreciate the beauty and power of the underlying ideas.","title":"thinking_logically_games"},{"location":"knowledge-base/general/think/thinking_logically_games/#games-list","text":"","title":"games list:"},{"location":"knowledge-base/general/think/thinking_logically_games/#balls-in-boxes","text":"Distribute N black balls among 10 boxes so that every two boxes have different number of balls \\(you can put 0 balls in some box if you want to\\) . Fill in numbers of black balls in each box below. http://dm.compsciclub.ru/app/quiz-balls-in-boxes","title":"Balls in Boxes :"},{"location":"knowledge-base/general/think/thinking_logically_games/#activity-selection","text":"Select as many non-overlapping segments as possible http://dm.compsciclub.ru/app/quiz-activity-selection","title":"Activity selection"},{"location":"knowledge-base/general/think/thinking_logically_games/#touch-all-segments","text":"Touch the given k segments with minimum number of vertical lines http://dm.compsciclub.ru/app/quiz-touch-all-segments","title":"Touch all segments"},{"location":"knowledge-base/general/think/thinking_logically_games/#two-adjacent-cells-of-opposite-colors","text":"You are given 20 black and white cells. The leftmost one is white, the rightmost one is black, the colors of all other cells are hidden. You can reveal the color of a cell by clicking on it. Your goal is to find two adjacent cells of different colors by using at most 5 clicks. http://dm.compsciclub.ru/app/quiz-opposite-colors","title":"Two Adjacent Cells of Opposite Colors"},{"location":"knowledge-base/general/tools/","text":"","title":"Index"},{"location":"knowledge-base/general/tools/bash/","text":"Bash shell \u00b6 intro \u00b6 intro time // show the time date // show the date dd/mm/yyyy echo \"\" // print something path // show env vars path which < program name > // show the path for this program pwd // print working directory tail // show the last 10 lines of file tail - n1 // show the last line of file tee < file name > // open interactive to write to file line by line cd cd < relative path | absolute path > // move to that path cd ~ // go home cd - // go to previous path clear the terminal clear // clear the terminal Ctrl + l // clear the console. File Tree \u00b6 ls ls -- help // ls help ls // show all files. ls - a // show all files, including hiddens. ls - l // show all files with details tree tree // show only folder structure tree / f // show folders with files Handle files \u00b6 open files cat file . ext // open file in the terminal delete file rm < file_name > // delete a file rm - r < directory_name > // delete a directory move, copy, rename files mv < path1 / file > < path2 / file > // move file from path1 to path2 mv < path / file1 > < path / file2 > // rename file1 to be file2 cp < path1 > < path2 > // copy file from path to path cat < file1 > file2 // extract file1 content and store it in file2, copy file1 into file 2 edit files echo \"something\" > file // create file and write something in it. symbol ( > file ) // bind the input stream to something, store something in file symbol ( < file ) // bind the output stream to something, extract something from file cat file // print file content to the terminal cat < file // extract the content of file, and handle it as input to cat. cat < file1 > file2 // extract file1 content and store it in file2, copy file1 into file 2 cat < file1 >> file2 // extract file1 content and \"append\" it in file2, append file 2 symbol ( | ) // take the output of command to the left, make it input to the right ls - l | tail - n1 // show the last line only of the ls command, ls -l output will be input to tail -n1 Curl \u00b6 curl <url> : make get to url curl -v <url> : make get to the url with the request info curl -v -d \"<data>\" <url> : passing data to your request References \u00b6 [1] CAMBRIDGE: bash","title":"Bash"},{"location":"knowledge-base/general/tools/bash/#bash-shell","text":"","title":"Bash shell"},{"location":"knowledge-base/general/tools/bash/#intro","text":"intro time // show the time date // show the date dd/mm/yyyy echo \"\" // print something path // show env vars path which < program name > // show the path for this program pwd // print working directory tail // show the last 10 lines of file tail - n1 // show the last line of file tee < file name > // open interactive to write to file line by line cd cd < relative path | absolute path > // move to that path cd ~ // go home cd - // go to previous path clear the terminal clear // clear the terminal Ctrl + l // clear the console.","title":"intro"},{"location":"knowledge-base/general/tools/bash/#file-tree","text":"ls ls -- help // ls help ls // show all files. ls - a // show all files, including hiddens. ls - l // show all files with details tree tree // show only folder structure tree / f // show folders with files","title":"File Tree"},{"location":"knowledge-base/general/tools/bash/#handle-files","text":"open files cat file . ext // open file in the terminal delete file rm < file_name > // delete a file rm - r < directory_name > // delete a directory move, copy, rename files mv < path1 / file > < path2 / file > // move file from path1 to path2 mv < path / file1 > < path / file2 > // rename file1 to be file2 cp < path1 > < path2 > // copy file from path to path cat < file1 > file2 // extract file1 content and store it in file2, copy file1 into file 2 edit files echo \"something\" > file // create file and write something in it. symbol ( > file ) // bind the input stream to something, store something in file symbol ( < file ) // bind the output stream to something, extract something from file cat file // print file content to the terminal cat < file // extract the content of file, and handle it as input to cat. cat < file1 > file2 // extract file1 content and store it in file2, copy file1 into file 2 cat < file1 >> file2 // extract file1 content and \"append\" it in file2, append file 2 symbol ( | ) // take the output of command to the left, make it input to the right ls - l | tail - n1 // show the last line only of the ls command, ls -l output will be input to tail -n1","title":"Handle files"},{"location":"knowledge-base/general/tools/bash/#curl","text":"curl <url> : make get to url curl -v <url> : make get to the url with the request info curl -v -d \"<data>\" <url> : passing data to your request","title":"Curl"},{"location":"knowledge-base/general/tools/bash/#references","text":"[1] CAMBRIDGE: bash","title":"References"},{"location":"knowledge-base/general/tools/docker/","text":"Docker \u00b6 alawys start by this command to start the defaul machine docker-machine start default Resource: Brad Traversy Help General \u00b6 if your using docker virtualbox make sure you open virtualbox and the machine is working while you are using the cmd you will not find your server on http://localhost:port or 0.0.0.0:port . you will find it on http://192.168.99.100:port docker images are like environments. ex: node.js docker containers are your own projects that are built on a specific image. when you create a container, if the image is not preveously downloaded on your machine it's going to be downloaded from docker hub . docker container run <-d | -it> -p <our_local_port>:<virtual_port_for_this_image> <image> docker volume is a way to pull a continer into your local file system and start editing it. commands \u00b6 docker version : show docker version docker info : show server and client info docker container run -it -p 80:80 <nginx> : create and run and publish docker container of enginx server on port 80 \\(default\\) in the foreground. docker container run -d -p 8080:80 --name <container_name> <image> : create and run a container in the background docker container < ls | ps > : list all running containers. docker container ls -a : list all containers wether they are running or not docker container rm <container_id> : delete a container with the id of :container_id from your system. docker images : list all images on your machine docker pull <image_name> : download the <image_name> from docker hub` docker stop <container_id> : stop a running container. docker ps : list all running containers. docker container run -d -p 3306:3306 --name <my_name> --env MYSQL_ROOT_PASSWORD=<pass> mysql : create mysql container with env variables which is password . docker container rm <contaier_name | container_id> -f : remove a running container. docker container exec -it mynginx bash : open the container file system interactively into bash command line create a volume and grap the files into your local machine docker container run -d -p 8080:80 -v /${pwd}://usr/share/nginx/html --name nginxwebsite nginx make sure to use extra / before your 2 paths as /${pwd} and //usr/share/nginx/hhtml on windows. push the current container to docker hub: docker push <username>/<repo_name> Edit contianer files \u00b6 Edit files of nginx container on the fly > docker container exec -it mynginx bash > ls > cd usr/share/nginx create a volume and grap the files into your local machine docker container run -d -p 8080 :80 -v / ${ pwd } ://usr/share/nginx/html --name nginxwebsite nginx make sure to use extra / before your 2 paths as /${pwd} and //usr/share/nginx/hhtml on windows. Docker-machine commands: > docker - machine ls // list all machines > docker - machine start default // starts the default machine > docker - machine stop default //stops the default machine","title":"Docker"},{"location":"knowledge-base/general/tools/docker/#docker","text":"alawys start by this command to start the defaul machine docker-machine start default Resource: Brad Traversy Help","title":"Docker"},{"location":"knowledge-base/general/tools/docker/#general","text":"if your using docker virtualbox make sure you open virtualbox and the machine is working while you are using the cmd you will not find your server on http://localhost:port or 0.0.0.0:port . you will find it on http://192.168.99.100:port docker images are like environments. ex: node.js docker containers are your own projects that are built on a specific image. when you create a container, if the image is not preveously downloaded on your machine it's going to be downloaded from docker hub . docker container run <-d | -it> -p <our_local_port>:<virtual_port_for_this_image> <image> docker volume is a way to pull a continer into your local file system and start editing it.","title":"General"},{"location":"knowledge-base/general/tools/docker/#commands","text":"docker version : show docker version docker info : show server and client info docker container run -it -p 80:80 <nginx> : create and run and publish docker container of enginx server on port 80 \\(default\\) in the foreground. docker container run -d -p 8080:80 --name <container_name> <image> : create and run a container in the background docker container < ls | ps > : list all running containers. docker container ls -a : list all containers wether they are running or not docker container rm <container_id> : delete a container with the id of :container_id from your system. docker images : list all images on your machine docker pull <image_name> : download the <image_name> from docker hub` docker stop <container_id> : stop a running container. docker ps : list all running containers. docker container run -d -p 3306:3306 --name <my_name> --env MYSQL_ROOT_PASSWORD=<pass> mysql : create mysql container with env variables which is password . docker container rm <contaier_name | container_id> -f : remove a running container. docker container exec -it mynginx bash : open the container file system interactively into bash command line create a volume and grap the files into your local machine docker container run -d -p 8080:80 -v /${pwd}://usr/share/nginx/html --name nginxwebsite nginx make sure to use extra / before your 2 paths as /${pwd} and //usr/share/nginx/hhtml on windows. push the current container to docker hub: docker push <username>/<repo_name>","title":"commands"},{"location":"knowledge-base/general/tools/docker/#edit-contianer-files","text":"Edit files of nginx container on the fly > docker container exec -it mynginx bash > ls > cd usr/share/nginx create a volume and grap the files into your local machine docker container run -d -p 8080 :80 -v / ${ pwd } ://usr/share/nginx/html --name nginxwebsite nginx make sure to use extra / before your 2 paths as /${pwd} and //usr/share/nginx/hhtml on windows. Docker-machine commands: > docker - machine ls // list all machines > docker - machine start default // starts the default machine > docker - machine stop default //stops the default machine","title":"Edit contianer files"},{"location":"knowledge-base/general/tools/git/","text":"Git \u00b6 Theory [1]: \u00b6 data types in git system: /* 1 */ type blob = array < bytes > // file = blob, contain text as bytes /* 2 */ type tree = map < string >< tree | blog > // tree = folder, the string (name of folder) points to the //contents ( can be files or folders) (blobs or another tree) /* 3 */ type commit = struct { parents : array < commits > // the actual commit, snapchot of the whole contnet of the repo author : string message : string snapshot : commit } /* 4 */ type object = blob | tree | commit // we refer to any type with object /* 5 */ type objects = map < string >< object > // the string (object name) points to the actual object /* 6 */ function store ( object ){ id = hash ( object ); // hash the object then store this hash in objects map, where it points to the actual object objects [ id ] = object ; } /* 7 */ function load ( id ){ return objects [ id ] ; // it retrives the actual object by its id (or hash) } /* 8 */ type references = map < string > < string > // maps the names I give to the hashes that git gives, // so I can access eather by hash or my name. logging \u00b6 loging info about repo status: git log -- oneline -- graph // list all commits, line for every commit git log -- all -- graph -- decorate // more visual representation. log more data on a specific commit [1]: git cat-file -p <commit hash> commits \u00b6 unstage last commit: git reset -- soft HEAD ~ 1 // unstage last commit and keep the changes git reset -- hard HEAD ~ 1 // unstage last commit and discard the changes ** careful show diff: git diff // shows diff between HEAD commit, and your files now for entire repo git diff < file name > // shows diff between HEAD commit, and your files now for specific file git diff < commit1 | branch1 > < commit2 | branch2 > /* diff between commit1/branch1 and commit2/branch2 for all files */ git diff < commit1 | branch1 > < commit2 | branch2 > < file name > /* diff between commit1/branch1 and commit2/branch2 for specifi file. */ git diff -- cached // show the staged chunks delete un staged changes : git checkout < file name > // through away all un stagged changes git commit -- amend // edit a commit\u2019s contents/message stage only part of the changes to one file (stage chunk of a file): git add - p < file name > // open interactive terminal to choose which parts u want to commit git diff -- cached // show the staged chunks show git show < commit hash > // show deatailed info for that commit Undo changes \u00b6 discard all changes before staging or commiting them git reset --hard - discard changes in one file: git checkout -- <file> Branches \u00b6 list all branches: git branch // all branches git branch - a // list local and remote branches git branch - r // list remote branches only git show - branch // list branches and their last commit git branch - vv // list all branches that my env is aware of in details. create Branches git branch < branch name > ; git checkout < branch name > ; // create new branch, checkout to its git checkout - b < branch name > // create new branch, checkout to its delete branches git branch - d < branch name > //delete branch locally git branch - D < branch name > // force delete branch locally, if the branch did not merged (force) git push origin -- delete < branch name > // delete remote branch remove all deleted remote branches from local environment (list of branches in VS code)s: javascript git fetch --prune //rmove deleted branches from VS list merge branches: git merge < branch name > // merges brannc_name with the current branch you are in. git merge -- abort // aborting merge command git merge -- continue // continuing merge process after we resolve conflicts. reslove conflicts: git mergetool // opens interactive command line tool to resolve conflicts, or open your editor remote \u00b6 clone git clone < url > < folder name > // clone remote repo to ur env with all version control history git clone < url > < folder name > -- shallow // clone only the latest snapshot of remote repo to ur env. fetch and pull: git push // if u set upstream, u can push directly to the origin. git fetch < remote > < branch > // fetch changes in the remote branch, but don't merge them. git fetch < remote > < branch > ; git merge ; // fetch remote branch, merge it with the current branches git pull < remote > < branch > // fetch remote branch and merge it. handle remote branches: git remote // list all remote repos git remote add < name > < url > // add remote repo, u can access it by name. git push < remote name > < local branch >:< remote branch > // push local branch content into remote branch. \"git branch --set-upstream-to=<remote_name>\" // setting the upstream as remote repo gitconfig \u00b6 git blame \u00b6 show whose and which commits resposible for particular challenges git blame < file name > // show whose and which commits resposible for every line in the file git stash \u00b6 save changes temporarly without staging them, then retrive those changes later git stash // save changes temprarly git stash pop // retrive stashed changes git stash list // list all stashes as /** stash@{0}: WIP on feat-reminders: c3012fd chore: reminders final QA and testing adjustments stash@{1}: On develop: stach1 stash@{2}: On improve-asana-sync: move to develop */ git apply stash @ { 0 } // apply this stash git rebase [2] \u00b6 The second way of combining work between branches is rebasing (first is merging). Rebasing takes a set of commits, \"copies\" them, and plops them down somewhere else. rebasing can be used to make a nice linear sequence of commits. The commit log / history of the repository will be a lot cleaner if only rebasing is allowed. we have this situation: git rebase master the commit C3 still exists somewhere (it has a faded appearance in the tree), and C3' is the \"copy\" that we rebased onto master. git rebase bugfix Since master was an ancestor of bugFix, git simply moved the master branch reference forward in history. rebase commands: git rebase < branch name > // rebase the parrelel branch into the main backbone of changes git rebase - i // interactive rebasing git bisect \u00b6 manually search history for something useful in find the first commit where something broke do binary search in the history references \u00b6 [1] CAMBRIDGE: lecture 6 : version control [2] learnbranching.js.org [3] successful git branching model","title":"Git"},{"location":"knowledge-base/general/tools/git/#git","text":"","title":"Git"},{"location":"knowledge-base/general/tools/git/#theory-1","text":"data types in git system: /* 1 */ type blob = array < bytes > // file = blob, contain text as bytes /* 2 */ type tree = map < string >< tree | blog > // tree = folder, the string (name of folder) points to the //contents ( can be files or folders) (blobs or another tree) /* 3 */ type commit = struct { parents : array < commits > // the actual commit, snapchot of the whole contnet of the repo author : string message : string snapshot : commit } /* 4 */ type object = blob | tree | commit // we refer to any type with object /* 5 */ type objects = map < string >< object > // the string (object name) points to the actual object /* 6 */ function store ( object ){ id = hash ( object ); // hash the object then store this hash in objects map, where it points to the actual object objects [ id ] = object ; } /* 7 */ function load ( id ){ return objects [ id ] ; // it retrives the actual object by its id (or hash) } /* 8 */ type references = map < string > < string > // maps the names I give to the hashes that git gives, // so I can access eather by hash or my name.","title":"Theory [1]:"},{"location":"knowledge-base/general/tools/git/#logging","text":"loging info about repo status: git log -- oneline -- graph // list all commits, line for every commit git log -- all -- graph -- decorate // more visual representation. log more data on a specific commit [1]: git cat-file -p <commit hash>","title":"logging"},{"location":"knowledge-base/general/tools/git/#commits","text":"unstage last commit: git reset -- soft HEAD ~ 1 // unstage last commit and keep the changes git reset -- hard HEAD ~ 1 // unstage last commit and discard the changes ** careful show diff: git diff // shows diff between HEAD commit, and your files now for entire repo git diff < file name > // shows diff between HEAD commit, and your files now for specific file git diff < commit1 | branch1 > < commit2 | branch2 > /* diff between commit1/branch1 and commit2/branch2 for all files */ git diff < commit1 | branch1 > < commit2 | branch2 > < file name > /* diff between commit1/branch1 and commit2/branch2 for specifi file. */ git diff -- cached // show the staged chunks delete un staged changes : git checkout < file name > // through away all un stagged changes git commit -- amend // edit a commit\u2019s contents/message stage only part of the changes to one file (stage chunk of a file): git add - p < file name > // open interactive terminal to choose which parts u want to commit git diff -- cached // show the staged chunks show git show < commit hash > // show deatailed info for that commit","title":"commits"},{"location":"knowledge-base/general/tools/git/#undo-changes","text":"discard all changes before staging or commiting them git reset --hard - discard changes in one file: git checkout -- <file>","title":"Undo changes"},{"location":"knowledge-base/general/tools/git/#branches","text":"list all branches: git branch // all branches git branch - a // list local and remote branches git branch - r // list remote branches only git show - branch // list branches and their last commit git branch - vv // list all branches that my env is aware of in details. create Branches git branch < branch name > ; git checkout < branch name > ; // create new branch, checkout to its git checkout - b < branch name > // create new branch, checkout to its delete branches git branch - d < branch name > //delete branch locally git branch - D < branch name > // force delete branch locally, if the branch did not merged (force) git push origin -- delete < branch name > // delete remote branch remove all deleted remote branches from local environment (list of branches in VS code)s: javascript git fetch --prune //rmove deleted branches from VS list merge branches: git merge < branch name > // merges brannc_name with the current branch you are in. git merge -- abort // aborting merge command git merge -- continue // continuing merge process after we resolve conflicts. reslove conflicts: git mergetool // opens interactive command line tool to resolve conflicts, or open your editor","title":"Branches"},{"location":"knowledge-base/general/tools/git/#remote","text":"clone git clone < url > < folder name > // clone remote repo to ur env with all version control history git clone < url > < folder name > -- shallow // clone only the latest snapshot of remote repo to ur env. fetch and pull: git push // if u set upstream, u can push directly to the origin. git fetch < remote > < branch > // fetch changes in the remote branch, but don't merge them. git fetch < remote > < branch > ; git merge ; // fetch remote branch, merge it with the current branches git pull < remote > < branch > // fetch remote branch and merge it. handle remote branches: git remote // list all remote repos git remote add < name > < url > // add remote repo, u can access it by name. git push < remote name > < local branch >:< remote branch > // push local branch content into remote branch. \"git branch --set-upstream-to=<remote_name>\" // setting the upstream as remote repo","title":"remote"},{"location":"knowledge-base/general/tools/git/#gitconfig","text":"","title":"gitconfig"},{"location":"knowledge-base/general/tools/git/#git-blame","text":"show whose and which commits resposible for particular challenges git blame < file name > // show whose and which commits resposible for every line in the file","title":"git blame"},{"location":"knowledge-base/general/tools/git/#git-stash","text":"save changes temporarly without staging them, then retrive those changes later git stash // save changes temprarly git stash pop // retrive stashed changes git stash list // list all stashes as /** stash@{0}: WIP on feat-reminders: c3012fd chore: reminders final QA and testing adjustments stash@{1}: On develop: stach1 stash@{2}: On improve-asana-sync: move to develop */ git apply stash @ { 0 } // apply this stash","title":"git stash"},{"location":"knowledge-base/general/tools/git/#git-rebase-2","text":"The second way of combining work between branches is rebasing (first is merging). Rebasing takes a set of commits, \"copies\" them, and plops them down somewhere else. rebasing can be used to make a nice linear sequence of commits. The commit log / history of the repository will be a lot cleaner if only rebasing is allowed. we have this situation: git rebase master the commit C3 still exists somewhere (it has a faded appearance in the tree), and C3' is the \"copy\" that we rebased onto master. git rebase bugfix Since master was an ancestor of bugFix, git simply moved the master branch reference forward in history. rebase commands: git rebase < branch name > // rebase the parrelel branch into the main backbone of changes git rebase - i // interactive rebasing","title":"git rebase [2]"},{"location":"knowledge-base/general/tools/git/#git-bisect","text":"manually search history for something useful in find the first commit where something broke do binary search in the history","title":"git bisect"},{"location":"knowledge-base/general/tools/git/#references","text":"[1] CAMBRIDGE: lecture 6 : version control [2] learnbranching.js.org [3] successful git branching model","title":"references"},{"location":"knowledge-base/general/tools/grunt/","text":"Grunt \u00b6 task runners : simplfiy the pre-proccesses of deployment: grunt gulp . grunt : - npm i grunt - touch Gruntfile.js - npm i grunt-contrib-less time-grunt jit-grunt - npm i grunt-contrib-watch grunt-browser-sync - simple grunt file for watching and compieling .less files: module . exports = function ( grunt ) { // Time how long tasks take. Can help when optimizing build times require ( \"time-grunt\" )( grunt ); // Automatically load required Grunt tasks require ( \"jit-grunt\" )( grunt ); // Define the configuration for all the tasks grunt . initConfig ({ less : { css : { files : { \"css/styles.css\" : \"css/styles.less\" , }, }, }, watch : { files : \"css/*.less\" , tasks : [ \"less\" ], }, browserSync : { dev : { bsFiles : { src : [ \"css/*.css\" , \"*.html\" , \"js/*.js\" ], }, options : { watchTask : true , server : { baseDir : \"./\" , }, }, }, }, }); grunt . registerTask ( \"css\" , [ \"less\" ]); grunt . registerTask ( \"default\" , [ \"browserSync\" , \"watch\" ]); }; Grunt: at the cmd : grunt less => All .less files compiling into css. at the cmd : grunt => watching and compiling automatically for pre-deploy process we need: - grunt-contrib-copy : copying files to dist folder. grunt-contrib-clean : clean dist each time before run build grunt-contrib-imagemin : minimizing images. - grunt-contrib-concat : concat files. - grunt-contrib-cssmin : min css. grunt-contrib-htmlmin : min html. grunt-contrib-uglify : min js. grunt-filerev . - grunt-usemin : use all min files. Best resource for Grunt : https://www.coursera.org/learn/bootstrap-4/supplement/SIHkS/exercise-instructions-grunt-part-2","title":"Grunt"},{"location":"knowledge-base/general/tools/grunt/#grunt","text":"task runners : simplfiy the pre-proccesses of deployment: grunt gulp . grunt : - npm i grunt - touch Gruntfile.js - npm i grunt-contrib-less time-grunt jit-grunt - npm i grunt-contrib-watch grunt-browser-sync - simple grunt file for watching and compieling .less files: module . exports = function ( grunt ) { // Time how long tasks take. Can help when optimizing build times require ( \"time-grunt\" )( grunt ); // Automatically load required Grunt tasks require ( \"jit-grunt\" )( grunt ); // Define the configuration for all the tasks grunt . initConfig ({ less : { css : { files : { \"css/styles.css\" : \"css/styles.less\" , }, }, }, watch : { files : \"css/*.less\" , tasks : [ \"less\" ], }, browserSync : { dev : { bsFiles : { src : [ \"css/*.css\" , \"*.html\" , \"js/*.js\" ], }, options : { watchTask : true , server : { baseDir : \"./\" , }, }, }, }, }); grunt . registerTask ( \"css\" , [ \"less\" ]); grunt . registerTask ( \"default\" , [ \"browserSync\" , \"watch\" ]); }; Grunt: at the cmd : grunt less => All .less files compiling into css. at the cmd : grunt => watching and compiling automatically for pre-deploy process we need: - grunt-contrib-copy : copying files to dist folder. grunt-contrib-clean : clean dist each time before run build grunt-contrib-imagemin : minimizing images. - grunt-contrib-concat : concat files. - grunt-contrib-cssmin : min css. grunt-contrib-htmlmin : min html. grunt-contrib-uglify : min js. grunt-filerev . - grunt-usemin : use all min files. Best resource for Grunt : https://www.coursera.org/learn/bootstrap-4/supplement/SIHkS/exercise-instructions-grunt-part-2","title":"Grunt"},{"location":"knowledge-base/general/tools/linux/","text":"linux \u00b6 Notes \u00b6 Kill process on port 3000 fuser -k -n tcp 3000 sudo kill -9 `sudo lsof -t -i:3000` Show most consuming process top Show memory usage free -m Show file system df -h Kill all node process Killall node Check if there is node working netstat -an","title":"linux"},{"location":"knowledge-base/general/tools/linux/#linux","text":"","title":"linux"},{"location":"knowledge-base/general/tools/linux/#notes","text":"Kill process on port 3000 fuser -k -n tcp 3000 sudo kill -9 `sudo lsof -t -i:3000` Show most consuming process top Show memory usage free -m Show file system df -h Kill all node process Killall node Check if there is node working netstat -an","title":"Notes"},{"location":"knowledge-base/general/tools/shel_scripting/","text":"shell scripting \u00b6 intro \u00b6 intro file . sh // file extention for shel files. $foo = \"bar\" // define foo echo $foo // print foo value echo \"something $foo\" // $foo is a variable, this won't work with single quote. echo \"something $(<cmnd>)\" // cmnd will be executed, and its value will be addeed to the string. $ # // number of arguments $0 // comand name $1 - $9 // first - 9th argument $$ // Process identification number (PID) for the current script - ne // not equal convert image .{ png , jpg } // convert image.png or image.jpg comments \u00b6 .cmd files: @echo off rem your comment here rem another comment line .sh files: # your comment here # another comment line Execute \u00b6 run .sh files in cmd or powershell: Bash <file.sh> <args> sh <file.sh> <args> handy shell scripts \u00b6 create private github repo from the terminal: create = curl -H \"Authorization: token <token>\" -H \"Contnet-Type: application/json\" https://api.github.com/user/repos -d \"{ \\\"name\\\": \\\" $1 \\\", \\\"private\\\": true }\" create private github repo and add it to the existing project: create-add = curl -H \"Authorization: token <token>\" -H \"Contnet-Type: application/json\" https://api.github.com/user/repos -d \"{ \\\"name\\\": \\\" $1 \\\", \\\"private\\\": true }\" && git remote add origin https://github.com/<your github user name>/ $1 .git create folder and cd into it: mcd = mkdir $1 && cd $1 references: \u00b6 [1] CAMBRIDGE: shell scripting","title":"Shel scripting"},{"location":"knowledge-base/general/tools/shel_scripting/#shell-scripting","text":"","title":"shell scripting"},{"location":"knowledge-base/general/tools/shel_scripting/#intro","text":"intro file . sh // file extention for shel files. $foo = \"bar\" // define foo echo $foo // print foo value echo \"something $foo\" // $foo is a variable, this won't work with single quote. echo \"something $(<cmnd>)\" // cmnd will be executed, and its value will be addeed to the string. $ # // number of arguments $0 // comand name $1 - $9 // first - 9th argument $$ // Process identification number (PID) for the current script - ne // not equal convert image .{ png , jpg } // convert image.png or image.jpg","title":"intro"},{"location":"knowledge-base/general/tools/shel_scripting/#comments","text":".cmd files: @echo off rem your comment here rem another comment line .sh files: # your comment here # another comment line","title":"comments"},{"location":"knowledge-base/general/tools/shel_scripting/#execute","text":"run .sh files in cmd or powershell: Bash <file.sh> <args> sh <file.sh> <args>","title":"Execute"},{"location":"knowledge-base/general/tools/shel_scripting/#handy-shell-scripts","text":"create private github repo from the terminal: create = curl -H \"Authorization: token <token>\" -H \"Contnet-Type: application/json\" https://api.github.com/user/repos -d \"{ \\\"name\\\": \\\" $1 \\\", \\\"private\\\": true }\" create private github repo and add it to the existing project: create-add = curl -H \"Authorization: token <token>\" -H \"Contnet-Type: application/json\" https://api.github.com/user/repos -d \"{ \\\"name\\\": \\\" $1 \\\", \\\"private\\\": true }\" && git remote add origin https://github.com/<your github user name>/ $1 .git create folder and cd into it: mcd = mkdir $1 && cd $1","title":"handy shell scripts"},{"location":"knowledge-base/general/tools/shel_scripting/#references","text":"[1] CAMBRIDGE: shell scripting","title":"references:"},{"location":"knowledge-base/general/tools/vim/","text":"vim \u00b6 intro \u00b6 vim modes normal : useful for navigating between files insert : type code replace: code you are writting replaces the previous visual select : 1. line 2. block command line mode : command shell from inside vim buttons in normal mode: \u00b6 i // insert mode esc // back to normal mode r // replace mode v // visual mode Shift + v // select line ctrl + v // select block : // command line mode Basic movement: hjkl (left, down, up, right) Words: w (next word), b (beginning of word), e (end of word) Lines: 0 (beginning of line), ^ (first non-blank character), $ (end of line) Screen: H (top of screen), M (middle of screen), L (bottom of screen) Scroll: Ctrl-u (up), Ctrl-d (down) File: gg (beginning of file), G (end of file) Line numbers: :{number} <CR> or {number}G (line {number}) Misc: % (corresponding item) Find: f{character}, t{character}, F{character}, T{character} find/to forward/backward {character} on the current line , / ; for navigating matches Search: /{regex}, n / N for navigating matches buttons in command line mode: \u00b6 : q // quit without saving : w // save, don't exit Vim in VS Code \u00b6 you start in Normal mode: /* button */ I , i // insert mode, type normally H // top of the Screen L < upper L > // end of Screen M // middle of the Screen esc // back to normal mode j // next line k // previous line l < lower L > // right one letter h < lower H > // left one letter w // move one word e // end of the next word g > e // move to the previous word f > { char } // move to specific char in the line 0 // Moves to the first character of a line ^ // Moves to the first non-blank character of a line $ // Moves to the end of a line g_ // Moves to the non-blank character at the end of a line } // jumps entire paragraphs downwards { // similarly but upwards CTRL - D // let\u2019s you move down half a page CTRL - U // let\u2019s you move up half a page /{pattern} / / to search forward inside a file ? { pattern } // to search backwards * // searchfor the word under the cursor { number } > { pattern } // execute the pattern n times like: 2 > j : moves down 2 lines. g > d // to jump to definition of whatever is under your cursor g > f // to jump to a file in an import g > g // to go to the top of the file { line } > g > g // to go to a specific line G // to go to the end of the file % // jump to matching ({[]}) References \u00b6 [1] Cambridge: Vim","title":"Vim"},{"location":"knowledge-base/general/tools/vim/#vim","text":"","title":"vim"},{"location":"knowledge-base/general/tools/vim/#intro","text":"vim modes normal : useful for navigating between files insert : type code replace: code you are writting replaces the previous visual select : 1. line 2. block command line mode : command shell from inside vim","title":"intro"},{"location":"knowledge-base/general/tools/vim/#buttons-in-normal-mode","text":"i // insert mode esc // back to normal mode r // replace mode v // visual mode Shift + v // select line ctrl + v // select block : // command line mode Basic movement: hjkl (left, down, up, right) Words: w (next word), b (beginning of word), e (end of word) Lines: 0 (beginning of line), ^ (first non-blank character), $ (end of line) Screen: H (top of screen), M (middle of screen), L (bottom of screen) Scroll: Ctrl-u (up), Ctrl-d (down) File: gg (beginning of file), G (end of file) Line numbers: :{number} <CR> or {number}G (line {number}) Misc: % (corresponding item) Find: f{character}, t{character}, F{character}, T{character} find/to forward/backward {character} on the current line , / ; for navigating matches Search: /{regex}, n / N for navigating matches","title":"buttons in normal mode:"},{"location":"knowledge-base/general/tools/vim/#buttons-in-command-line-mode","text":": q // quit without saving : w // save, don't exit","title":"buttons in command line mode:"},{"location":"knowledge-base/general/tools/vim/#vim-in-vs-code","text":"you start in Normal mode: /* button */ I , i // insert mode, type normally H // top of the Screen L < upper L > // end of Screen M // middle of the Screen esc // back to normal mode j // next line k // previous line l < lower L > // right one letter h < lower H > // left one letter w // move one word e // end of the next word g > e // move to the previous word f > { char } // move to specific char in the line 0 // Moves to the first character of a line ^ // Moves to the first non-blank character of a line $ // Moves to the end of a line g_ // Moves to the non-blank character at the end of a line } // jumps entire paragraphs downwards { // similarly but upwards CTRL - D // let\u2019s you move down half a page CTRL - U // let\u2019s you move up half a page /{pattern} / / to search forward inside a file ? { pattern } // to search backwards * // searchfor the word under the cursor { number } > { pattern } // execute the pattern n times like: 2 > j : moves down 2 lines. g > d // to jump to definition of whatever is under your cursor g > f // to jump to a file in an import g > g // to go to the top of the file { line } > g > g // to go to a specific line G // to go to the end of the file % // jump to matching ({[]})","title":"Vim in VS Code"},{"location":"knowledge-base/general/tools/vim/#references","text":"[1] Cambridge: Vim","title":"References"},{"location":"knowledge-base/general/tools/windows/","text":"Windows \u00b6 General notes \u00b6 it's fine to delete files that are no more than 24 hours old from c/windows/users/username/AppData/Temp freeing memory using cleanManager \u00b6 open task runner windowsIcon + R . type cleanmgr.exe then hit run. windows Powershell \u00b6 env path $env : path // shows env path variables $env : Path - split ';' // show env path vars, single line for each var ## UEFI and secure boot some new computers onlly allow windows to run, through their uefi settings. secure boot is the feature that prevent other system from runing. IMPORTANT: if you want in any case to change the boot path, you need to disable secuer boot from: start_computer > F2 > uefi settings.","title":"Windows"},{"location":"knowledge-base/general/tools/windows/#windows","text":"","title":"Windows"},{"location":"knowledge-base/general/tools/windows/#general-notes","text":"it's fine to delete files that are no more than 24 hours old from c/windows/users/username/AppData/Temp","title":"General notes"},{"location":"knowledge-base/general/tools/windows/#freeing-memory-using-cleanmanager","text":"open task runner windowsIcon + R . type cleanmgr.exe then hit run.","title":"freeing memory using cleanManager"},{"location":"knowledge-base/general/tools/windows/#windows-powershell","text":"env path $env : path // shows env path variables $env : Path - split ';' // show env path vars, single line for each var ## UEFI and secure boot some new computers onlly allow windows to run, through their uefi settings. secure boot is the feature that prevent other system from runing. IMPORTANT: if you want in any case to change the boot path, you need to disable secuer boot from: start_computer > F2 > uefi settings.","title":"windows Powershell"},{"location":"knowledge-base/general/web/","text":"","title":"Index"},{"location":"knowledge-base/general/web/dom/","text":"DOM \u00b6 input.value is alaways string, event if its type was number. client side temporary storage: window . x = value ; => window . x ; window . localStorage . x = value ; => window . loacalStorage . x ; add event listener to the key and use the key name intead of key code. window . addEventListener ( \"key-event\" , ( event ) => { event . key == \"arrowLeft\" ; //you can use key name instead of key code. }); make textarea with inly one line and disable multilines: js <textarea rows=\"1\" value ={ value } > { value } </textare> textarea.onChange = (e) => { let x = e.target.value; // deleting every new line signs from textarea value x.replace(/(\\r\\n|\\n|\\r)/gm, \"\"); } sanitize third party code before inject it into your website using innerHTML var sanitizeHTML = function ( str ) { var temp = document . createElement ( \"div\" ); temp . textContent = str ; return temp . innerHTML ; }; quickly check existance of a file using vanilla js: function doesFileExist ( urlToFile ) { var xhr = new XMLHttpRequest (); xhr . open ( \"HEAD\" , urlToFile , false ); xhr . send (); if ( xhr . status == \"404\" ) { return false ; } else { return true ; } } control scrolling: \u00b6 window . scrollTo ( x , y ); // x,y are the window cordinates. /* Using options: */ window . scrollTo ({ top : x , left : y , behavior : 'smooth' }); browser hsitory: \u00b6 let x = window . history // Array of the nubmer of pages in the window history // you can go back and forth between pages x . goBack () x . foreward () // you can never extract the path from window.history extract the url that you are coming from: let x = document . referrer // \"https://www.google.com/\" // you were in google page and they refer you to here window location: // typical window.location object Location = { href : \"https://www.w3schools.com/js/js_window_location.asp\" , ancestorOrigins : DOMStringList , origin : \"https://www.w3schools.com\" , protocol : \"https:\" , host : \"www.w3schools.com\" , pathname : \"/js/js_window_location.asp\" } respondWith() The respondWith() method of FetchEvent prevents the browser's default fetch handling, and allows you to provide a promise for a Response yourself. addEventListener ( 'fetch' , event => { // Prevent the default, and handle the request ourselves. event . respondWith ( async function () { // Try to get the response from a cache. const cachedResponse = await caches . match ( event . request ); // Return it if we found one. if ( cachedResponse ) return cachedResponse ; // If we didn't find a match in the cache, use the network. return fetch ( event . request ); }()); }); window.localStorage save data into 'localStorage' object. saved data will be available on the client side, will stay if the tab closed or the session ended. no Expiary date. can be cleared only by javascript or clearing browser data for this website. data can be stored only as strings, so you need to use JSON.stringify() before saving. to retrieve the stringified saved data you need to use JSON.Parse() more: https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage window.sessionStorage \u00b6 save data into sessionStorage objec. if the tab closed or session ended the data will be deleted . at most 5MB more: https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage window.IndexedDB \u00b6 clinet side SQL-like temporary storage. allows you to save a significant amount of data, including files/blobs more: https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API FetchEvent.respondWith() \u00b6 more: https://developer.mozilla.org/en-US/docs/Web/API/FetchEvent/respondWith fetchEvent . respondWith ( // Promise that resolves to a Response. \u200b ); Get list of all Dom elements \u00b6 var all = document . getElementsByTagName ( \"*\" ); Make post Request from the browser instead of postman \u00b6 // you don't need the full url, the end point is fine. await fetch ( '/endpoint' , { method : 'POST' , Headers : { \"Content-Type\" : \"application/json\" }, body : JSON . stringify ({ Title : \"post0\" , Post : \"first post req from browser\" }) }); Input Event after user finish typing instead of onChange \u00b6 //setup before functions var typingTimer ; //timer identifier var doneTypingInterval = 1000 ; //time in ms, 5 second for example //on keyup, start the countdown const waitAbitOnKeyUp = ( e , i ) => { clearTimeout ( typingTimer ); typingTimer = setTimeout (() => doneTyping ( e , i ), doneTypingInterval ); }; //on keydown, clear the countdown const waitAbitOnKeyDown = () => { clearTimeout ( typingTimer ); }; //user is \"finished typing,\" do something function doneTyping ( e , i ) { //do something } Resources \u00b6 Add to homescreen button form mobiles and new chrome desktop","title":"DOM"},{"location":"knowledge-base/general/web/dom/#dom","text":"input.value is alaways string, event if its type was number. client side temporary storage: window . x = value ; => window . x ; window . localStorage . x = value ; => window . loacalStorage . x ; add event listener to the key and use the key name intead of key code. window . addEventListener ( \"key-event\" , ( event ) => { event . key == \"arrowLeft\" ; //you can use key name instead of key code. }); make textarea with inly one line and disable multilines: js <textarea rows=\"1\" value ={ value } > { value } </textare> textarea.onChange = (e) => { let x = e.target.value; // deleting every new line signs from textarea value x.replace(/(\\r\\n|\\n|\\r)/gm, \"\"); } sanitize third party code before inject it into your website using innerHTML var sanitizeHTML = function ( str ) { var temp = document . createElement ( \"div\" ); temp . textContent = str ; return temp . innerHTML ; }; quickly check existance of a file using vanilla js: function doesFileExist ( urlToFile ) { var xhr = new XMLHttpRequest (); xhr . open ( \"HEAD\" , urlToFile , false ); xhr . send (); if ( xhr . status == \"404\" ) { return false ; } else { return true ; } }","title":"DOM"},{"location":"knowledge-base/general/web/dom/#control-scrolling","text":"window . scrollTo ( x , y ); // x,y are the window cordinates. /* Using options: */ window . scrollTo ({ top : x , left : y , behavior : 'smooth' });","title":"control scrolling:"},{"location":"knowledge-base/general/web/dom/#browser-hsitory","text":"let x = window . history // Array of the nubmer of pages in the window history // you can go back and forth between pages x . goBack () x . foreward () // you can never extract the path from window.history extract the url that you are coming from: let x = document . referrer // \"https://www.google.com/\" // you were in google page and they refer you to here window location: // typical window.location object Location = { href : \"https://www.w3schools.com/js/js_window_location.asp\" , ancestorOrigins : DOMStringList , origin : \"https://www.w3schools.com\" , protocol : \"https:\" , host : \"www.w3schools.com\" , pathname : \"/js/js_window_location.asp\" } respondWith() The respondWith() method of FetchEvent prevents the browser's default fetch handling, and allows you to provide a promise for a Response yourself. addEventListener ( 'fetch' , event => { // Prevent the default, and handle the request ourselves. event . respondWith ( async function () { // Try to get the response from a cache. const cachedResponse = await caches . match ( event . request ); // Return it if we found one. if ( cachedResponse ) return cachedResponse ; // If we didn't find a match in the cache, use the network. return fetch ( event . request ); }()); }); window.localStorage save data into 'localStorage' object. saved data will be available on the client side, will stay if the tab closed or the session ended. no Expiary date. can be cleared only by javascript or clearing browser data for this website. data can be stored only as strings, so you need to use JSON.stringify() before saving. to retrieve the stringified saved data you need to use JSON.Parse() more: https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage","title":"browser hsitory:"},{"location":"knowledge-base/general/web/dom/#windowsessionstorage","text":"save data into sessionStorage objec. if the tab closed or session ended the data will be deleted . at most 5MB more: https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage","title":"window.sessionStorage"},{"location":"knowledge-base/general/web/dom/#windowindexeddb","text":"clinet side SQL-like temporary storage. allows you to save a significant amount of data, including files/blobs more: https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API","title":"window.IndexedDB"},{"location":"knowledge-base/general/web/dom/#fetcheventrespondwith","text":"more: https://developer.mozilla.org/en-US/docs/Web/API/FetchEvent/respondWith fetchEvent . respondWith ( // Promise that resolves to a Response. \u200b );","title":"FetchEvent.respondWith()"},{"location":"knowledge-base/general/web/dom/#get-list-of-all-dom-elements","text":"var all = document . getElementsByTagName ( \"*\" );","title":"Get list of all Dom elements"},{"location":"knowledge-base/general/web/dom/#make-post-request-from-the-browser-instead-of-postman","text":"// you don't need the full url, the end point is fine. await fetch ( '/endpoint' , { method : 'POST' , Headers : { \"Content-Type\" : \"application/json\" }, body : JSON . stringify ({ Title : \"post0\" , Post : \"first post req from browser\" }) });","title":"Make post Request from the browser instead of postman"},{"location":"knowledge-base/general/web/dom/#input-event-after-user-finish-typing-instead-of-onchange","text":"//setup before functions var typingTimer ; //timer identifier var doneTypingInterval = 1000 ; //time in ms, 5 second for example //on keyup, start the countdown const waitAbitOnKeyUp = ( e , i ) => { clearTimeout ( typingTimer ); typingTimer = setTimeout (() => doneTyping ( e , i ), doneTypingInterval ); }; //on keydown, clear the countdown const waitAbitOnKeyDown = () => { clearTimeout ( typingTimer ); }; //user is \"finished typing,\" do something function doneTyping ( e , i ) { //do something }","title":"Input Event after user finish typing instead of onChange"},{"location":"knowledge-base/general/web/dom/#resources","text":"Add to homescreen button form mobiles and new chrome desktop","title":"Resources"},{"location":"knowledge-base/general/web/fonts/","text":"Fonts I like \u00b6 1- Roboto 2- Raleway light 3- prompt bold","title":"Fonts I like"},{"location":"knowledge-base/general/web/fonts/#fonts-i-like","text":"1- Roboto 2- Raleway light 3- prompt bold","title":"Fonts I like"},{"location":"knowledge-base/general/web/meta/","text":"Meta \u00b6 check if your client using mobile or not: function isMobile () { return /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i . test ( window . navigator . userAgent ); } dowonloading a file without user approval **: function downloadFile ( url , name ) { const elink = document . createElement ( \"a\" ); elink . style . display = \"none\" ; elink . href = url ; elink . download = name ; document . body . appendChild ( elink ); elink . click (); document . body . removeChild ( elink ); }","title":"Meta"},{"location":"knowledge-base/general/web/meta/#meta","text":"check if your client using mobile or not: function isMobile () { return /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i . test ( window . navigator . userAgent ); } dowonloading a file without user approval **: function downloadFile ( url , name ) { const elink = document . createElement ( \"a\" ); elink . style . display = \"none\" ; elink . href = url ; elink . download = name ; document . body . appendChild ( elink ); elink . click (); document . body . removeChild ( elink ); }","title":"Meta"},{"location":"knowledge-base/general/web/pwa/","text":"Progressive Web Apps PWA \u00b6 start here \u00b6 https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API/Using_Service_Workers follow this checklist to make sure you have PWA \u00b6 https://web.dev/pwa-checklist/ Regeister worker life cycle \u00b6 Regester => Install => Activate => fetch you can add event listener to each one of them. caching statc files happens in install caching dynamically happens in fetch Register \u00b6 inside main html file. <script> if ('serviceWorker' in navigator) { window.addEventListener('load', function() { navigator.serviceWorker .register('dynamicWorker.js') .then( function(registration) { // Registration was successful console.log( 'ServiceWorker registration successful with scope: ', registration.scope ) }, function(err) { // registration failed :( console.log('ServiceWorker registration failed: ', err) } ) .catch(function(err) { console.log(err) }) }) } else { console.log('service worker is not supported') } </script> install and activate \u00b6 inside serviceWorker.js ```js self.addEventListener \\('install', function\\(event\\) { event.waitUntil( fetchStuffAndInitDatabases\\(\\) ); }); self.addEventListener \\('activate', function\\(event\\) { // You're good to go! }); ### fetch - inside `serviceWorker.js` - listening to any fetch request from the client. ```js // fetch event self.addEventListener('fetch', (e) => { console.log('fetching ....................'); e.respondWith( fetch(e.request) .then(res => { // copy the reponse const resClone = res.clone(); //caching the response dynamically caches.open(currentChacheName).then(cache => { // puting in the cache cache.put(e.request, resClone); }) // preceeding with the response return res; }) // if no reponse .catch((err) => caches.match(e.request).then(res => res) )) }) getInstalledRelatedApps() \u00b6 checks if your app installed or not. https://web.dev/get-installed-related-apps/ BeforeInstallPromptEvent \u00b6 https://developer.mozilla.org/en-US/docs/Web/API/BeforeInstallPromptEvent What does it take to be installable? \u00b6 https://web.dev/install-criteria/ Add a web app manifest \u00b6 https://web.dev/add-manifest/ ways to prompt installations for your app \u00b6 https://web.dev/promote-install/ Examples \u00b6 https://web.dev/codelab-make-installable/ https://github.com/mdn/pwa-examples resourses \u00b6 google official documentaions service worker Explained official react pwa documentation service worker Extensive Google docs","title":"Progressive Web Apps PWA"},{"location":"knowledge-base/general/web/pwa/#progressive-web-apps-pwa","text":"","title":"Progressive Web Apps PWA"},{"location":"knowledge-base/general/web/pwa/#start-here","text":"https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API/Using_Service_Workers","title":"start here"},{"location":"knowledge-base/general/web/pwa/#follow-this-checklist-to-make-sure-you-have-pwa","text":"https://web.dev/pwa-checklist/","title":"follow this checklist to make sure you have PWA"},{"location":"knowledge-base/general/web/pwa/#regeister-worker-life-cycle","text":"Regester => Install => Activate => fetch you can add event listener to each one of them. caching statc files happens in install caching dynamically happens in fetch","title":"Regeister worker life cycle"},{"location":"knowledge-base/general/web/pwa/#register","text":"inside main html file. <script> if ('serviceWorker' in navigator) { window.addEventListener('load', function() { navigator.serviceWorker .register('dynamicWorker.js') .then( function(registration) { // Registration was successful console.log( 'ServiceWorker registration successful with scope: ', registration.scope ) }, function(err) { // registration failed :( console.log('ServiceWorker registration failed: ', err) } ) .catch(function(err) { console.log(err) }) }) } else { console.log('service worker is not supported') } </script>","title":"Register"},{"location":"knowledge-base/general/web/pwa/#install-and-activate","text":"inside serviceWorker.js ```js self.addEventListener \\('install', function\\(event\\) { event.waitUntil( fetchStuffAndInitDatabases\\(\\) ); }); self.addEventListener \\('activate', function\\(event\\) { // You're good to go! }); ### fetch - inside `serviceWorker.js` - listening to any fetch request from the client. ```js // fetch event self.addEventListener('fetch', (e) => { console.log('fetching ....................'); e.respondWith( fetch(e.request) .then(res => { // copy the reponse const resClone = res.clone(); //caching the response dynamically caches.open(currentChacheName).then(cache => { // puting in the cache cache.put(e.request, resClone); }) // preceeding with the response return res; }) // if no reponse .catch((err) => caches.match(e.request).then(res => res) )) })","title":"install and activate"},{"location":"knowledge-base/general/web/pwa/#getinstalledrelatedapps","text":"checks if your app installed or not. https://web.dev/get-installed-related-apps/","title":"getInstalledRelatedApps()"},{"location":"knowledge-base/general/web/pwa/#beforeinstallpromptevent","text":"https://developer.mozilla.org/en-US/docs/Web/API/BeforeInstallPromptEvent","title":"BeforeInstallPromptEvent"},{"location":"knowledge-base/general/web/pwa/#what-does-it-take-to-be-installable","text":"https://web.dev/install-criteria/","title":"What does it take to be installable?"},{"location":"knowledge-base/general/web/pwa/#add-a-web-app-manifest","text":"https://web.dev/add-manifest/","title":"Add a web app manifest"},{"location":"knowledge-base/general/web/pwa/#ways-to-prompt-installations-for-your-app","text":"https://web.dev/promote-install/","title":"ways to prompt installations for your app"},{"location":"knowledge-base/general/web/pwa/#examples","text":"https://web.dev/codelab-make-installable/ https://github.com/mdn/pwa-examples","title":"Examples"},{"location":"knowledge-base/general/web/pwa/#resourses","text":"google official documentaions service worker Explained official react pwa documentation service worker Extensive Google docs","title":"resourses"},{"location":"knowledge-base/general/web/seo/","text":"SEO \u00b6 retrieve how google caching your website, visit: cache : example . com ; refresh yuor sitemap: https : //www.google.com/ping?sitemap=`url to your siteMap` social media tags: <meta property=\"og:title\" content=\"#\" /> <meta property=\"og:description\" content=\"#\" /> <meta property=\"og:image\" content=\"#\" /> <meta property=\"og:url\" content=\"#\" /> <meta name=\"twitter:card\" content=\"#\" /> <meta name=\"twitter:image\" content=\"#\" /> <meta name=\"author\" content=\"[Author name here]\" />","title":"SEO"},{"location":"knowledge-base/general/web/seo/#seo","text":"retrieve how google caching your website, visit: cache : example . com ; refresh yuor sitemap: https : //www.google.com/ping?sitemap=`url to your siteMap` social media tags: <meta property=\"og:title\" content=\"#\" /> <meta property=\"og:description\" content=\"#\" /> <meta property=\"og:image\" content=\"#\" /> <meta property=\"og:url\" content=\"#\" /> <meta name=\"twitter:card\" content=\"#\" /> <meta name=\"twitter:image\" content=\"#\" /> <meta name=\"author\" content=\"[Author name here]\" />","title":"SEO"},{"location":"knowledge-base/general/web/urls/","text":"URLs \u00b6 chain of funtions to hanlde urls: function getExt ( url ) { if ( url . includes ( \"?\" )) { return getExt ( url . split ( \"?\" )[ 0 ]); } if ( url . includes ( \"#\" )) { return getExt ( url . split ( \"#\" )[ 0 ]); } //change those functions depends on the situation return url . trim (). toLowerCase (). split ( \".\" ). pop (); } regular expressions again, to handle urls: let regex = /^(?<start>https|http)?(?<colon_slashes>:\\/\\/)?(?<three_w>www.)?(?<main>[\\w\\-\\_\\:]+)(?<dot_com>\\.[\\w]+)\\/*(?<text1>[\\w\\-\\_\\#\\?\\&\\=]*)\\/*(?<text2>[\\w\\-\\_\\#\\?\\&\\=]*)\\/*(?<text3>[\\w\\-\\_\\#\\?\\&\\=]*)/ ;","title":"URLs"},{"location":"knowledge-base/general/web/urls/#urls","text":"chain of funtions to hanlde urls: function getExt ( url ) { if ( url . includes ( \"?\" )) { return getExt ( url . split ( \"?\" )[ 0 ]); } if ( url . includes ( \"#\" )) { return getExt ( url . split ( \"#\" )[ 0 ]); } //change those functions depends on the situation return url . trim (). toLowerCase (). split ( \".\" ). pop (); } regular expressions again, to handle urls: let regex = /^(?<start>https|http)?(?<colon_slashes>:\\/\\/)?(?<three_w>www.)?(?<main>[\\w\\-\\_\\:]+)(?<dot_com>\\.[\\w]+)\\/*(?<text1>[\\w\\-\\_\\#\\?\\&\\=]*)\\/*(?<text2>[\\w\\-\\_\\#\\?\\&\\=]*)\\/*(?<text3>[\\w\\-\\_\\#\\?\\&\\=]*)/ ;","title":"URLs"},{"location":"knowledge-base/math1280-statistics/unit1/","text":"MATH 1280: unit 1: Introduction to statistics \u00b6 Definitions \u00b6 science of statistics : deals with the collection, analysis, interpretation and presentation of data. probability : study uncertainty, formalization and quantification of the notion of uncertainty, deals with the chance of event occurring. population : entire collection of persons, things or objects under study. sample : studying entire large populations is inefficient; a sample is a portion (or subset) of the entire population, this sample will be studied to gather information about the population. statistic : a number that is a property of the sample . parameter : a number that is a property of the population ; a statistic of a sample is used to estimate a parameter of the population; the better the sample representing the entire population the more accurate the estimation of the parameter from the statistic. average (mean) : (sum) / (count) portion : (count (size) of specific group) / (size of entire population or sample). R programming language \u00b6 R is object oriented; <- is the assignment operator in R. every variable is an object in R . some R functions that are available in R shell: R function its action c(...args) combine all its args into one object table(object) prints a table that contains the frequency of each element in the object plot(table) plots the table content getwd() get the current working directory setwd(path) set current directory to the supplied path mean(object) calculates the mean of this object summary(object) find the most common stats calculations for this object such as: mean, median, 1,2,3 rd Quarter .. save.image (filename.Rdata) saves the current workspace image into a workspace image file called filename.Rdata in the current working directory rm(list = ls()) clear session memory q() quits the shell load(filename.Rdata) loads values saved in a workspace image file.Rdata into session memory read.csv (filePath) reads csv file file.choose() opens a finder navigator to select a file read.table (filePath, headers, sep =\",\") same as read.csv() read.delim(filePath) reads tabular data from tab delimited .txt files View(object) open detailed description of the object in the view popup write.table (object, filename, separator) export object data into external file, filename contains the extension of the new file write.csv(...), write.csv2(...) as above, but only export csv files References \u00b6 Yakir, B. (2011). Introduction to Statistical Thinking (With R, Without Calculus). Retrieved from http://pluto.huji.ac.il/~msby/StatThink/IntroStat.pdf","title":"MATH 1280: unit 1: Introduction to statistics"},{"location":"knowledge-base/math1280-statistics/unit1/#math-1280-unit-1-introduction-to-statistics","text":"","title":"MATH 1280: unit 1: Introduction to statistics"},{"location":"knowledge-base/math1280-statistics/unit1/#definitions","text":"science of statistics : deals with the collection, analysis, interpretation and presentation of data. probability : study uncertainty, formalization and quantification of the notion of uncertainty, deals with the chance of event occurring. population : entire collection of persons, things or objects under study. sample : studying entire large populations is inefficient; a sample is a portion (or subset) of the entire population, this sample will be studied to gather information about the population. statistic : a number that is a property of the sample . parameter : a number that is a property of the population ; a statistic of a sample is used to estimate a parameter of the population; the better the sample representing the entire population the more accurate the estimation of the parameter from the statistic. average (mean) : (sum) / (count) portion : (count (size) of specific group) / (size of entire population or sample).","title":"Definitions"},{"location":"knowledge-base/math1280-statistics/unit1/#r-programming-language","text":"R is object oriented; <- is the assignment operator in R. every variable is an object in R . some R functions that are available in R shell: R function its action c(...args) combine all its args into one object table(object) prints a table that contains the frequency of each element in the object plot(table) plots the table content getwd() get the current working directory setwd(path) set current directory to the supplied path mean(object) calculates the mean of this object summary(object) find the most common stats calculations for this object such as: mean, median, 1,2,3 rd Quarter .. save.image (filename.Rdata) saves the current workspace image into a workspace image file called filename.Rdata in the current working directory rm(list = ls()) clear session memory q() quits the shell load(filename.Rdata) loads values saved in a workspace image file.Rdata into session memory read.csv (filePath) reads csv file file.choose() opens a finder navigator to select a file read.table (filePath, headers, sep =\",\") same as read.csv() read.delim(filePath) reads tabular data from tab delimited .txt files View(object) open detailed description of the object in the view popup write.table (object, filename, separator) export object data into external file, filename contains the extension of the new file write.csv(...), write.csv2(...) as above, but only export csv files","title":"R programming language"},{"location":"knowledge-base/math1280-statistics/unit1/#references","text":"Yakir, B. (2011). Introduction to Statistical Thinking (With R, Without Calculus). Retrieved from http://pluto.huji.ac.il/~msby/StatThink/IntroStat.pdf","title":"References"},{"location":"knowledge-base/mysql/","text":"MYSQL \u00b6","title":"MYSQL"},{"location":"knowledge-base/mysql/#mysql","text":"","title":"MYSQL"},{"location":"knowledge-base/mysql/windows/","text":"MYSQL on windows \u00b6 console \u00b6 mysqld -u root to start server, make sure mysql\\bin folder is added to the path mysql -u root to start interactive console show databases; to display all databases for the logged-in user create schema <DB_NAME> to create a new database. use <DB> to select a database.","title":"MYSQL on windows"},{"location":"knowledge-base/mysql/windows/#mysql-on-windows","text":"","title":"MYSQL on windows"},{"location":"knowledge-base/mysql/windows/#console","text":"mysqld -u root to start server, make sure mysql\\bin folder is added to the path mysql -u root to start interactive console show databases; to display all databases for the logged-in user create schema <DB_NAME> to create a new database. use <DB> to select a database.","title":"console"},{"location":"knowledge-base/todo/to_learn/","text":"list of things to learn more about them \u00b6 web \u00b6 handlers can be used to manage tab communication and synchronization: The Web Locks API SharedWorker BroadcastChannel postMessage unload localStorage and sessionStorage. shape detection API new standard payment request API page visibility api - another resource web share api - second resource - third resource web share target push api","title":"list of things to learn more about them"},{"location":"knowledge-base/todo/to_learn/#list-of-things-to-learn-more-about-them","text":"","title":"list of things to learn more about them"},{"location":"knowledge-base/todo/to_learn/#web","text":"handlers can be used to manage tab communication and synchronization: The Web Locks API SharedWorker BroadcastChannel postMessage unload localStorage and sessionStorage. shape detection API new standard payment request API page visibility api - another resource web share api - second resource - third resource web share target push api","title":"web"},{"location":"knowledge-base/todo/to_record/","text":"Things I need to make a videos about \u00b6 delaying heavy react component after others loaded so it does not block the whole App. Error: can not set headers after they set to the client. passing react props in the most efficent way.","title":"Things I need to make a videos about"},{"location":"knowledge-base/todo/to_record/#things-i-need-to-make-a-videos-about","text":"delaying heavy react component after others loaded so it does not block the whole App. Error: can not set headers after they set to the client. passing react props in the most efficent way.","title":"Things I need to make a videos about"},{"location":"projects/projects/","text":"list of live projects: \u00b6 Restaurante con Fusion: view Live -- code tv Shows: view live portfolio: view live -- code slider from scratch: view live -- code valency number calculator live and code codeWars katas \u00b6 human readable time: https://www.codewars.com/kata/52685f7382004e774f0001f7 valid parenthesis : https://www.codewars.com/kata/52774a314c2333f0a7000688 moving zeros : https://www.codewars.com/kata/52597aa56021e91c93000cb0/ Replacement: https://www.codewars.com/kata/598d89971928a085c000001a/solutions/javascript/me/best_practice Break camelCase: https://www.codewars.com/kata/5208f99aee097e6552000148/solutions/javascript/me/best_practice","title":"list of live projects:"},{"location":"projects/projects/#list-of-live-projects","text":"Restaurante con Fusion: view Live -- code tv Shows: view live portfolio: view live -- code slider from scratch: view live -- code valency number calculator live and code","title":"list of live projects:"},{"location":"projects/projects/#codewars-katas","text":"human readable time: https://www.codewars.com/kata/52685f7382004e774f0001f7 valid parenthesis : https://www.codewars.com/kata/52774a314c2333f0a7000688 moving zeros : https://www.codewars.com/kata/52597aa56021e91c93000cb0/ Replacement: https://www.codewars.com/kata/598d89971928a085c000001a/solutions/javascript/me/best_practice Break camelCase: https://www.codewars.com/kata/5208f99aee097e6552000148/solutions/javascript/me/best_practice","title":"codeWars katas"}]}